
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">





  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="Peter Frick" />
<meta name="description" content="I've been working on text classification recently. I've found keras to be a quite good high-level language and great for learning different neural network architectures. In this notebook I will examine Tweet classification using CNN and LSTM model architechtures. While CNNs are widely used in Computer Vision, I saw a paper" />
<meta name="keywords" content="python, machine learning, keras, nlp, deep learning, classification">
<meta property="og:site_name" content="Peter Frick"/>
<meta property="og:title" content="Exploring neural networks for text classification"/>
<meta property="og:description" content="I've been working on text classification recently. I've found keras to be a quite good high-level language and great for learning different neural network architectures. In this notebook I will examine Tweet classification using CNN and LSTM model architechtures. While CNNs are widely used in Computer Vision, I saw a paper"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/neural-networks-text_classification.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-11-17 00:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/peter-frick.html">
<meta property="article:section" content="blog"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="machine learning"/>
<meta property="article:tag" content="keras"/>
<meta property="article:tag" content="nlp"/>
<meta property="article:tag" content="deep learning"/>
<meta property="article:tag" content="classification"/>
<meta property="og:image" content="https://raw.githubusercontent.com/frickp/insight/master/FrickHeadshot.jpg">

  <title>Peter Frick &ndash; Exploring neural networks for text classification</title>

</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="https://raw.githubusercontent.com/frickp/insight/master/FrickHeadshot.jpg" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">
          <li><a href="/pages/about-me.html#about-me">About me</a></li>

          <li><a href="http://frickp.github.io" target="_blank">Blog</a></li>
          <li><a href="http://ccsb.vanderbilt.edu/qlab/frontpage" target="_blank">Quaranta lab</a></li>
          <li><a href="http://greenleaf.stanford.edu" target="_blank">Greenleaf lab</a></li>
          <li><a href="http://insightdatascience.com" target="_blank">Insight Data Science</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-LinkedIn" href="https://www.linkedin.com/pub/peter-frick/39/455/ab7" target="_blank"><i class="fa fa-LinkedIn"></i></a></li>
        <li><a class="sc-GitHub" href="https://github.com/frickp" target="_blank"><i class="fa fa-GitHub"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
    <h1 id="neural-networks-text_classification">Exploring neural networks for text classification</h1>
    <p>
          Posted on Fri 17 November 2017 in <a href="/category/blog.html">blog</a>


    </p>
  </header>


  <div>
    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style><html><body><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I've been working on text classification recently. I've found keras to be a quite good high-level language and great for learning different neural network architectures. In this notebook I will examine Tweet classification using CNN and LSTM model architechtures. While CNNs are widely used in Computer Vision, I saw a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.717.298&amp;rep=rep1&amp;type=pdf">paper</a>, and a <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2760764.pdf">poster</a> describing CNNs for use in text classification. Also, I heard at MLConf2017 that Facebook recently switched to using 2017 because it gave an order magnitude increase in speed. LSTMs on the other hand are sequence-based models, which account for recency of contextual data, and so are well suited to text modeling. For an excellent intro, see this <a href="http://blog.echen.me/2017/05/30/exploring-lstms/">post</a> by Edwin Chen.</p>
<p>The data I'm using for this is the "distant labeled" Twitter <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjK37ak7sbXAhUM72MKHfr4CmcQFggoMAA&amp;url=http%3A%2F%2Fwww.yuefly.com%2FPublic%2FFiles%2F2017-03-07%2F58beb0822faef.pdf&amp;usg=AOvVaw12_owMHOcoTXfEgLmUHbb5">dataset</a>. This is a set of 1.6 million tweets that all contain some emoticon. Thus, positive emoticons are a proxy for positive sentiment, and vice versa for negative sentiment. As such, it is cheap to get training data and potentially good enough for getting useful sentiment training data. I've tested this, and while not shown here, it generalizes very well across datasets to even now when nobody uses :) or :-( to convey their emotions.</p>
<p>I've tried to include practical tips from my experience, since there are enough tutorials on keras already.</p>
<p><em>Lessons learned</em></p>
<ul>
<li>weight normalization matters</li>
<li>the right data pre-processing can make huge differences</li>
<li>callbacks are essential for model benchmarking and testing</li>
<li>get a working model and iterate!</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="import-dependencies">import dependencies<a class="anchor-link" href="#import-dependencies">¶</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">MaxPooling1D</span><span class="p">,</span> <span class="n">Convolution1D</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span>\
        <span class="n">Conv2D</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">concatenate</span><span class="p">,</span> <span class="n">merge</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Bidirectional</span>
<span class="kn">from</span> <span class="nn">keras.layers.merge</span> <span class="kn">import</span> <span class="n">Concatenate</span>
<span class="kn">from</span> <span class="nn">keras.utils.np_utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.layers.convolutional</span> <span class="kn">import</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">adam</span><span class="p">,</span> <span class="n">Adagrad</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">keras.layers.normalization</span> <span class="kn">import</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">History</span><span class="p">,</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">Callback</span><span class="p">,</span> <span class="n">CSVLogger</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="kn">import</span> <span class="nn">pydot</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>Using Theano backend.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">import</span> <span class="nn">keras</span>
<span class="k">print</span> <span class="n">keras</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>2.1.1
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="text-pre-processing-funtions">text pre-processing funtions<a class="anchor-link" href="#text-pre-processing-funtions">¶</a></h4><p>I found these are quite helpful for improving performance. If not included, the vocabulary size gets too big, since each username or url is counted as a unique word</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">standardize_html</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
    <span class="sd">"""replace all http links with 'URL'"""</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"http\S+"</span><span class="p">,</span> <span class="s2">"URL"</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">standardize_www</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
    <span class="sd">"""replace all www links with 'URL'"""</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"www\S+"</span><span class="p">,</span> <span class="s2">"URL"</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">standardize_mentions</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
    <span class="sd">"""replace all mentions with 'twittermention'"""</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"(?&lt;=^|(?&lt;=[^a-zA-Z0-9-_\.]))@([A-Za-z_]+[A-Za-z0-9_]+)"</span><span class="p">,</span> <span class="s2">"twittermention"</span><span class="p">,</span> <span class="n">tweet</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>define  dimensions for input data</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">glove_vec_size</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># dimension of pre-trained glove vector</span>
<span class="n">top_n_words_to_process</span> <span class="o">=</span> <span class="mi">100000</span> <span class="c1"># number of words to keep in tokenizer</span>
<span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># max word length per tweet</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>define data dirs</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">glove_dir</span> <span class="o">=</span> <span class="s1">'../glove_pretrained/glove.twitter.27B.200d.txt'</span>
<span class="n">labeled_twts_dir</span> <span class="o">=</span> <span class="s1">'./'</span>

<span class="k">def</span> <span class="nf">loadGloveModel</span><span class="p">(</span><span class="n">gloveFile</span><span class="p">):</span>
    <span class="sd">"""load pre-trained glove vector into memory as a dictionary"""</span>
    <span class="k">print</span> <span class="s2">"Loading Glove Model"</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">gloveFile</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">splitLine</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">splitLine</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">splitLine</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">model</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>
    <span class="k">print</span> <span class="s2">"Done."</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">),</span><span class="s2">" words loaded!"</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">glove_word_vecs</span> <span class="o">=</span> <span class="n">loadGloveModel</span><span class="p">(</span><span class="n">glove_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Loading Glove Model
Done. 1193515  words loaded!
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="data-processing">data processing<a class="anchor-link" href="#data-processing">¶</a></h4><p>Split data to train and test, then clean the text. Note that I'm only using a fraction of the tweets (50,000), with more time and GPU power, scaling to much larger sizes would be feasible</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'lab_twts_1million.csv'</span><span class="p">,</span><span class="n">error_bad_lines</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># labeled data</span>
<span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1">#df_sub = df.sample(frac=0.001,random_state=42)</span>
<span class="n">df_sub</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">),</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="k">print</span> <span class="s1">'subsampled training examples: </span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">df_sub</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df_sub</span><span class="p">[</span><span class="s1">'normed_text'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_sub</span><span class="p">[</span><span class="s1">'SentimentText'</span><span class="p">]</span>\
    <span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">standardize_mentions</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">standardize_html</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">standardize_www</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df_sub</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">training_texts</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">'normed_text'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">training_labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">'Sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>Skipping line 8836: expected 4 fields, saw 5

Skipping line 535882: expected 4 fields, saw 7

</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>subsampled training examples: 50000
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Convert-input-tweets-to-padded-sequences">Convert input tweets to padded sequences<a class="anchor-link" href="#Convert-input-tweets-to-padded-sequences">¶</a></h4><p>Keras expects a sequence, e.g., [0,0,0,153,48,...,]. To format the data appropriately, fit a tokenizer to the input tweets, then transform them. To ensure all data is of the same dimensions, pad the front of the sequences with zeros until all tweets are length 50.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">top_n_words_to_process</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">training_texts</span><span class="p">)</span>
<span class="n">raw_train_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">training_texts</span><span class="p">)</span>

<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="n">training_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">raw_train_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Build-embedding-matrix">Build embedding matrix<a class="anchor-link" href="#Build-embedding-matrix">¶</a></h4><p>The embedding matrix will be used to bake the glove vector into the keras <code>Embedding</code> layer. If the word is not in the pre-defined GloVe dictionary, then set it to be a vector of all zeros</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">glove_vec_size</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">glove_word_vecs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># words not found in embedding index will be all-zeros.</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Scale-the-embedding-matrix">Scale the embedding matrix<a class="anchor-link" href="#Scale-the-embedding-matrix">¶</a></h4><p>This step is helpful in part because out-of-vocabulary words will be equal to the mean for each dimension, since they have all been mean centered. Also, <a href="https://www.quora.com/Should-I-do-normalization-to-word-embeddings-from-word2vec-if-I-want-to-do-semantic-tasks">others</a> have found this to improve performance on nlp tasks.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">embedding_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">embedding_mean_centered</span> <span class="o">=</span> <span class="n">embedding_matrix</span> <span class="o">-</span> <span class="n">embedding_mean</span>
<span class="n">embedding_stdev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">embedding_mean_centered</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">normed_embedding_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">embedding_mean_centered</span><span class="p">,</span><span class="n">embedding_stdev</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Define-the-Embedding-layer">Define the Embedding layer<a class="anchor-link" href="#Define-the-Embedding-layer">¶</a></h4><p>This is the first layer for both the CNN and LSTM network. It incorporates the glove vector and has the necessary dimensions to define the computation graph</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [10]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># define word embedding layer for subsequent plug-and-play in model architectures</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">output_dim</span><span class="o">=</span><span class="n">glove_vec_size</span><span class="p">,</span>
                            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">normed_embedding_mat</span><span class="p">],</span>
                            <span class="n">input_length</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">,</span>
                            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">'embedding layer'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Define-callbacks">Define callbacks<a class="anchor-link" href="#Define-callbacks">¶</a></h4><p>Callbacks are a set of classes in keras that allow various interactions during training. These can be especially useful for saving, logging, etc., especially when training can be quite time consuming</p>
<p>Use <code>EarlyStopping</code> to prevent needless training once the model has begun overfitting. This stops training after the loss on the validation set has not improved for 3 consecutive epochs. Note that if you are trying to optimize for perforance here it would probably be best to 1) overfit the data to &gt;99% accuracy on the training set to show your model is sufficiently complex and <em>then</em> 2) introduce regularization to reduce model variance</p>
<p>Use <code>CSVLogger</code> to store all accuracy metrics to file</p>
<p>You can save the best model to file using <code>ModelCheckpoint</code></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [11]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span><span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">csv_logger</span> <span class="o">=</span> <span class="n">CSVLogger</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">'cnn_model_initial'</span><span class="o">+</span><span class="s1">'.csv'</span><span class="p">)</span>
<span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">'cnn_model_initial'</span><span class="o">+</span><span class="s1">'.hdf5'</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-architecture-for-the-CNN-network">Define architecture for the CNN network<a class="anchor-link" href="#Define-architecture-for-the-CNN-network">¶</a></h3><p>this is loosely based on the inception architecture described in <a href="https://www.youtube.com/watch?v=VxhSouuSZDY">this video</a>, where the full model is <a href="https://arxiv.org/abs/1602.07261">here</a>. In order to implement this architecture, we need to use the functional API to accomodate the non-sequential graph.</p>
<p>The idea to this architecture is that multiple layers of convolution and pooling are able to learn higher order features. Then you can concatenate all layers together in order to maintain higher- and lower order features.</p>
<p>You may be wondering what a 1x1 convolution is. <a href="https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network">Functionally</a>, these reduce dimensionality and introduce a ReLu operation.</p>
<p>Additionally, batch norm is a helpful <a href="https://www.quora.com/Why-does-batch-normalization-help">internal normalization step</a> and the for help on how to use it in practice, see <a href="https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout-in-tensorflow">here</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [12]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># input data as word embeddings</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">'input'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1"># define serial and parallel convolutional layers</span>
<span class="n">first_1x1</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">first_1x1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>

<span class="n">second_5x5</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">second_5x5</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_5x5</span><span class="p">)</span>
<span class="n">second_3x3</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">second_3x3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_3x3</span><span class="p">)</span>

<span class="n">first_pool</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">first_pool</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">first_pool</span><span class="p">)</span>
<span class="n">second_1x1</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_pool</span><span class="p">)</span>
<span class="n">second_1x1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_1x1</span><span class="p">)</span>

<span class="n">flat_first_1x1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">flat_second_5x5</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_5x5</span><span class="p">)</span>
<span class="n">flat_second_3x3</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_3x3</span><span class="p">)</span>
<span class="n">flat_second_1x1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_1x1</span><span class="p">)</span>

<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">flat_first_1x1</span><span class="p">,</span><span class="n">flat_second_5x5</span><span class="p">,</span><span class="n">flat_second_3x3</span><span class="p">,</span><span class="n">flat_second_1x1</span><span class="p">])</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">batch_normed</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">'hidden_layer'</span><span class="p">)(</span><span class="n">batch_normed</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">training_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">cnn_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Show-the-CNN-architecture">Show the CNN architecture<a class="anchor-link" href="#Show-the-CNN-architecture">¶</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [13]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">cnn_model</span><span class="p">,</span><span class="n">to_file</span><span class="o">=</span><span class="s1">'cnn_initial_architecture.png'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">'model.png'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABKUAAAY/CAYAAABS305HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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" />
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="fit-the-CNN-model-on-training-data">fit the CNN model on training data<a class="anchor-link" href="#fit-the-CNN-model-on-training-data">¶</a></h4><p>and store relevant files</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [14]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="o">%%</span><span class="k">time</span>
cnn_model.fit(training_seq, training_labels,\
            epochs=200, batch_size=256,validation_split=0.2,callbacks=[csv_logger,early_stopping,checkpointer],\
            verbose=0)
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 00001: val_loss improved from inf to 0.66872, saving model to cnn_model_initial.hdf5
Epoch 00002: val_loss improved from 0.66872 to 0.56254, saving model to cnn_model_initial.hdf5
Epoch 00003: val_loss improved from 0.56254 to 0.51160, saving model to cnn_model_initial.hdf5
Epoch 00004: val_loss improved from 0.51160 to 0.48894, saving model to cnn_model_initial.hdf5
Epoch 00005: val_loss improved from 0.48894 to 0.47835, saving model to cnn_model_initial.hdf5
Epoch 00006: val_loss improved from 0.47835 to 0.46669, saving model to cnn_model_initial.hdf5
Epoch 00007: val_loss did not improve
Epoch 00008: val_loss improved from 0.46669 to 0.46147, saving model to cnn_model_initial.hdf5
Epoch 00009: val_loss improved from 0.46147 to 0.46056, saving model to cnn_model_initial.hdf5
Epoch 00010: val_loss improved from 0.46056 to 0.45561, saving model to cnn_model_initial.hdf5
Epoch 00011: val_loss did not improve
Epoch 00012: val_loss did not improve
CPU times: user 48min 23s, sys: 6min 57s, total: 55min 21s
Wall time: 43min 8s
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[14]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x1588811d0&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It took a total of 43 minutes to train 12 epochs for the CNN model. We will come back to check the performance afterswitching gears to the LSTM model</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="LSTM-model">LSTM model<a class="anchor-link" href="#LSTM-model">¶</a></h1><p>LSTM models are more complex and slower, but suited nicely to text data. A couple things to note. First is that I'm using bidirectional LSTMs. Consider the sentence: "I'm going to get in my ..... and go to the store. Reading this sentence forward tells you that ..... is likely something you can get into, and reading it backwards suggests ..... can take you to the store. The bidirectionality captures this.</p>
<p>To use consecutive LSTM sequences, the first LSTM must return a sequence as the output. This enables <a href="https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms">stacking</a> which allow for increased model complexity.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [15]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">'lstm_model_initial'</span><span class="o">+</span><span class="s1">'.hdf5'</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">csv_logger</span> <span class="o">=</span> <span class="n">CSVLogger</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">'lstm_model_initial'</span><span class="o">+</span><span class="s1">'.csv'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [16]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="n">batch_normed</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">batch_normed</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">training_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="fit-the-LSTM-model">fit the LSTM model<a class="anchor-link" href="#fit-the-LSTM-model">¶</a></h4><p>and store pertinent files</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [17]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="o">%%</span><span class="k">time</span>
lstm_text_model.fit(training_seq, training_labels,\
            epochs=200, batch_size=256,validation_split=0.2,callbacks=[early_stopping,csv_logger,checkpointer],\
            verbose=0)
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 00001: val_loss improved from inf to 0.49246, saving model to lstm_model_initial.hdf5
Epoch 00002: val_loss improved from 0.49246 to 0.45506, saving model to lstm_model_initial.hdf5
Epoch 00003: val_loss did not improve
Epoch 00004: val_loss did not improve
CPU times: user 1h 28min 41s, sys: 15min 32s, total: 1h 44min 13s
Wall time: 1h 28min 16s
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[17]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x16ec22f10&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The LSTM model took substantially longer than the CNN model, about twice as much total time elapsed for one third the number of training epochs.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="visualize-the-LSTM-architecture">visualize the LSTM architecture<a class="anchor-link" href="#visualize-the-LSTM-architecture">¶</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [18]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">lstm_text_model</span><span class="p">,</span><span class="n">to_file</span><span class="o">=</span><span class="s1">'lstm_initial_architecture.png'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">'lstm_initial_architecture.png'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAp4AAAPkCAYAAAAAuD6AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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=
" />
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-evaluation">Model evaluation<a class="anchor-link" href="#Model-evaluation">¶</a></h2><p>Now we compare the performance of the two models. First, pull the best performing models back into memory</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [62]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">lstm_best</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">'lstm_model_initial.hdf5'</span><span class="p">)</span>
<span class="n">cnn_best</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">'cnn_model_initial.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>process the testing tweets to padded sequences</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [63]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">testing_texts</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'normed_text'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">testing_labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s1">'Sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">raw_test_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">testing_texts</span><span class="p">)</span>
<span class="n">testing_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">raw_test_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>then predict the labels</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [64]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">lstm_preds</span> <span class="o">=</span> <span class="n">lstm_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testing_seq</span><span class="p">)</span>
<span class="n">cnn_preds</span> <span class="o">=</span> <span class="n">cnn_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testing_seq</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [65]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">collapse_one_hot</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="sd">"""For checking accuracy of multiclass classifiers (e.g., neural nets) </span>
<span class="sd">    Take binary classification with two columns and collapse to a single column. </span>
<span class="sd">    Returns `0` for the first class and `1` for the second class"""</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>convert to binary for accuracy scoring</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [66]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">lstm_preds_binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">collapse_one_hot</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lstm_preds</span><span class="p">)</span>
<span class="n">cnn_preds_binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">collapse_one_hot</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cnn_preds</span><span class="p">)</span>
<span class="n">test_labels_normed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">collapse_one_hot</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">testing_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [74]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">lstm_acc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_labels_normed</span><span class="p">,</span><span class="n">lstm_preds_binary</span><span class="p">)</span>
<span class="n">cnn_acc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_labels_normed</span><span class="p">,</span><span class="n">cnn_preds_binary</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">"LSTM model accuracy: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">CNN model accuracy: </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">lstm_acc</span><span class="p">,</span><span class="n">cnn_acc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>LSTM model accuracy: 0.7918
CNN model accuracy: 0.7946
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>the accuracies are essentially the same, with the CNN model slightly outperforming the LSTM</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="plot-ROC-curve-for-each-model">plot ROC curve for each model<a class="anchor-link" href="#plot-ROC-curve-for-each-model">¶</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [69]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">fpr1</span><span class="p">,</span> <span class="n">tpr1</span><span class="p">,</span> <span class="n">thresholds1</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">test_labels_normed</span><span class="p">,</span> <span class="n">lstm_preds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fpr2</span><span class="p">,</span> <span class="n">tpr2</span><span class="p">,</span> <span class="n">thresholds2</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">test_labels_normed</span><span class="p">,</span> <span class="n">cnn_preds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [72]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tpr1</span><span class="p">,</span> <span class="n">fpr1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'lstm'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tpr2</span><span class="p">,</span> <span class="n">fpr2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'cnn'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'navy'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'False Positive Rate'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True Positive Rate'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Receiver operating characteristic'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">"lower right"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAdEAAAGBCAYAAAA5TYtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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" />
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusions">Conclusions<a class="anchor-link" href="#Conclusions">¶</a></h1><p>I hope you found this post helpful. In it I showed a couple of architectures that I spent just a bit of time optimizing. Both models acheived ~79% accuracy, even when trained with only 50,000 examples. I don't show it here, but I've found these accuracy metrics to be maintained for other datasets, in different domains, and from a more recent time, when people don't use emoticons as much :( with gifs, emojis, videos, etc.</p>
<p>The CNN trained twice about twice as fast and performed still slightly better than the LSTM. Since the data size is so small, this isn't terribly surprising. With more data, the LSTM may flex its complexity to outperform the CNN. Another thing to note is that I did not do any hyperparameter optimization. Take a look at all those dropouts, for example. Perhaps the regularization could be improved.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="To-do">To do<a class="anchor-link" href="#To-do">¶</a></h1><ul>
<li>Compare the training curves of LSTM and CNN</li>
<li>Compare the accuracy on the test data</li>
<li>summarize stuff</li>
<li>other tips: hyperparameter optimization</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [33]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[33]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array([[ 1.,  0.],
       [ 1.,  0.],
       [ 0.,  1.],
       ..., 
       [ 0.,  1.],
       [ 0.,  1.],
       [ 1.,  0.]])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [17]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Process-text-features">Process text features<a class="anchor-link" href="#Process-text-features">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [18]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#texts = df_train['SentimentText'] # read in tweets</span>
<span class="n">training_texts</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">'SentimentText'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="c1"># read in tweets</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [19]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">training_labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">'Sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Tokenize-and-standardize-data">Tokenize and standardize data<a class="anchor-link" href="#Tokenize-and-standardize-data">¶</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [20]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">top_n_words_to_process</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">training_texts</span><span class="p">)</span>
<span class="n">raw_train_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">training_texts</span><span class="p">)</span>

<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> unique tokens.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>

<span class="n">training_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">raw_train_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">)</span> <span class="c1"># append zeros to make tweet length consistent</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'Shape of data tensor:'</span><span class="p">,</span> <span class="n">training_seq</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Shape of label tensor:'</span><span class="p">,</span> <span class="n">training_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 4919 unique tokens.
('Shape of data tensor:', (1421, 50))
('Shape of label tensor:', (1421, 2))
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Build-look-up-for-translating-words-to-word-embeddings">Build look-up for translating words to word embeddings<a class="anchor-link" href="#Build-look-up-for-translating-words-to-word-embeddings">¶</a></h4><p>scripts adapted from <a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">this keras blog post</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [23]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">glove_vec_size</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">glove_word_vecs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># words not found in embedding index will be all-zeros.</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [24]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">normed_embedding_mat</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [25]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">normed_embedding_mat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[25]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(4920, 200)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="define-word-embedding-layer-for-subsequent-plug-and-play-in-model-architecture">define word embedding layer for subsequent plug-and-play in model architecture<a class="anchor-link" href="#define-word-embedding-layer-for-subsequent-plug-and-play-in-model-architecture">¶</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [26]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">output_dim</span><span class="o">=</span><span class="n">glove_vec_size</span><span class="p">,</span>
<span class="c1">#                            weights=[embedding_matrix],</span>
                            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">normed_embedding_mat</span><span class="p">],</span>
                            <span class="n">input_length</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">,</span>
                            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [27]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)</span>
<span class="c1">#sgd = SGD(lr=0.01, decay=1e-3, momentum=0.3, nesterov=True)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [29]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">rand_ix</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_labels</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">rand_ix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="First-try-a-CNN">First try a CNN<a class="anchor-link" href="#First-try-a-CNN">¶</a></h1><p>Based on a <a href="http://www.aclweb.org/anthology/D14-1181">paper</a> using CNNs for sentence classification. Also saw a <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2760764.pdf">poster</a> at a Stanford poster session CNNs were used for effective tweet classification. Additionally, CNNs are faster and these analyseis are limited to laptops.
The model architechture I chose to use is based on <a href="https://www.youtube.com/watch?v=VxhSouuSZDY&amp;t=2s">this</a> description of Inception</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [30]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># CONV/FC -&gt; BatchNorm -&gt; ReLu(or other activation) -&gt; Dropout -&gt; CONV/FC -&gt;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [39]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># input data as word embeddings</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1"># define serial and parallel convolutional layers</span>
<span class="n">first_1x1</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">first_1x1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>

<span class="n">second_5x5</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">second_5x5</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_5x5</span><span class="p">)</span>
<span class="n">second_3x3</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">second_3x3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_3x3</span><span class="p">)</span>

<span class="n">first_pool</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">first_pool</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">first_pool</span><span class="p">)</span>
<span class="n">second_1x1</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_pool</span><span class="p">)</span>
<span class="n">second_1x1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_1x1</span><span class="p">)</span>

<span class="n">flat_first_1x1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">flat_second_5x5</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_5x5</span><span class="p">)</span>
<span class="n">flat_second_3x3</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_3x3</span><span class="p">)</span>
<span class="n">flat_second_1x1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_1x1</span><span class="p">)</span>

<span class="c1"># combine inception layers and manual-derived features</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">flat_first_1x1</span><span class="p">,</span><span class="n">flat_second_5x5</span><span class="p">,</span><span class="n">flat_second_3x3</span><span class="p">,</span><span class="n">flat_second_1x1</span><span class="p">])</span>
<span class="c1">#merged_flat = merge([flat_first_1x1,flat_second_5x5,flat_second_3x3,flat_second_1x1,hidden_layer],mode='concat')</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">training_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>

<span class="n">my_history</span> <span class="o">=</span> <span class="n">LossHistory</span><span class="p">()</span>

<span class="n">cnn_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">cnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_seq</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span> <span class="n">training_labels</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">my_history</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1136 samples, validate on 285 samples
Epoch 1/1
1136/1136 [==============================] - 4s - loss: 1.0622 - acc: 0.5106 - val_loss: 0.6881 - val_acc: 0.5544
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[39]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x14bcc7490&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [41]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">my_history</span><span class="o">.</span><span class="n">losses</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[41]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>[array(1.166055679321289, dtype=float32),
 array(1.2242510318756104, dtype=float32),
 array(1.8228983879089355, dtype=float32),
 array(1.2668801546096802, dtype=float32),
 array(1.1044981479644775, dtype=float32),
 array(0.6815330386161804, dtype=float32),
 array(0.7793344855308533, dtype=float32),
 array(0.7143323421478271, dtype=float32),
 array(0.762489378452301, dtype=float32)]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># input data as word embeddings</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="c1">#embedded_sequences = Dropout(0.5)(embedded_sequences)</span>

<span class="c1"># define serial and parallel convolutional layers</span>
<span class="n">first_1x1</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">first_1x1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>

<span class="n">second_5x5</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">second_5x5</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_5x5</span><span class="p">)</span>
<span class="n">second_3x3</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">second_3x3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_3x3</span><span class="p">)</span>

<span class="n">first_pool</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">first_pool</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">first_pool</span><span class="p">)</span>
<span class="n">second_1x1</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">first_pool</span><span class="p">)</span>
<span class="n">second_1x1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">second_1x1</span><span class="p">)</span>

<span class="n">flat_first_1x1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">first_1x1</span><span class="p">)</span>
<span class="n">flat_second_5x5</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_5x5</span><span class="p">)</span>
<span class="n">flat_second_3x3</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_3x3</span><span class="p">)</span>
<span class="n">flat_second_1x1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">second_1x1</span><span class="p">)</span>

<span class="c1"># combine inception layers and manual-derived features</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">flat_first_1x1</span><span class="p">,</span><span class="n">flat_second_5x5</span><span class="p">,</span><span class="n">flat_second_3x3</span><span class="p">,</span><span class="n">flat_second_1x1</span><span class="p">])</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="c1">#merged_flat = merge([flat_first_1x1,flat_second_5x5,flat_second_3x3,flat_second_1x1,hidden_layer],mode='concat')</span>
<span class="c1">#merged_flat = Dropout(0.5)(merged_flat)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>

<span class="n">cnn_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">cnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_seq</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span> <span class="n">training_labels</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="LSTM-sentiment-classifier">LSTM sentiment classifier<a class="anchor-link" href="#LSTM-sentiment-classifier">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [113]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers.normalization</span> <span class="kn">import</span> <span class="n">BatchNormalization</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># CONV/FC -&gt; BatchNorm -&gt; ReLu(or other activation) -&gt; Dropout -&gt; CONV/FC -&gt;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [144]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="c1">#LSTM_layer = Dropout(0.5)(LSTM_layer)</span>

<span class="n">normed</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">normed</span><span class="p">)</span> <span class="c1"># optional hidden layer</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>

<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normed_twts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1136 samples, validate on 285 samples
Epoch 1/200
1136/1136 [==============================] - 18s - loss: 1.1436 - acc: 0.4921 - val_loss: 0.6987 - val_acc: 0.4947
Epoch 2/200
1136/1136 [==============================] - 4s - loss: 1.0335 - acc: 0.4965 - val_loss: 0.6959 - val_acc: 0.5018
Epoch 3/200
1136/1136 [==============================] - 4s - loss: 0.8948 - acc: 0.5590 - val_loss: 0.6885 - val_acc: 0.5158
Epoch 4/200
1136/1136 [==============================] - 4s - loss: 0.8356 - acc: 0.5687 - val_loss: 0.6846 - val_acc: 0.5123
Epoch 5/200
1136/1136 [==============================] - 4s - loss: 0.7937 - acc: 0.5634 - val_loss: 0.6806 - val_acc: 0.5298
Epoch 6/200
1136/1136 [==============================] - 4s - loss: 0.7552 - acc: 0.5713 - val_loss: 0.6763 - val_acc: 0.5754
Epoch 7/200
1136/1136 [==============================] - 4s - loss: 0.7563 - acc: 0.5783 - val_loss: 0.6739 - val_acc: 0.5719
Epoch 8/200
1136/1136 [==============================] - 5s - loss: 0.7211 - acc: 0.5907 - val_loss: 0.6720 - val_acc: 0.5895
Epoch 9/200
1136/1136 [==============================] - 5s - loss: 0.7068 - acc: 0.6074 - val_loss: 0.6712 - val_acc: 0.5930
Epoch 10/200
1136/1136 [==============================] - 5s - loss: 0.6931 - acc: 0.6100 - val_loss: 0.6695 - val_acc: 0.6070
Epoch 11/200
1136/1136 [==============================] - 5s - loss: 0.7027 - acc: 0.6144 - val_loss: 0.6689 - val_acc: 0.5930
Epoch 12/200
1136/1136 [==============================] - 5s - loss: 0.6989 - acc: 0.6039 - val_loss: 0.6665 - val_acc: 0.6035
Epoch 13/200
1136/1136 [==============================] - 5s - loss: 0.6769 - acc: 0.6092 - val_loss: 0.6635 - val_acc: 0.6105
Epoch 14/200
1136/1136 [==============================] - 5s - loss: 0.6659 - acc: 0.6171 - val_loss: 0.6606 - val_acc: 0.6246
Epoch 15/200
1136/1136 [==============================] - 5s - loss: 0.6394 - acc: 0.6329 - val_loss: 0.6574 - val_acc: 0.6281
Epoch 16/200
1136/1136 [==============================] - 5s - loss: 0.6493 - acc: 0.6364 - val_loss: 0.6567 - val_acc: 0.6175
Epoch 17/200
1136/1136 [==============================] - 4s - loss: 0.6399 - acc: 0.6426 - val_loss: 0.6552 - val_acc: 0.6211
Epoch 18/200
1136/1136 [==============================] - 5s - loss: 0.6473 - acc: 0.6197 - val_loss: 0.6547 - val_acc: 0.6246
Epoch 19/200
1136/1136 [==============================] - 4s - loss: 0.6393 - acc: 0.6232 - val_loss: 0.6518 - val_acc: 0.6386
Epoch 20/200
1136/1136 [==============================] - 5s - loss: 0.6185 - acc: 0.6602 - val_loss: 0.6492 - val_acc: 0.6491
Epoch 21/200
1136/1136 [==============================] - 5s - loss: 0.6396 - acc: 0.6276 - val_loss: 0.6468 - val_acc: 0.6456
Epoch 22/200
1136/1136 [==============================] - 5s - loss: 0.6212 - acc: 0.6461 - val_loss: 0.6441 - val_acc: 0.6526
Epoch 23/200
1136/1136 [==============================] - 4s - loss: 0.6101 - acc: 0.6673 - val_loss: 0.6422 - val_acc: 0.6491
Epoch 24/200
1136/1136 [==============================] - 5s - loss: 0.6245 - acc: 0.6417 - val_loss: 0.6405 - val_acc: 0.6491
Epoch 25/200
1136/1136 [==============================] - 5s - loss: 0.6106 - acc: 0.6725 - val_loss: 0.6362 - val_acc: 0.6526
Epoch 26/200
1136/1136 [==============================] - 4s - loss: 0.6208 - acc: 0.6567 - val_loss: 0.6336 - val_acc: 0.6421
Epoch 27/200
1136/1136 [==============================] - 4s - loss: 0.6112 - acc: 0.6620 - val_loss: 0.6318 - val_acc: 0.6526
Epoch 28/200
1136/1136 [==============================] - 5s - loss: 0.6208 - acc: 0.6540 - val_loss: 0.6281 - val_acc: 0.6561
Epoch 29/200
1136/1136 [==============================] - 5s - loss: 0.6233 - acc: 0.6549 - val_loss: 0.6270 - val_acc: 0.6561
Epoch 30/200
1136/1136 [==============================] - 5s - loss: 0.5881 - acc: 0.6919 - val_loss: 0.6279 - val_acc: 0.6561
Epoch 31/200
1136/1136 [==============================] - 5s - loss: 0.5900 - acc: 0.6813 - val_loss: 0.6255 - val_acc: 0.6561
Epoch 32/200
1136/1136 [==============================] - 5s - loss: 0.5886 - acc: 0.6884 - val_loss: 0.6211 - val_acc: 0.6491
Epoch 33/200
1136/1136 [==============================] - 4s - loss: 0.6091 - acc: 0.6629 - val_loss: 0.6209 - val_acc: 0.6491
Epoch 34/200
1136/1136 [==============================] - 5s - loss: 0.5897 - acc: 0.6937 - val_loss: 0.6183 - val_acc: 0.6561
Epoch 35/200
1136/1136 [==============================] - 5s - loss: 0.5835 - acc: 0.7060 - val_loss: 0.6172 - val_acc: 0.6632
Epoch 36/200
1136/1136 [==============================] - 5s - loss: 0.5935 - acc: 0.6866 - val_loss: 0.6155 - val_acc: 0.6632
Epoch 37/200
1136/1136 [==============================] - 5s - loss: 0.5810 - acc: 0.6778 - val_loss: 0.6157 - val_acc: 0.6632
Epoch 38/200
1136/1136 [==============================] - 5s - loss: 0.5934 - acc: 0.6796 - val_loss: 0.6145 - val_acc: 0.6596
Epoch 39/200
1136/1136 [==============================] - 5s - loss: 0.5679 - acc: 0.7051 - val_loss: 0.6136 - val_acc: 0.6632
Epoch 40/200
1136/1136 [==============================] - 5s - loss: 0.5689 - acc: 0.7060 - val_loss: 0.6145 - val_acc: 0.6561
Epoch 41/200
1136/1136 [==============================] - 4s - loss: 0.5729 - acc: 0.6928 - val_loss: 0.6142 - val_acc: 0.6667
Epoch 42/200
1136/1136 [==============================] - 5s - loss: 0.5673 - acc: 0.7007 - val_loss: 0.6137 - val_acc: 0.6807
Epoch 43/200
1136/1136 [==============================] - 5s - loss: 0.5737 - acc: 0.7033 - val_loss: 0.6146 - val_acc: 0.6807
Epoch 44/200
1136/1136 [==============================] - 4s - loss: 0.5828 - acc: 0.6901 - val_loss: 0.6159 - val_acc: 0.6772
Epoch 45/200
1136/1136 [==============================] - 4s - loss: 0.5743 - acc: 0.6963 - val_loss: 0.6124 - val_acc: 0.6807
Epoch 46/200
1136/1136 [==============================] - 4s - loss: 0.5772 - acc: 0.6893 - val_loss: 0.6087 - val_acc: 0.6702
Epoch 47/200
1136/1136 [==============================] - 4s - loss: 0.5684 - acc: 0.7042 - val_loss: 0.6063 - val_acc: 0.6667
Epoch 48/200
1136/1136 [==============================] - 4s - loss: 0.5798 - acc: 0.6963 - val_loss: 0.6063 - val_acc: 0.6807
Epoch 49/200
1136/1136 [==============================] - 5s - loss: 0.5827 - acc: 0.7025 - val_loss: 0.6048 - val_acc: 0.6842
Epoch 50/200
1136/1136 [==============================] - 4s - loss: 0.5979 - acc: 0.6690 - val_loss: 0.6062 - val_acc: 0.6807
Epoch 51/200
1136/1136 [==============================] - 4s - loss: 0.5633 - acc: 0.7069 - val_loss: 0.6018 - val_acc: 0.6737
Epoch 52/200
1136/1136 [==============================] - 5s - loss: 0.5688 - acc: 0.7086 - val_loss: 0.6010 - val_acc: 0.6737
Epoch 53/200
1136/1136 [==============================] - 4s - loss: 0.5602 - acc: 0.7060 - val_loss: 0.6007 - val_acc: 0.6807
Epoch 54/200
1136/1136 [==============================] - 4s - loss: 0.5859 - acc: 0.6972 - val_loss: 0.5998 - val_acc: 0.6842
Epoch 55/200
1136/1136 [==============================] - 4s - loss: 0.5544 - acc: 0.7245 - val_loss: 0.5991 - val_acc: 0.6807
Epoch 56/200
1136/1136 [==============================] - 4s - loss: 0.5553 - acc: 0.7280 - val_loss: 0.5980 - val_acc: 0.6737
Epoch 57/200
1136/1136 [==============================] - 5s - loss: 0.5566 - acc: 0.7201 - val_loss: 0.5990 - val_acc: 0.6772
Epoch 58/200
1136/1136 [==============================] - 6s - loss: 0.5754 - acc: 0.6875 - val_loss: 0.6003 - val_acc: 0.6702
Epoch 59/200
1136/1136 [==============================] - 6s - loss: 0.5509 - acc: 0.7174 - val_loss: 0.6010 - val_acc: 0.6772
Epoch 60/200
1136/1136 [==============================] - 5s - loss: 0.5604 - acc: 0.7174 - val_loss: 0.6019 - val_acc: 0.6912
Epoch 61/200
1136/1136 [==============================] - 5s - loss: 0.5577 - acc: 0.7183 - val_loss: 0.6022 - val_acc: 0.6912
Epoch 62/200
1136/1136 [==============================] - 5s - loss: 0.5407 - acc: 0.7298 - val_loss: 0.6037 - val_acc: 0.6912
Epoch 63/200
1136/1136 [==============================] - 5s - loss: 0.5357 - acc: 0.7236 - val_loss: 0.6031 - val_acc: 0.6912
Epoch 64/200
1136/1136 [==============================] - 5s - loss: 0.5413 - acc: 0.7201 - val_loss: 0.6001 - val_acc: 0.6877
Epoch 65/200
1136/1136 [==============================] - 5s - loss: 0.5637 - acc: 0.7130 - val_loss: 0.5990 - val_acc: 0.6947
Epoch 66/200
1136/1136 [==============================] - 4s - loss: 0.5577 - acc: 0.7121 - val_loss: 0.5989 - val_acc: 0.6912
Epoch 67/200
1136/1136 [==============================] - 4s - loss: 0.5776 - acc: 0.6945 - val_loss: 0.5967 - val_acc: 0.6912
Epoch 68/200
1136/1136 [==============================] - 5s - loss: 0.5560 - acc: 0.7254 - val_loss: 0.5980 - val_acc: 0.6877
Epoch 69/200
1136/1136 [==============================] - 4s - loss: 0.5383 - acc: 0.7139 - val_loss: 0.5975 - val_acc: 0.6877
Epoch 70/200
1136/1136 [==============================] - 5s - loss: 0.5559 - acc: 0.7086 - val_loss: 0.5958 - val_acc: 0.6842
Epoch 71/200
1136/1136 [==============================] - 4s - loss: 0.5638 - acc: 0.7060 - val_loss: 0.5926 - val_acc: 0.6842
Epoch 72/200
1136/1136 [==============================] - 5s - loss: 0.5518 - acc: 0.7245 - val_loss: 0.5904 - val_acc: 0.6842
Epoch 73/200
1136/1136 [==============================] - 4s - loss: 0.5416 - acc: 0.7095 - val_loss: 0.5905 - val_acc: 0.6807
Epoch 74/200
1136/1136 [==============================] - 5s - loss: 0.5399 - acc: 0.7271 - val_loss: 0.5907 - val_acc: 0.6737
Epoch 75/200
1136/1136 [==============================] - 4s - loss: 0.5251 - acc: 0.7394 - val_loss: 0.5922 - val_acc: 0.6737
Epoch 76/200
1136/1136 [==============================] - 5s - loss: 0.5297 - acc: 0.7350 - val_loss: 0.5924 - val_acc: 0.6737
Epoch 77/200
1136/1136 [==============================] - 4s - loss: 0.5278 - acc: 0.7342 - val_loss: 0.5915 - val_acc: 0.6807
Epoch 78/200
1136/1136 [==============================] - 5s - loss: 0.5427 - acc: 0.7289 - val_loss: 0.5912 - val_acc: 0.6912
Epoch 79/200
1136/1136 [==============================] - 4s - loss: 0.5426 - acc: 0.7262 - val_loss: 0.5918 - val_acc: 0.6947
Epoch 80/200
1136/1136 [==============================] - 5s - loss: 0.5371 - acc: 0.7491 - val_loss: 0.5922 - val_acc: 0.6772
Epoch 81/200
1136/1136 [==============================] - 4s - loss: 0.5400 - acc: 0.7201 - val_loss: 0.5912 - val_acc: 0.6842
Epoch 82/200
1136/1136 [==============================] - 5s - loss: 0.5382 - acc: 0.7280 - val_loss: 0.5902 - val_acc: 0.6807
Epoch 83/200
1136/1136 [==============================] - 4s - loss: 0.5439 - acc: 0.7227 - val_loss: 0.5879 - val_acc: 0.6772
Epoch 84/200
1136/1136 [==============================] - 5s - loss: 0.5234 - acc: 0.7421 - val_loss: 0.5876 - val_acc: 0.6877
Epoch 85/200
1136/1136 [==============================] - 5s - loss: 0.5367 - acc: 0.7306 - val_loss: 0.5898 - val_acc: 0.6772
Epoch 86/200
1136/1136 [==============================] - 5s - loss: 0.5187 - acc: 0.7438 - val_loss: 0.5950 - val_acc: 0.6807
Epoch 87/200
1136/1136 [==============================] - 5s - loss: 0.5396 - acc: 0.7465 - val_loss: 0.5961 - val_acc: 0.6842
Epoch 88/200
1136/1136 [==============================] - 5s - loss: 0.5159 - acc: 0.7535 - val_loss: 0.5939 - val_acc: 0.6912
Epoch 89/200
1136/1136 [==============================] - 5s - loss: 0.5257 - acc: 0.7447 - val_loss: 0.5930 - val_acc: 0.6877
Epoch 90/200
1136/1136 [==============================] - 5s - loss: 0.5364 - acc: 0.7306 - val_loss: 0.5905 - val_acc: 0.6877
Epoch 91/200
1136/1136 [==============================] - 5s - loss: 0.5155 - acc: 0.7456 - val_loss: 0.5882 - val_acc: 0.6912
Epoch 92/200
1136/1136 [==============================] - 5s - loss: 0.5306 - acc: 0.7280 - val_loss: 0.5876 - val_acc: 0.6947
Epoch 93/200
1136/1136 [==============================] - 5s - loss: 0.5251 - acc: 0.7456 - val_loss: 0.5850 - val_acc: 0.6947
Epoch 94/200
1136/1136 [==============================] - 5s - loss: 0.5178 - acc: 0.7403 - val_loss: 0.5872 - val_acc: 0.6877
Epoch 95/200
1136/1136 [==============================] - 4s - loss: 0.5346 - acc: 0.7254 - val_loss: 0.5868 - val_acc: 0.6947
Epoch 96/200
1136/1136 [==============================] - 5s - loss: 0.5083 - acc: 0.7526 - val_loss: 0.5878 - val_acc: 0.6947
Epoch 97/200
1136/1136 [==============================] - 5s - loss: 0.5228 - acc: 0.7377 - val_loss: 0.5882 - val_acc: 0.6912
Epoch 98/200
1136/1136 [==============================] - 5s - loss: 0.5282 - acc: 0.7570 - val_loss: 0.5864 - val_acc: 0.6947
Epoch 99/200
1136/1136 [==============================] - 5s - loss: 0.5205 - acc: 0.7421 - val_loss: 0.5848 - val_acc: 0.6947
Epoch 100/200
1136/1136 [==============================] - 5s - loss: 0.5112 - acc: 0.7386 - val_loss: 0.5854 - val_acc: 0.6912
Epoch 101/200
1136/1136 [==============================] - 5s - loss: 0.5150 - acc: 0.7271 - val_loss: 0.5850 - val_acc: 0.6947
Epoch 102/200
1136/1136 [==============================] - 5s - loss: 0.5089 - acc: 0.7632 - val_loss: 0.5856 - val_acc: 0.6912
Epoch 103/200
1136/1136 [==============================] - 5s - loss: 0.5106 - acc: 0.7509 - val_loss: 0.5842 - val_acc: 0.6912
Epoch 104/200
1136/1136 [==============================] - 5s - loss: 0.4963 - acc: 0.7702 - val_loss: 0.5858 - val_acc: 0.6877
Epoch 105/200
1136/1136 [==============================] - 5s - loss: 0.5309 - acc: 0.7139 - val_loss: 0.5854 - val_acc: 0.6877
Epoch 106/200
1136/1136 [==============================] - 5s - loss: 0.5043 - acc: 0.7526 - val_loss: 0.5834 - val_acc: 0.6877
Epoch 107/200
1136/1136 [==============================] - 5s - loss: 0.5246 - acc: 0.7350 - val_loss: 0.5827 - val_acc: 0.6912
Epoch 108/200
1136/1136 [==============================] - 5s - loss: 0.5049 - acc: 0.7667 - val_loss: 0.5816 - val_acc: 0.6912
Epoch 109/200
1136/1136 [==============================] - 5s - loss: 0.4840 - acc: 0.7623 - val_loss: 0.5826 - val_acc: 0.6842
Epoch 110/200
1136/1136 [==============================] - 5s - loss: 0.5056 - acc: 0.7474 - val_loss: 0.5809 - val_acc: 0.6877
Epoch 111/200
1136/1136 [==============================] - 5s - loss: 0.5089 - acc: 0.7333 - val_loss: 0.5815 - val_acc: 0.6877
Epoch 112/200
1136/1136 [==============================] - 5s - loss: 0.5224 - acc: 0.7544 - val_loss: 0.5813 - val_acc: 0.6877
Epoch 113/200
1136/1136 [==============================] - 5s - loss: 0.5200 - acc: 0.7403 - val_loss: 0.5825 - val_acc: 0.6877
Epoch 114/200
1136/1136 [==============================] - 5s - loss: 0.5132 - acc: 0.7623 - val_loss: 0.5798 - val_acc: 0.6947
Epoch 115/200
1136/1136 [==============================] - 5s - loss: 0.5051 - acc: 0.7474 - val_loss: 0.5767 - val_acc: 0.6982
Epoch 116/200
1136/1136 [==============================] - 5s - loss: 0.4940 - acc: 0.7562 - val_loss: 0.5795 - val_acc: 0.6982
Epoch 117/200
1136/1136 [==============================] - 5s - loss: 0.5105 - acc: 0.7421 - val_loss: 0.5803 - val_acc: 0.6982
Epoch 118/200
1136/1136 [==============================] - 5s - loss: 0.5022 - acc: 0.7553 - val_loss: 0.5801 - val_acc: 0.6947
Epoch 119/200
1136/1136 [==============================] - 5s - loss: 0.5038 - acc: 0.7518 - val_loss: 0.5794 - val_acc: 0.6947
Epoch 120/200
1136/1136 [==============================] - 4s - loss: 0.5143 - acc: 0.7535 - val_loss: 0.5789 - val_acc: 0.6912
Epoch 121/200
1136/1136 [==============================] - 5s - loss: 0.5110 - acc: 0.7456 - val_loss: 0.5766 - val_acc: 0.6912
Epoch 122/200
1136/1136 [==============================] - 5s - loss: 0.4981 - acc: 0.7641 - val_loss: 0.5777 - val_acc: 0.6877
Epoch 123/200
1136/1136 [==============================] - 4s - loss: 0.5119 - acc: 0.7500 - val_loss: 0.5781 - val_acc: 0.6912
Epoch 124/200
1136/1136 [==============================] - 5s - loss: 0.4828 - acc: 0.7799 - val_loss: 0.5791 - val_acc: 0.6912
Epoch 125/200
1136/1136 [==============================] - 5s - loss: 0.4942 - acc: 0.7491 - val_loss: 0.5773 - val_acc: 0.6912
Epoch 126/200
1136/1136 [==============================] - 4s - loss: 0.4867 - acc: 0.7667 - val_loss: 0.5787 - val_acc: 0.6912
Epoch 127/200
1136/1136 [==============================] - 5s - loss: 0.5063 - acc: 0.7553 - val_loss: 0.5785 - val_acc: 0.6982
Epoch 128/200
1136/1136 [==============================] - 6s - loss: 0.5048 - acc: 0.7482 - val_loss: 0.5763 - val_acc: 0.6982
Epoch 129/200
1136/1136 [==============================] - 5s - loss: 0.5229 - acc: 0.7280 - val_loss: 0.5731 - val_acc: 0.6912
Epoch 130/200
1136/1136 [==============================] - 5s - loss: 0.4976 - acc: 0.7579 - val_loss: 0.5716 - val_acc: 0.6912
Epoch 131/200
1136/1136 [==============================] - 5s - loss: 0.4793 - acc: 0.7667 - val_loss: 0.5715 - val_acc: 0.6947
Epoch 132/200
1136/1136 [==============================] - 5s - loss: 0.4895 - acc: 0.7438 - val_loss: 0.5716 - val_acc: 0.6947
Epoch 133/200
1136/1136 [==============================] - 5s - loss: 0.5028 - acc: 0.7491 - val_loss: 0.5734 - val_acc: 0.6947
Epoch 134/200
1136/1136 [==============================] - 5s - loss: 0.4887 - acc: 0.7632 - val_loss: 0.5754 - val_acc: 0.6947
Epoch 135/200
1136/1136 [==============================] - 4s - loss: 0.4996 - acc: 0.7570 - val_loss: 0.5759 - val_acc: 0.6982
Epoch 136/200
1136/1136 [==============================] - 4s - loss: 0.5016 - acc: 0.7579 - val_loss: 0.5755 - val_acc: 0.7018
Epoch 137/200
1136/1136 [==============================] - 5s - loss: 0.4689 - acc: 0.7729 - val_loss: 0.5780 - val_acc: 0.6947
Epoch 138/200
1136/1136 [==============================] - 5s - loss: 0.4979 - acc: 0.7465 - val_loss: 0.5801 - val_acc: 0.6982
Epoch 139/200
1136/1136 [==============================] - 5s - loss: 0.4654 - acc: 0.7746 - val_loss: 0.5821 - val_acc: 0.6982
Epoch 140/200
1136/1136 [==============================] - 5s - loss: 0.4688 - acc: 0.7720 - val_loss: 0.5847 - val_acc: 0.6982
Epoch 141/200
1136/1136 [==============================] - 5s - loss: 0.4683 - acc: 0.7720 - val_loss: 0.5836 - val_acc: 0.6947
Epoch 142/200
1136/1136 [==============================] - 5s - loss: 0.4666 - acc: 0.7949 - val_loss: 0.5842 - val_acc: 0.6982
Epoch 143/200
1136/1136 [==============================] - 5s - loss: 0.4781 - acc: 0.7790 - val_loss: 0.5835 - val_acc: 0.6982
Epoch 144/200
1136/1136 [==============================] - 5s - loss: 0.4720 - acc: 0.7782 - val_loss: 0.5835 - val_acc: 0.6982
Epoch 145/200
1136/1136 [==============================] - 5s - loss: 0.4672 - acc: 0.7729 - val_loss: 0.5828 - val_acc: 0.6982
Epoch 146/200
1136/1136 [==============================] - 5s - loss: 0.4707 - acc: 0.7738 - val_loss: 0.5836 - val_acc: 0.6982
Epoch 147/200
1136/1136 [==============================] - 5s - loss: 0.4584 - acc: 0.7808 - val_loss: 0.5844 - val_acc: 0.6982
Epoch 148/200
1136/1136 [==============================] - 5s - loss: 0.4744 - acc: 0.7641 - val_loss: 0.5862 - val_acc: 0.6982
Epoch 149/200
1136/1136 [==============================] - 5s - loss: 0.4591 - acc: 0.7764 - val_loss: 0.5843 - val_acc: 0.6947
Epoch 150/200
1136/1136 [==============================] - 5s - loss: 0.4671 - acc: 0.7826 - val_loss: 0.5827 - val_acc: 0.6912
Epoch 151/200
1136/1136 [==============================] - 5s - loss: 0.4695 - acc: 0.7694 - val_loss: 0.5821 - val_acc: 0.6877
Epoch 152/200
1136/1136 [==============================] - 5s - loss: 0.4813 - acc: 0.7597 - val_loss: 0.5798 - val_acc: 0.6912
Epoch 153/200
1136/1136 [==============================] - 5s - loss: 0.4538 - acc: 0.7746 - val_loss: 0.5808 - val_acc: 0.6982
Epoch 154/200
1136/1136 [==============================] - 5s - loss: 0.4422 - acc: 0.7843 - val_loss: 0.5822 - val_acc: 0.6947
Epoch 155/200
1136/1136 [==============================] - 5s - loss: 0.4647 - acc: 0.7729 - val_loss: 0.5832 - val_acc: 0.7018
Epoch 156/200
1136/1136 [==============================] - 4s - loss: 0.4498 - acc: 0.7940 - val_loss: 0.5840 - val_acc: 0.6982
Epoch 157/200
1136/1136 [==============================] - 5s - loss: 0.4687 - acc: 0.7808 - val_loss: 0.5849 - val_acc: 0.6947
Epoch 158/200
1136/1136 [==============================] - 5s - loss: 0.4676 - acc: 0.7817 - val_loss: 0.5853 - val_acc: 0.6947
Epoch 159/200
1136/1136 [==============================] - 5s - loss: 0.4802 - acc: 0.7526 - val_loss: 0.5834 - val_acc: 0.6947
Epoch 160/200
1136/1136 [==============================] - 5s - loss: 0.4470 - acc: 0.7808 - val_loss: 0.5832 - val_acc: 0.6947
Epoch 161/200
1136/1136 [==============================] - 4s - loss: 0.4579 - acc: 0.7835 - val_loss: 0.5831 - val_acc: 0.6982
Epoch 162/200
1136/1136 [==============================] - 4s - loss: 0.4605 - acc: 0.7835 - val_loss: 0.5852 - val_acc: 0.7018
Epoch 163/200
1136/1136 [==============================] - 4s - loss: 0.4596 - acc: 0.7729 - val_loss: 0.5851 - val_acc: 0.6982
Epoch 164/200
1136/1136 [==============================] - 4s - loss: 0.4844 - acc: 0.7773 - val_loss: 0.5817 - val_acc: 0.7018
Epoch 165/200
1136/1136 [==============================] - 5s - loss: 0.4486 - acc: 0.7729 - val_loss: 0.5811 - val_acc: 0.6982
Epoch 166/200
1136/1136 [==============================] - 4s - loss: 0.4458 - acc: 0.7861 - val_loss: 0.5809 - val_acc: 0.6982
Epoch 167/200
1136/1136 [==============================] - 4s - loss: 0.4436 - acc: 0.8028 - val_loss: 0.5815 - val_acc: 0.6947
Epoch 168/200
1136/1136 [==============================] - 5s - loss: 0.4625 - acc: 0.7711 - val_loss: 0.5804 - val_acc: 0.6982
Epoch 169/200
1136/1136 [==============================] - 5s - loss: 0.4455 - acc: 0.7861 - val_loss: 0.5814 - val_acc: 0.6947
Epoch 170/200
1136/1136 [==============================] - 4s - loss: 0.4546 - acc: 0.7923 - val_loss: 0.5824 - val_acc: 0.6982
Epoch 171/200
1136/1136 [==============================] - 5s - loss: 0.4646 - acc: 0.7896 - val_loss: 0.5823 - val_acc: 0.6947
Epoch 172/200
1136/1136 [==============================] - 5s - loss: 0.4483 - acc: 0.7905 - val_loss: 0.5827 - val_acc: 0.6982
Epoch 173/200
1136/1136 [==============================] - 4s - loss: 0.4588 - acc: 0.7887 - val_loss: 0.5814 - val_acc: 0.6947
Epoch 174/200
1136/1136 [==============================] - 5s - loss: 0.4442 - acc: 0.7949 - val_loss: 0.5801 - val_acc: 0.6982
Epoch 175/200
1136/1136 [==============================] - 5s - loss: 0.4459 - acc: 0.7879 - val_loss: 0.5814 - val_acc: 0.6982
Epoch 176/200
1136/1136 [==============================] - 5s - loss: 0.4457 - acc: 0.7808 - val_loss: 0.5823 - val_acc: 0.6982
Epoch 177/200
1136/1136 [==============================] - 5s - loss: 0.4478 - acc: 0.7967 - val_loss: 0.5835 - val_acc: 0.6912
Epoch 178/200
1136/1136 [==============================] - 4s - loss: 0.4410 - acc: 0.7940 - val_loss: 0.5844 - val_acc: 0.6947
Epoch 179/200
1136/1136 [==============================] - 5s - loss: 0.4311 - acc: 0.7949 - val_loss: 0.5865 - val_acc: 0.6912
Epoch 180/200
1136/1136 [==============================] - 4s - loss: 0.4475 - acc: 0.7861 - val_loss: 0.5892 - val_acc: 0.6947
Epoch 181/200
1136/1136 [==============================] - 4s - loss: 0.4392 - acc: 0.7861 - val_loss: 0.5893 - val_acc: 0.6912
Epoch 182/200
1136/1136 [==============================] - 5s - loss: 0.4436 - acc: 0.7949 - val_loss: 0.5895 - val_acc: 0.6947
Epoch 183/200
1136/1136 [==============================] - 4s - loss: 0.4441 - acc: 0.7896 - val_loss: 0.5895 - val_acc: 0.6912
Epoch 184/200
1136/1136 [==============================] - 4s - loss: 0.4186 - acc: 0.8028 - val_loss: 0.5900 - val_acc: 0.6912
Epoch 185/200
1136/1136 [==============================] - 5s - loss: 0.4371 - acc: 0.7861 - val_loss: 0.5894 - val_acc: 0.6912
Epoch 186/200
1136/1136 [==============================] - 5s - loss: 0.4222 - acc: 0.8072 - val_loss: 0.5896 - val_acc: 0.6947
Epoch 187/200
1136/1136 [==============================] - 5s - loss: 0.4226 - acc: 0.8081 - val_loss: 0.5924 - val_acc: 0.6912
Epoch 188/200
1136/1136 [==============================] - 4s - loss: 0.4351 - acc: 0.8063 - val_loss: 0.5952 - val_acc: 0.6947
Epoch 189/200
1136/1136 [==============================] - 5s - loss: 0.4152 - acc: 0.8028 - val_loss: 0.5965 - val_acc: 0.6947
Epoch 190/200
1136/1136 [==============================] - 4s - loss: 0.4424 - acc: 0.7940 - val_loss: 0.5968 - val_acc: 0.6982
Epoch 191/200
1136/1136 [==============================] - 5s - loss: 0.4273 - acc: 0.7975 - val_loss: 0.5959 - val_acc: 0.7018
Epoch 192/200
1136/1136 [==============================] - 4s - loss: 0.4098 - acc: 0.8081 - val_loss: 0.5949 - val_acc: 0.7053
Epoch 193/200
1136/1136 [==============================] - 5s - loss: 0.4270 - acc: 0.8037 - val_loss: 0.5922 - val_acc: 0.7053
Epoch 194/200
1136/1136 [==============================] - 5s - loss: 0.4252 - acc: 0.7931 - val_loss: 0.5916 - val_acc: 0.7053
Epoch 195/200
1136/1136 [==============================] - 5s - loss: 0.4131 - acc: 0.8169 - val_loss: 0.5918 - val_acc: 0.7018
Epoch 196/200
1136/1136 [==============================] - 4s - loss: 0.4274 - acc: 0.7958 - val_loss: 0.5932 - val_acc: 0.7053
Epoch 197/200
1136/1136 [==============================] - 5s - loss: 0.4288 - acc: 0.8081 - val_loss: 0.5945 - val_acc: 0.7053
Epoch 198/200
1136/1136 [==============================] - 5s - loss: 0.4057 - acc: 0.8169 - val_loss: 0.5977 - val_acc: 0.7018
Epoch 199/200
1136/1136 [==============================] - 5s - loss: 0.4112 - acc: 0.8090 - val_loss: 0.5992 - val_acc: 0.7088
Epoch 200/200
1136/1136 [==============================] - 5s - loss: 0.4091 - acc: 0.8046 - val_loss: 0.6011 - val_acc: 0.7088
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[144]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x1f501cdd0&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Try-stacked-LSTM">Try stacked LSTM<a class="anchor-link" href="#Try-stacked-LSTM">¶</a></h3><p>docs <a href="https://keras.io/layers/recurrent/">here</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [261]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="c1">#LSTM_layer = Dropout(0.5)(LSTM_layer)</span>

<span class="n">normed</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">normed</span><span class="p">)</span> <span class="c1"># optional hidden layer</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>

<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_seq</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1136 samples, validate on 285 samples
Epoch 1/200
1136/1136 [==============================] - 134s - loss: 1.0715 - acc: 0.4938 - val_loss: 0.6913 - val_acc: 0.5439
Epoch 2/200
1136/1136 [==============================] - 9s - loss: 1.0118 - acc: 0.5044 - val_loss: 0.6874 - val_acc: 0.5754
Epoch 3/200
1136/1136 [==============================] - 10s - loss: 0.9830 - acc: 0.5238 - val_loss: 0.6847 - val_acc: 0.5965
Epoch 4/200
1136/1136 [==============================] - 9s - loss: 1.0022 - acc: 0.4727 - val_loss: 0.6823 - val_acc: 0.6070
Epoch 5/200
1136/1136 [==============================] - 9s - loss: 0.9131 - acc: 0.5176 - val_loss: 0.6801 - val_acc: 0.6211
Epoch 6/200
1136/1136 [==============================] - 9s - loss: 0.8225 - acc: 0.5475 - val_loss: 0.6779 - val_acc: 0.6105
Epoch 7/200
1136/1136 [==============================] - 9s - loss: 0.8236 - acc: 0.5511 - val_loss: 0.6758 - val_acc: 0.6175
Epoch 8/200
1136/1136 [==============================] - 9s - loss: 0.8247 - acc: 0.5290 - val_loss: 0.6743 - val_acc: 0.6211
Epoch 9/200
1136/1136 [==============================] - 9s - loss: 0.7981 - acc: 0.5484 - val_loss: 0.6738 - val_acc: 0.6351
Epoch 10/200
1136/1136 [==============================] - 8s - loss: 0.7971 - acc: 0.5519 - val_loss: 0.6728 - val_acc: 0.6421
Epoch 11/200
1136/1136 [==============================] - 8s - loss: 0.7338 - acc: 0.5783 - val_loss: 0.6716 - val_acc: 0.6421
Epoch 12/200
1136/1136 [==============================] - 8s - loss: 0.7954 - acc: 0.5387 - val_loss: 0.6705 - val_acc: 0.6386
Epoch 13/200
1136/1136 [==============================] - 8s - loss: 0.7282 - acc: 0.5889 - val_loss: 0.6695 - val_acc: 0.6456
Epoch 14/200
1136/1136 [==============================] - 8s - loss: 0.7351 - acc: 0.5669 - val_loss: 0.6681 - val_acc: 0.6456
Epoch 15/200
1136/1136 [==============================] - 8s - loss: 0.7399 - acc: 0.5783 - val_loss: 0.6664 - val_acc: 0.6526
Epoch 16/200
1136/1136 [==============================] - 8s - loss: 0.7283 - acc: 0.5739 - val_loss: 0.6650 - val_acc: 0.6561
Epoch 17/200
1136/1136 [==============================] - 8s - loss: 0.6985 - acc: 0.5907 - val_loss: 0.6638 - val_acc: 0.6561
Epoch 18/200
1136/1136 [==============================] - 8s - loss: 0.6874 - acc: 0.5986 - val_loss: 0.6623 - val_acc: 0.6667
Epoch 19/200
1136/1136 [==============================] - 8s - loss: 0.6884 - acc: 0.5942 - val_loss: 0.6604 - val_acc: 0.6632
Epoch 20/200
1136/1136 [==============================] - 8s - loss: 0.6871 - acc: 0.5924 - val_loss: 0.6582 - val_acc: 0.6667
Epoch 21/200
1136/1136 [==============================] - 8s - loss: 0.6875 - acc: 0.6056 - val_loss: 0.6560 - val_acc: 0.6807
Epoch 22/200
1136/1136 [==============================] - 8s - loss: 0.6880 - acc: 0.5880 - val_loss: 0.6544 - val_acc: 0.6737
Epoch 23/200
1136/1136 [==============================] - 8s - loss: 0.6612 - acc: 0.6004 - val_loss: 0.6520 - val_acc: 0.6702
Epoch 24/200
1136/1136 [==============================] - 8s - loss: 0.6854 - acc: 0.5968 - val_loss: 0.6496 - val_acc: 0.6667
Epoch 25/200
1136/1136 [==============================] - 8s - loss: 0.6762 - acc: 0.5942 - val_loss: 0.6483 - val_acc: 0.6737
Epoch 26/200
1136/1136 [==============================] - 8s - loss: 0.6684 - acc: 0.6039 - val_loss: 0.6471 - val_acc: 0.6772
Epoch 27/200
1136/1136 [==============================] - 8s - loss: 0.6746 - acc: 0.6048 - val_loss: 0.6454 - val_acc: 0.6772
Epoch 28/200
1136/1136 [==============================] - 8s - loss: 0.6572 - acc: 0.6056 - val_loss: 0.6436 - val_acc: 0.6737
Epoch 29/200
1136/1136 [==============================] - 8s - loss: 0.6475 - acc: 0.6276 - val_loss: 0.6408 - val_acc: 0.6737
Epoch 30/200
1136/1136 [==============================] - 8s - loss: 0.6611 - acc: 0.6030 - val_loss: 0.6377 - val_acc: 0.6807
Epoch 31/200
1136/1136 [==============================] - 8s - loss: 0.6454 - acc: 0.6250 - val_loss: 0.6354 - val_acc: 0.6702
Epoch 32/200
1136/1136 [==============================] - 8s - loss: 0.6578 - acc: 0.6039 - val_loss: 0.6326 - val_acc: 0.6772
Epoch 33/200
1136/1136 [==============================] - 8s - loss: 0.6432 - acc: 0.6356 - val_loss: 0.6301 - val_acc: 0.6772
Epoch 34/200
1136/1136 [==============================] - 8s - loss: 0.6466 - acc: 0.6197 - val_loss: 0.6276 - val_acc: 0.6772
Epoch 35/200
1136/1136 [==============================] - 8s - loss: 0.6341 - acc: 0.6276 - val_loss: 0.6253 - val_acc: 0.6702
Epoch 36/200
1136/1136 [==============================] - 8s - loss: 0.6416 - acc: 0.6356 - val_loss: 0.6226 - val_acc: 0.6737
Epoch 37/200
1136/1136 [==============================] - 8s - loss: 0.6464 - acc: 0.6268 - val_loss: 0.6208 - val_acc: 0.6737
Epoch 38/200
1136/1136 [==============================] - 8s - loss: 0.6471 - acc: 0.6162 - val_loss: 0.6185 - val_acc: 0.6807
Epoch 39/200
1136/1136 [==============================] - 8s - loss: 0.6439 - acc: 0.6417 - val_loss: 0.6164 - val_acc: 0.6842
Epoch 40/200
1136/1136 [==============================] - 8s - loss: 0.6445 - acc: 0.6206 - val_loss: 0.6147 - val_acc: 0.6877
Epoch 41/200
1136/1136 [==============================] - 8s - loss: 0.6328 - acc: 0.6312 - val_loss: 0.6141 - val_acc: 0.6842
Epoch 42/200
1136/1136 [==============================] - 8s - loss: 0.6270 - acc: 0.6400 - val_loss: 0.6122 - val_acc: 0.6772
Epoch 43/200
1136/1136 [==============================] - 8s - loss: 0.6357 - acc: 0.6285 - val_loss: 0.6104 - val_acc: 0.6807
Epoch 44/200
1136/1136 [==============================] - 8s - loss: 0.6416 - acc: 0.6364 - val_loss: 0.6096 - val_acc: 0.6772
Epoch 45/200
1136/1136 [==============================] - 8s - loss: 0.6386 - acc: 0.6400 - val_loss: 0.6076 - val_acc: 0.6772
Epoch 46/200
1136/1136 [==============================] - 8s - loss: 0.6369 - acc: 0.6408 - val_loss: 0.6061 - val_acc: 0.6772
Epoch 47/200
1136/1136 [==============================] - 9s - loss: 0.6323 - acc: 0.6558 - val_loss: 0.6043 - val_acc: 0.6702
Epoch 48/200
1136/1136 [==============================] - 8s - loss: 0.6222 - acc: 0.6593 - val_loss: 0.6030 - val_acc: 0.6772
Epoch 49/200
1136/1136 [==============================] - 9s - loss: 0.6269 - acc: 0.6496 - val_loss: 0.6015 - val_acc: 0.6807
Epoch 50/200
1136/1136 [==============================] - 8s - loss: 0.6158 - acc: 0.6532 - val_loss: 0.5992 - val_acc: 0.6842
Epoch 51/200
1136/1136 [==============================] - 8s - loss: 0.6091 - acc: 0.6699 - val_loss: 0.5971 - val_acc: 0.6877
Epoch 52/200
1136/1136 [==============================] - 9s - loss: 0.6244 - acc: 0.6673 - val_loss: 0.5964 - val_acc: 0.6877
Epoch 53/200
1136/1136 [==============================] - 8s - loss: 0.6230 - acc: 0.6602 - val_loss: 0.5952 - val_acc: 0.6912
Epoch 54/200
1136/1136 [==============================] - 8s - loss: 0.6181 - acc: 0.6655 - val_loss: 0.5936 - val_acc: 0.6912
Epoch 55/200
1136/1136 [==============================] - 8s - loss: 0.6228 - acc: 0.6523 - val_loss: 0.5925 - val_acc: 0.6842
Epoch 56/200
1136/1136 [==============================] - 9s - loss: 0.6168 - acc: 0.6690 - val_loss: 0.5916 - val_acc: 0.6842
Epoch 57/200
1136/1136 [==============================] - 10s - loss: 0.6066 - acc: 0.6778 - val_loss: 0.5905 - val_acc: 0.6842
Epoch 58/200
1136/1136 [==============================] - 12s - loss: 0.6142 - acc: 0.6708 - val_loss: 0.5887 - val_acc: 0.6877
Epoch 59/200
1136/1136 [==============================] - 8s - loss: 0.6147 - acc: 0.6611 - val_loss: 0.5883 - val_acc: 0.6877
Epoch 60/200
1136/1136 [==============================] - 11s - loss: 0.6111 - acc: 0.6593 - val_loss: 0.5878 - val_acc: 0.6877
Epoch 61/200
1136/1136 [==============================] - 11s - loss: 0.6068 - acc: 0.6769 - val_loss: 0.5862 - val_acc: 0.6877
Epoch 62/200
1136/1136 [==============================] - 11s - loss: 0.6124 - acc: 0.6664 - val_loss: 0.5854 - val_acc: 0.6877
Epoch 63/200
1136/1136 [==============================] - 11s - loss: 0.6137 - acc: 0.6602 - val_loss: 0.5847 - val_acc: 0.6877
Epoch 64/200
1136/1136 [==============================] - 12s - loss: 0.6097 - acc: 0.6593 - val_loss: 0.5838 - val_acc: 0.6842
Epoch 65/200
1136/1136 [==============================] - 11s - loss: 0.6178 - acc: 0.6752 - val_loss: 0.5825 - val_acc: 0.6877
Epoch 66/200
1136/1136 [==============================] - 11s - loss: 0.6058 - acc: 0.6655 - val_loss: 0.5817 - val_acc: 0.6877
Epoch 67/200
1136/1136 [==============================] - 11s - loss: 0.6021 - acc: 0.6761 - val_loss: 0.5810 - val_acc: 0.6877
Epoch 68/200
1136/1136 [==============================] - 11s - loss: 0.6084 - acc: 0.6664 - val_loss: 0.5812 - val_acc: 0.6912
Epoch 69/200
1136/1136 [==============================] - 16s - loss: 0.5994 - acc: 0.6699 - val_loss: 0.5811 - val_acc: 0.6842
Epoch 70/200
1136/1136 [==============================] - 12s - loss: 0.5923 - acc: 0.6690 - val_loss: 0.5801 - val_acc: 0.6842
Epoch 71/200
1136/1136 [==============================] - 11s - loss: 0.5966 - acc: 0.6822 - val_loss: 0.5786 - val_acc: 0.6912
Epoch 72/200
1136/1136 [==============================] - 13s - loss: 0.5977 - acc: 0.6893 - val_loss: 0.5777 - val_acc: 0.6947
Epoch 73/200
1136/1136 [==============================] - 15s - loss: 0.6092 - acc: 0.6602 - val_loss: 0.5775 - val_acc: 0.6982
Epoch 74/200
1136/1136 [==============================] - 14s - loss: 0.5900 - acc: 0.6945 - val_loss: 0.5770 - val_acc: 0.6947
Epoch 75/200
1136/1136 [==============================] - 17s - loss: 0.5998 - acc: 0.6875 - val_loss: 0.5763 - val_acc: 0.6947
Epoch 76/200
1136/1136 [==============================] - 15s - loss: 0.6073 - acc: 0.6857 - val_loss: 0.5753 - val_acc: 0.6982
Epoch 77/200
1136/1136 [==============================] - 17s - loss: 0.6076 - acc: 0.6831 - val_loss: 0.5759 - val_acc: 0.6947
Epoch 78/200
1136/1136 [==============================] - 22s - loss: 0.6005 - acc: 0.6655 - val_loss: 0.5758 - val_acc: 0.6912
Epoch 79/200
1136/1136 [==============================] - 19s - loss: 0.6051 - acc: 0.6778 - val_loss: 0.5752 - val_acc: 0.6912
Epoch 80/200
1136/1136 [==============================] - 30s - loss: 0.5839 - acc: 0.7042 - val_loss: 0.5738 - val_acc: 0.6947
Epoch 81/200
1136/1136 [==============================] - 25s - loss: 0.5842 - acc: 0.6901 - val_loss: 0.5733 - val_acc: 0.6877
Epoch 82/200
1136/1136 [==============================] - 31s - loss: 0.5852 - acc: 0.6910 - val_loss: 0.5728 - val_acc: 0.6912
Epoch 83/200
1136/1136 [==============================] - 33s - loss: 0.5897 - acc: 0.6919 - val_loss: 0.5721 - val_acc: 0.6947
Epoch 84/200
1136/1136 [==============================] - 31s - loss: 0.5934 - acc: 0.6989 - val_loss: 0.5720 - val_acc: 0.6947
Epoch 85/200
1136/1136 [==============================] - 40s - loss: 0.5881 - acc: 0.6954 - val_loss: 0.5722 - val_acc: 0.6947
Epoch 86/200
1136/1136 [==============================] - 38s - loss: 0.5923 - acc: 0.6673 - val_loss: 0.5719 - val_acc: 0.6947
Epoch 87/200
1136/1136 [==============================] - 37s - loss: 0.5848 - acc: 0.7025 - val_loss: 0.5713 - val_acc: 0.6947
Epoch 88/200
1136/1136 [==============================] - 35s - loss: 0.5870 - acc: 0.6796 - val_loss: 0.5705 - val_acc: 0.6947
Epoch 89/200
1136/1136 [==============================] - 14s - loss: 0.5762 - acc: 0.6919 - val_loss: 0.5697 - val_acc: 0.6982
Epoch 90/200
1136/1136 [==============================] - 8s - loss: 0.5889 - acc: 0.6963 - val_loss: 0.5694 - val_acc: 0.6947
Epoch 91/200
1136/1136 [==============================] - 9s - loss: 0.5881 - acc: 0.6963 - val_loss: 0.5699 - val_acc: 0.6947
Epoch 92/200
1136/1136 [==============================] - 9s - loss: 0.5871 - acc: 0.7060 - val_loss: 0.5682 - val_acc: 0.7018
Epoch 93/200
1136/1136 [==============================] - 9s - loss: 0.5700 - acc: 0.7201 - val_loss: 0.5674 - val_acc: 0.7053
Epoch 94/200
1136/1136 [==============================] - 10s - loss: 0.5720 - acc: 0.7069 - val_loss: 0.5679 - val_acc: 0.6982
Epoch 95/200
1136/1136 [==============================] - 10s - loss: 0.5836 - acc: 0.6972 - val_loss: 0.5688 - val_acc: 0.6982
Epoch 96/200
1136/1136 [==============================] - 10s - loss: 0.5710 - acc: 0.7183 - val_loss: 0.5683 - val_acc: 0.7018
Epoch 97/200
1136/1136 [==============================] - 10s - loss: 0.5957 - acc: 0.6875 - val_loss: 0.5678 - val_acc: 0.7018
Epoch 98/200
1136/1136 [==============================] - 9s - loss: 0.5841 - acc: 0.6910 - val_loss: 0.5680 - val_acc: 0.6947
Epoch 99/200
1136/1136 [==============================] - 9s - loss: 0.5780 - acc: 0.6875 - val_loss: 0.5683 - val_acc: 0.6912
Epoch 100/200
1136/1136 [==============================] - 9s - loss: 0.5709 - acc: 0.6919 - val_loss: 0.5672 - val_acc: 0.7018
Epoch 101/200
1136/1136 [==============================] - 9s - loss: 0.5768 - acc: 0.6919 - val_loss: 0.5660 - val_acc: 0.7053
Epoch 102/200
1136/1136 [==============================] - 9s - loss: 0.5784 - acc: 0.6822 - val_loss: 0.5657 - val_acc: 0.7018
Epoch 103/200
1136/1136 [==============================] - 9s - loss: 0.5864 - acc: 0.6901 - val_loss: 0.5650 - val_acc: 0.7018
Epoch 104/200
1136/1136 [==============================] - 8s - loss: 0.5942 - acc: 0.6840 - val_loss: 0.5648 - val_acc: 0.7018
Epoch 105/200
1136/1136 [==============================] - 8s - loss: 0.5869 - acc: 0.7148 - val_loss: 0.5645 - val_acc: 0.6982
Epoch 106/200
1136/1136 [==============================] - 8s - loss: 0.5743 - acc: 0.7025 - val_loss: 0.5640 - val_acc: 0.6982
Epoch 107/200
1136/1136 [==============================] - 8s - loss: 0.5767 - acc: 0.6981 - val_loss: 0.5630 - val_acc: 0.7088
Epoch 108/200
1136/1136 [==============================] - 8s - loss: 0.5681 - acc: 0.7095 - val_loss: 0.5636 - val_acc: 0.7053
Epoch 109/200
1136/1136 [==============================] - 8s - loss: 0.5657 - acc: 0.6989 - val_loss: 0.5637 - val_acc: 0.7088
Epoch 110/200
1136/1136 [==============================] - 8s - loss: 0.5878 - acc: 0.6998 - val_loss: 0.5635 - val_acc: 0.7088
Epoch 111/200
1136/1136 [==============================] - 8s - loss: 0.5796 - acc: 0.7077 - val_loss: 0.5631 - val_acc: 0.7088
Epoch 112/200
1136/1136 [==============================] - 8s - loss: 0.5564 - acc: 0.7201 - val_loss: 0.5629 - val_acc: 0.7088
Epoch 113/200
1136/1136 [==============================] - 8s - loss: 0.5748 - acc: 0.7104 - val_loss: 0.5624 - val_acc: 0.7123
Epoch 114/200
1136/1136 [==============================] - 8s - loss: 0.5646 - acc: 0.7254 - val_loss: 0.5618 - val_acc: 0.7088
Epoch 115/200
1136/1136 [==============================] - 9s - loss: 0.5600 - acc: 0.7245 - val_loss: 0.5614 - val_acc: 0.7088
Epoch 116/200
1136/1136 [==============================] - 9s - loss: 0.5719 - acc: 0.7192 - val_loss: 0.5620 - val_acc: 0.7088
Epoch 117/200
1136/1136 [==============================] - 9s - loss: 0.5805 - acc: 0.6989 - val_loss: 0.5607 - val_acc: 0.7088
Epoch 118/200
1136/1136 [==============================] - 8s - loss: 0.5695 - acc: 0.6998 - val_loss: 0.5602 - val_acc: 0.7088
Epoch 119/200
1136/1136 [==============================] - 8s - loss: 0.5709 - acc: 0.7042 - val_loss: 0.5599 - val_acc: 0.7053
Epoch 120/200
1136/1136 [==============================] - 8s - loss: 0.5605 - acc: 0.7165 - val_loss: 0.5601 - val_acc: 0.7088
Epoch 121/200
1136/1136 [==============================] - 8s - loss: 0.5697 - acc: 0.7121 - val_loss: 0.5596 - val_acc: 0.7053
Epoch 122/200
1136/1136 [==============================] - 9s - loss: 0.5790 - acc: 0.7060 - val_loss: 0.5593 - val_acc: 0.7053
Epoch 123/200
1136/1136 [==============================] - 9s - loss: 0.5648 - acc: 0.7306 - val_loss: 0.5590 - val_acc: 0.7088
Epoch 124/200
1136/1136 [==============================] - 8s - loss: 0.5690 - acc: 0.7007 - val_loss: 0.5583 - val_acc: 0.7123
Epoch 125/200
1136/1136 [==============================] - 8s - loss: 0.5699 - acc: 0.7104 - val_loss: 0.5568 - val_acc: 0.7088
Epoch 126/200
1136/1136 [==============================] - 8s - loss: 0.5746 - acc: 0.7051 - val_loss: 0.5564 - val_acc: 0.7158
Epoch 127/200
1136/1136 [==============================] - 8s - loss: 0.5726 - acc: 0.7236 - val_loss: 0.5566 - val_acc: 0.7158
Epoch 128/200
1136/1136 [==============================] - 8s - loss: 0.5552 - acc: 0.7201 - val_loss: 0.5568 - val_acc: 0.7123
Epoch 129/200
1136/1136 [==============================] - 9s - loss: 0.5867 - acc: 0.7042 - val_loss: 0.5570 - val_acc: 0.7123
Epoch 130/200
1136/1136 [==============================] - 8s - loss: 0.5595 - acc: 0.7095 - val_loss: 0.5575 - val_acc: 0.7123
Epoch 131/200
1136/1136 [==============================] - 8s - loss: 0.5583 - acc: 0.7165 - val_loss: 0.5571 - val_acc: 0.7123
Epoch 132/200
1136/1136 [==============================] - 8s - loss: 0.5554 - acc: 0.7245 - val_loss: 0.5565 - val_acc: 0.7123
Epoch 133/200
1136/1136 [==============================] - 9s - loss: 0.5458 - acc: 0.7289 - val_loss: 0.5557 - val_acc: 0.7158
Epoch 134/200
1136/1136 [==============================] - 8s - loss: 0.5510 - acc: 0.7210 - val_loss: 0.5556 - val_acc: 0.7193
Epoch 135/200
1136/1136 [==============================] - 8s - loss: 0.5714 - acc: 0.6998 - val_loss: 0.5563 - val_acc: 0.7158
Epoch 136/200
1136/1136 [==============================] - 8s - loss: 0.5548 - acc: 0.7201 - val_loss: 0.5578 - val_acc: 0.7158
Epoch 137/200
1136/1136 [==============================] - 8s - loss: 0.5638 - acc: 0.7042 - val_loss: 0.5579 - val_acc: 0.7158
Epoch 138/200
1136/1136 [==============================] - 8s - loss: 0.5648 - acc: 0.7113 - val_loss: 0.5580 - val_acc: 0.7158
Epoch 139/200
1136/1136 [==============================] - 8s - loss: 0.5639 - acc: 0.7210 - val_loss: 0.5583 - val_acc: 0.7193
Epoch 140/200
1136/1136 [==============================] - 8s - loss: 0.5626 - acc: 0.7218 - val_loss: 0.5585 - val_acc: 0.7228
Epoch 141/200
1136/1136 [==============================] - 8s - loss: 0.5562 - acc: 0.7201 - val_loss: 0.5583 - val_acc: 0.7158
Epoch 142/200
1136/1136 [==============================] - 8s - loss: 0.5556 - acc: 0.7201 - val_loss: 0.5590 - val_acc: 0.7158
Epoch 143/200
1136/1136 [==============================] - 8s - loss: 0.5544 - acc: 0.7086 - val_loss: 0.5583 - val_acc: 0.7193
Epoch 144/200
1136/1136 [==============================] - 8s - loss: 0.5660 - acc: 0.7086 - val_loss: 0.5566 - val_acc: 0.7158
Epoch 145/200
1136/1136 [==============================] - 8s - loss: 0.5729 - acc: 0.7069 - val_loss: 0.5555 - val_acc: 0.7193
Epoch 146/200
1136/1136 [==============================] - 8s - loss: 0.5544 - acc: 0.7148 - val_loss: 0.5555 - val_acc: 0.7158
Epoch 147/200
1136/1136 [==============================] - 8s - loss: 0.5934 - acc: 0.6919 - val_loss: 0.5554 - val_acc: 0.7158
Epoch 148/200
1136/1136 [==============================] - 8s - loss: 0.5697 - acc: 0.7104 - val_loss: 0.5538 - val_acc: 0.7228
Epoch 149/200
1136/1136 [==============================] - 8s - loss: 0.5486 - acc: 0.7262 - val_loss: 0.5541 - val_acc: 0.7158
Epoch 150/200
1136/1136 [==============================] - 8s - loss: 0.5520 - acc: 0.7280 - val_loss: 0.5536 - val_acc: 0.7158
Epoch 151/200
1136/1136 [==============================] - 8s - loss: 0.5557 - acc: 0.7218 - val_loss: 0.5543 - val_acc: 0.7193
Epoch 152/200
1136/1136 [==============================] - 8s - loss: 0.5670 - acc: 0.7130 - val_loss: 0.5525 - val_acc: 0.7158
Epoch 153/200
1136/1136 [==============================] - 8s - loss: 0.5429 - acc: 0.7157 - val_loss: 0.5523 - val_acc: 0.7158
Epoch 154/200
1136/1136 [==============================] - 8s - loss: 0.5624 - acc: 0.7016 - val_loss: 0.5513 - val_acc: 0.7158
Epoch 155/200
1136/1136 [==============================] - 8s - loss: 0.5741 - acc: 0.7060 - val_loss: 0.5508 - val_acc: 0.7158
Epoch 156/200
1136/1136 [==============================] - 8s - loss: 0.5563 - acc: 0.7183 - val_loss: 0.5498 - val_acc: 0.7263
Epoch 157/200
1136/1136 [==============================] - 8s - loss: 0.5490 - acc: 0.7201 - val_loss: 0.5505 - val_acc: 0.7228
Epoch 158/200
1136/1136 [==============================] - 8s - loss: 0.5572 - acc: 0.7271 - val_loss: 0.5511 - val_acc: 0.7228
Epoch 159/200
1136/1136 [==============================] - 8s - loss: 0.5562 - acc: 0.7104 - val_loss: 0.5506 - val_acc: 0.7228
Epoch 160/200
1136/1136 [==============================] - 8s - loss: 0.5415 - acc: 0.7192 - val_loss: 0.5500 - val_acc: 0.7263
Epoch 161/200
1136/1136 [==============================] - 8s - loss: 0.5507 - acc: 0.7183 - val_loss: 0.5501 - val_acc: 0.7228
Epoch 162/200
1136/1136 [==============================] - 8s - loss: 0.5584 - acc: 0.7280 - val_loss: 0.5506 - val_acc: 0.7228
Epoch 163/200
1136/1136 [==============================] - 8s - loss: 0.5754 - acc: 0.6998 - val_loss: 0.5507 - val_acc: 0.7193
Epoch 164/200
1136/1136 [==============================] - 8s - loss: 0.5424 - acc: 0.7157 - val_loss: 0.5509 - val_acc: 0.7158
Epoch 165/200
1136/1136 [==============================] - 8s - loss: 0.5571 - acc: 0.7201 - val_loss: 0.5490 - val_acc: 0.7193
Epoch 166/200
1136/1136 [==============================] - 8s - loss: 0.5621 - acc: 0.7201 - val_loss: 0.5481 - val_acc: 0.7193
Epoch 167/200
1136/1136 [==============================] - 8s - loss: 0.5554 - acc: 0.7271 - val_loss: 0.5490 - val_acc: 0.7193
Epoch 168/200
1136/1136 [==============================] - 8s - loss: 0.5439 - acc: 0.7218 - val_loss: 0.5495 - val_acc: 0.7158
Epoch 169/200
1136/1136 [==============================] - 8s - loss: 0.5635 - acc: 0.7086 - val_loss: 0.5489 - val_acc: 0.7193
Epoch 170/200
1136/1136 [==============================] - 8s - loss: 0.5496 - acc: 0.7042 - val_loss: 0.5487 - val_acc: 0.7193
Epoch 171/200
1136/1136 [==============================] - 8s - loss: 0.5616 - acc: 0.7016 - val_loss: 0.5482 - val_acc: 0.7193
Epoch 172/200
1136/1136 [==============================] - 8s - loss: 0.5511 - acc: 0.7271 - val_loss: 0.5478 - val_acc: 0.7228
Epoch 173/200
1136/1136 [==============================] - 8s - loss: 0.5470 - acc: 0.7359 - val_loss: 0.5479 - val_acc: 0.7228
Epoch 174/200
1136/1136 [==============================] - 8s - loss: 0.5339 - acc: 0.7333 - val_loss: 0.5476 - val_acc: 0.7298
Epoch 175/200
1136/1136 [==============================] - 9s - loss: 0.5615 - acc: 0.7077 - val_loss: 0.5474 - val_acc: 0.7228
Epoch 176/200
1136/1136 [==============================] - 8s - loss: 0.5372 - acc: 0.7157 - val_loss: 0.5468 - val_acc: 0.7228
Epoch 177/200
1136/1136 [==============================] - 8s - loss: 0.5673 - acc: 0.7183 - val_loss: 0.5458 - val_acc: 0.7228
Epoch 178/200
1136/1136 [==============================] - 8s - loss: 0.5276 - acc: 0.7368 - val_loss: 0.5456 - val_acc: 0.7263
Epoch 179/200
1136/1136 [==============================] - 8s - loss: 0.5538 - acc: 0.7095 - val_loss: 0.5459 - val_acc: 0.7263
Epoch 180/200
1136/1136 [==============================] - 8s - loss: 0.5437 - acc: 0.7350 - val_loss: 0.5464 - val_acc: 0.7263
Epoch 181/200
1136/1136 [==============================] - 8s - loss: 0.5431 - acc: 0.7289 - val_loss: 0.5465 - val_acc: 0.7298
Epoch 182/200
1136/1136 [==============================] - 8s - loss: 0.5474 - acc: 0.7254 - val_loss: 0.5463 - val_acc: 0.7298
Epoch 183/200
1136/1136 [==============================] - 8s - loss: 0.5426 - acc: 0.7315 - val_loss: 0.5457 - val_acc: 0.7298
Epoch 184/200
1136/1136 [==============================] - 9s - loss: 0.5533 - acc: 0.7121 - val_loss: 0.5456 - val_acc: 0.7298
Epoch 185/200
1136/1136 [==============================] - 9s - loss: 0.5424 - acc: 0.7315 - val_loss: 0.5451 - val_acc: 0.7298
Epoch 186/200
1136/1136 [==============================] - 9s - loss: 0.5308 - acc: 0.7482 - val_loss: 0.5462 - val_acc: 0.7263
Epoch 187/200
1136/1136 [==============================] - 9s - loss: 0.5422 - acc: 0.7210 - val_loss: 0.5457 - val_acc: 0.7298
Epoch 188/200
1136/1136 [==============================] - 8s - loss: 0.5373 - acc: 0.7342 - val_loss: 0.5458 - val_acc: 0.7263
Epoch 189/200
1136/1136 [==============================] - 8s - loss: 0.5416 - acc: 0.7280 - val_loss: 0.5464 - val_acc: 0.7228
Epoch 190/200
1136/1136 [==============================] - 8s - loss: 0.5522 - acc: 0.7324 - val_loss: 0.5454 - val_acc: 0.7193
Epoch 191/200
1136/1136 [==============================] - 8s - loss: 0.5565 - acc: 0.7192 - val_loss: 0.5445 - val_acc: 0.7193
Epoch 192/200
1136/1136 [==============================] - 8s - loss: 0.5408 - acc: 0.7324 - val_loss: 0.5436 - val_acc: 0.7228
Epoch 193/200
1136/1136 [==============================] - 9s - loss: 0.5667 - acc: 0.7192 - val_loss: 0.5441 - val_acc: 0.7228
Epoch 194/200
1136/1136 [==============================] - 8s - loss: 0.5462 - acc: 0.7139 - val_loss: 0.5434 - val_acc: 0.7263
Epoch 195/200
1136/1136 [==============================] - 8s - loss: 0.5504 - acc: 0.7254 - val_loss: 0.5439 - val_acc: 0.7263
Epoch 196/200
1136/1136 [==============================] - 8s - loss: 0.5361 - acc: 0.7324 - val_loss: 0.5437 - val_acc: 0.7263
Epoch 197/200
1136/1136 [==============================] - 8s - loss: 0.5379 - acc: 0.7368 - val_loss: 0.5440 - val_acc: 0.7263
Epoch 198/200
1136/1136 [==============================] - 8s - loss: 0.5459 - acc: 0.7306 - val_loss: 0.5441 - val_acc: 0.7298
Epoch 199/200
1136/1136 [==============================] - 8s - loss: 0.5441 - acc: 0.7359 - val_loss: 0.5448 - val_acc: 0.7333
Epoch 200/200
1136/1136 [==============================] - 8s - loss: 0.5540 - acc: 0.7060 - val_loss: 0.5436 - val_acc: 0.7333
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[261]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x272055d10&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [262]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sgd2</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="c1">#LSTM_layer = Dropout(0.5)(LSTM_layer)</span>

<span class="n">normed</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">normed</span><span class="p">)</span> <span class="c1"># optional hidden layer</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>

<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd2</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_seq</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-7-db53cfe3454e&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>sequence_input <span class="ansi-blue-fg">=</span> Input<span class="ansi-blue-fg">(</span>shape<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">(</span>max_seq_length<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> dtype<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'int32'</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> embedded_sequences <span class="ansi-blue-fg">=</span> embedding_layer<span class="ansi-blue-fg">(</span>sequence_input<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> 
<span class="ansi-green-intense-fg ansi-bold">      4</span> LSTM_layer <span class="ansi-blue-fg">=</span> Bidirectional<span class="ansi-blue-fg">(</span>LSTM<span class="ansi-blue-fg">(</span>units<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">128</span><span class="ansi-blue-fg">,</span>dropout<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.5</span><span class="ansi-blue-fg">,</span> recurrent_dropout<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.5</span><span class="ansi-blue-fg">,</span>return_sequences<span class="ansi-blue-fg">=</span>True<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>embedded_sequences<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> LSTM_layer <span class="ansi-blue-fg">=</span> Bidirectional<span class="ansi-blue-fg">(</span>LSTM<span class="ansi-blue-fg">(</span>units<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">128</span><span class="ansi-blue-fg">,</span>dropout<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.5</span><span class="ansi-blue-fg">,</span> recurrent_dropout<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.5</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">(</span>LSTM_layer<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name 'Input' is not defined</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [270]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sgd3</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [274]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">))(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="c1">#LSTM_layer = Dropout(0.5)(LSTM_layer)</span>

<span class="n">normed</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">LSTM_layer</span><span class="p">)</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">normed</span><span class="p">)</span> <span class="c1"># optional hidden layer</span>
<span class="n">penultimate</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>

<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">penultimate</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd3</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_seq</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1136 samples, validate on 285 samples
Epoch 1/200
1136/1136 [==============================] - 31s - loss: 1.7832 - acc: 0.4912 - val_loss: 0.6968 - val_acc: 0.4667
Epoch 2/200
1136/1136 [==============================] - 9s - loss: 0.9996 - acc: 0.4991 - val_loss: 0.6943 - val_acc: 0.4632
Epoch 3/200
1136/1136 [==============================] - 10s - loss: 0.8981 - acc: 0.5185 - val_loss: 0.6958 - val_acc: 0.4140
Epoch 4/200
1136/1136 [==============================] - 10s - loss: 0.8679 - acc: 0.5097 - val_loss: 0.6945 - val_acc: 0.4737
Epoch 5/200
1136/1136 [==============================] - 10s - loss: 0.8294 - acc: 0.5000 - val_loss: 0.6908 - val_acc: 0.5474
Epoch 6/200
1136/1136 [==============================] - 9s - loss: 0.8001 - acc: 0.5379 - val_loss: 0.6909 - val_acc: 0.5684
Epoch 7/200
1136/1136 [==============================] - 9s - loss: 0.8251 - acc: 0.4956 - val_loss: 0.6945 - val_acc: 0.4702
Epoch 8/200
1136/1136 [==============================] - 9s - loss: 0.8116 - acc: 0.5176 - val_loss: 0.6905 - val_acc: 0.5649
Epoch 9/200
1136/1136 [==============================] - 9s - loss: 0.8055 - acc: 0.4886 - val_loss: 0.6913 - val_acc: 0.5193
Epoch 10/200
1136/1136 [==============================] - 8s - loss: 0.7985 - acc: 0.5123 - val_loss: 0.6879 - val_acc: 0.5930
Epoch 11/200
1136/1136 [==============================] - 8s - loss: 0.7906 - acc: 0.5185 - val_loss: 0.6888 - val_acc: 0.5860
Epoch 12/200
1136/1136 [==============================] - 8s - loss: 0.8117 - acc: 0.5088 - val_loss: 0.6869 - val_acc: 0.5930
Epoch 13/200
1136/1136 [==============================] - 8s - loss: 0.8097 - acc: 0.5000 - val_loss: 0.6907 - val_acc: 0.5368
Epoch 14/200
1136/1136 [==============================] - 9s - loss: 0.8112 - acc: 0.4894 - val_loss: 0.6919 - val_acc: 0.5158
Epoch 15/200
1136/1136 [==============================] - 9s - loss: 0.7955 - acc: 0.5167 - val_loss: 0.6867 - val_acc: 0.5684
Epoch 16/200
1136/1136 [==============================] - 8s - loss: 0.7520 - acc: 0.5449 - val_loss: 0.6797 - val_acc: 0.6596
Epoch 17/200
1136/1136 [==============================] - 9s - loss: 0.7615 - acc: 0.5431 - val_loss: 0.6824 - val_acc: 0.5895
Epoch 18/200
1136/1136 [==============================] - 8s - loss: 0.7893 - acc: 0.5220 - val_loss: 0.6818 - val_acc: 0.6561
Epoch 19/200
1136/1136 [==============================] - 8s - loss: 0.7887 - acc: 0.5062 - val_loss: 0.6813 - val_acc: 0.6421
Epoch 20/200
1136/1136 [==============================] - 8s - loss: 0.7682 - acc: 0.5290 - val_loss: 0.6800 - val_acc: 0.6316
Epoch 21/200
1136/1136 [==============================] - 8s - loss: 0.7602 - acc: 0.5132 - val_loss: 0.6841 - val_acc: 0.6140
Epoch 22/200
1136/1136 [==============================] - 8s - loss: 0.7741 - acc: 0.5493 - val_loss: 0.6747 - val_acc: 0.6386
Epoch 23/200
1136/1136 [==============================] - 8s - loss: 0.8137 - acc: 0.5150 - val_loss: 0.6648 - val_acc: 0.6737
Epoch 24/200
1136/1136 [==============================] - 8s - loss: 0.7514 - acc: 0.5308 - val_loss: 0.6685 - val_acc: 0.6596
Epoch 25/200
1136/1136 [==============================] - 8s - loss: 0.7577 - acc: 0.5290 - val_loss: 0.6661 - val_acc: 0.6421
Epoch 26/200
1136/1136 [==============================] - 8s - loss: 0.7327 - acc: 0.5651 - val_loss: 0.6508 - val_acc: 0.6772
Epoch 27/200
1136/1136 [==============================] - 9s - loss: 0.6969 - acc: 0.5898 - val_loss: 0.6359 - val_acc: 0.6807
Epoch 28/200
1136/1136 [==============================] - 8s - loss: 0.7204 - acc: 0.5651 - val_loss: 0.6397 - val_acc: 0.6772
Epoch 29/200
1136/1136 [==============================] - 8s - loss: 0.7187 - acc: 0.5722 - val_loss: 0.6228 - val_acc: 0.6807
Epoch 30/200
1136/1136 [==============================] - 8s - loss: 0.7016 - acc: 0.5907 - val_loss: 0.6258 - val_acc: 0.6947
Epoch 31/200
1136/1136 [==============================] - 8s - loss: 0.7208 - acc: 0.5713 - val_loss: 0.6313 - val_acc: 0.6596
Epoch 32/200
1136/1136 [==============================] - 9s - loss: 0.7210 - acc: 0.5783 - val_loss: 0.6298 - val_acc: 0.6807
Epoch 33/200
1136/1136 [==============================] - 8s - loss: 0.7089 - acc: 0.5634 - val_loss: 0.6076 - val_acc: 0.6842
Epoch 34/200
1136/1136 [==============================] - 8s - loss: 0.7252 - acc: 0.5907 - val_loss: 0.6112 - val_acc: 0.6912
Epoch 35/200
1136/1136 [==============================] - 8s - loss: 0.7112 - acc: 0.5968 - val_loss: 0.5979 - val_acc: 0.6912
Epoch 36/200
1136/1136 [==============================] - 8s - loss: 0.7061 - acc: 0.6048 - val_loss: 0.5907 - val_acc: 0.6842
Epoch 37/200
1136/1136 [==============================] - 8s - loss: 0.6810 - acc: 0.6426 - val_loss: 0.5933 - val_acc: 0.6912
Epoch 38/200
1136/1136 [==============================] - 8s - loss: 0.6653 - acc: 0.6197 - val_loss: 0.5803 - val_acc: 0.7018
Epoch 39/200
1136/1136 [==============================] - 8s - loss: 0.6997 - acc: 0.6056 - val_loss: 0.5794 - val_acc: 0.6947
Epoch 40/200
1136/1136 [==============================] - 8s - loss: 0.6867 - acc: 0.6329 - val_loss: 0.5786 - val_acc: 0.6947
Epoch 41/200
1136/1136 [==============================] - 8s - loss: 0.6924 - acc: 0.6004 - val_loss: 0.5812 - val_acc: 0.6947
Epoch 42/200
1136/1136 [==============================] - 8s - loss: 0.6546 - acc: 0.6338 - val_loss: 0.5704 - val_acc: 0.6947
Epoch 43/200
1136/1136 [==============================] - 8s - loss: 0.6604 - acc: 0.6576 - val_loss: 0.5740 - val_acc: 0.6947
Epoch 44/200
1136/1136 [==============================] - 8s - loss: 0.6518 - acc: 0.6426 - val_loss: 0.5704 - val_acc: 0.7053
Epoch 45/200
1136/1136 [==============================] - 8s - loss: 0.6509 - acc: 0.6426 - val_loss: 0.5688 - val_acc: 0.7123
Epoch 46/200
1136/1136 [==============================] - 8s - loss: 0.6595 - acc: 0.6637 - val_loss: 0.5661 - val_acc: 0.7053
Epoch 47/200
1136/1136 [==============================] - 8s - loss: 0.6440 - acc: 0.6426 - val_loss: 0.5672 - val_acc: 0.7263
Epoch 48/200
1136/1136 [==============================] - 8s - loss: 0.6669 - acc: 0.6408 - val_loss: 0.5778 - val_acc: 0.7158
Epoch 49/200
1136/1136 [==============================] - 8s - loss: 0.6379 - acc: 0.6540 - val_loss: 0.5707 - val_acc: 0.7228
Epoch 50/200
1136/1136 [==============================] - 8s - loss: 0.6423 - acc: 0.6664 - val_loss: 0.5837 - val_acc: 0.7193
Epoch 51/200
1136/1136 [==============================] - 8s - loss: 0.6479 - acc: 0.6593 - val_loss: 0.5617 - val_acc: 0.7158
Epoch 52/200
1136/1136 [==============================] - 8s - loss: 0.6185 - acc: 0.6540 - val_loss: 0.5738 - val_acc: 0.7228
Epoch 53/200
1136/1136 [==============================] - 8s - loss: 0.6512 - acc: 0.6717 - val_loss: 0.5764 - val_acc: 0.7123
Epoch 54/200
1136/1136 [==============================] - 9s - loss: 0.6431 - acc: 0.6549 - val_loss: 0.5685 - val_acc: 0.7158
Epoch 55/200
1136/1136 [==============================] - 8s - loss: 0.6683 - acc: 0.6505 - val_loss: 0.5777 - val_acc: 0.7053
Epoch 56/200
1136/1136 [==============================] - 8s - loss: 0.6272 - acc: 0.6637 - val_loss: 0.5651 - val_acc: 0.7018
Epoch 57/200
1136/1136 [==============================] - 8s - loss: 0.6354 - acc: 0.6769 - val_loss: 0.5774 - val_acc: 0.7123
Epoch 58/200
1136/1136 [==============================] - 8s - loss: 0.6276 - acc: 0.6734 - val_loss: 0.5747 - val_acc: 0.7158
Epoch 59/200
1136/1136 [==============================] - 8s - loss: 0.6328 - acc: 0.6761 - val_loss: 0.5632 - val_acc: 0.7088
Epoch 60/200
1136/1136 [==============================] - 8s - loss: 0.6087 - acc: 0.6769 - val_loss: 0.5719 - val_acc: 0.7088
Epoch 61/200
1136/1136 [==============================] - 8s - loss: 0.6278 - acc: 0.6981 - val_loss: 0.5714 - val_acc: 0.7088
Epoch 62/200
1136/1136 [==============================] - 8s - loss: 0.6350 - acc: 0.6717 - val_loss: 0.5626 - val_acc: 0.7053
Epoch 63/200
1136/1136 [==============================] - 8s - loss: 0.6360 - acc: 0.6690 - val_loss: 0.5840 - val_acc: 0.7123
Epoch 64/200
1136/1136 [==============================] - 8s - loss: 0.6134 - acc: 0.7148 - val_loss: 0.5719 - val_acc: 0.7053
Epoch 65/200
1136/1136 [==============================] - 8s - loss: 0.5850 - acc: 0.6813 - val_loss: 0.5805 - val_acc: 0.7158
Epoch 66/200
1136/1136 [==============================] - 8s - loss: 0.6205 - acc: 0.6857 - val_loss: 0.5659 - val_acc: 0.7193
Epoch 67/200
1136/1136 [==============================] - 8s - loss: 0.6268 - acc: 0.6734 - val_loss: 0.5633 - val_acc: 0.7193
Epoch 68/200
1136/1136 [==============================] - 8s - loss: 0.6181 - acc: 0.6787 - val_loss: 0.5620 - val_acc: 0.6947
Epoch 69/200
1136/1136 [==============================] - 8s - loss: 0.6076 - acc: 0.6954 - val_loss: 0.5624 - val_acc: 0.7053
Epoch 70/200
1136/1136 [==============================] - 8s - loss: 0.6158 - acc: 0.6857 - val_loss: 0.5641 - val_acc: 0.7053
Epoch 71/200
1136/1136 [==============================] - 8s - loss: 0.5975 - acc: 0.6893 - val_loss: 0.5628 - val_acc: 0.7123
Epoch 72/200
1136/1136 [==============================] - 8s - loss: 0.6015 - acc: 0.6901 - val_loss: 0.5557 - val_acc: 0.7088
Epoch 73/200
1136/1136 [==============================] - 8s - loss: 0.5866 - acc: 0.7157 - val_loss: 0.5530 - val_acc: 0.7193
Epoch 74/200
1136/1136 [==============================] - 8s - loss: 0.5817 - acc: 0.7192 - val_loss: 0.5659 - val_acc: 0.7123
Epoch 75/200
1136/1136 [==============================] - 8s - loss: 0.5783 - acc: 0.7095 - val_loss: 0.5685 - val_acc: 0.7193
Epoch 76/200
1136/1136 [==============================] - 8s - loss: 0.5982 - acc: 0.6893 - val_loss: 0.5654 - val_acc: 0.7193
Epoch 77/200
1136/1136 [==============================] - 8s - loss: 0.5992 - acc: 0.6981 - val_loss: 0.5529 - val_acc: 0.7158
Epoch 78/200
1136/1136 [==============================] - 8s - loss: 0.5704 - acc: 0.7113 - val_loss: 0.5551 - val_acc: 0.7088
Epoch 79/200
1136/1136 [==============================] - 8s - loss: 0.5516 - acc: 0.7368 - val_loss: 0.5661 - val_acc: 0.7088
Epoch 80/200
1136/1136 [==============================] - 8s - loss: 0.5982 - acc: 0.6954 - val_loss: 0.5577 - val_acc: 0.7158
Epoch 81/200
1136/1136 [==============================] - 9s - loss: 0.5586 - acc: 0.7245 - val_loss: 0.5728 - val_acc: 0.7193
Epoch 82/200
1136/1136 [==============================] - 9s - loss: 0.5612 - acc: 0.7315 - val_loss: 0.5868 - val_acc: 0.7263
Epoch 83/200
1136/1136 [==============================] - 8s - loss: 0.5545 - acc: 0.7271 - val_loss: 0.5695 - val_acc: 0.7193
Epoch 84/200
1136/1136 [==============================] - 8s - loss: 0.5620 - acc: 0.7394 - val_loss: 0.5561 - val_acc: 0.7123
Epoch 85/200
1136/1136 [==============================] - 8s - loss: 0.5497 - acc: 0.7245 - val_loss: 0.5595 - val_acc: 0.7158
Epoch 86/200
1136/1136 [==============================] - 8s - loss: 0.5659 - acc: 0.7033 - val_loss: 0.5637 - val_acc: 0.7193
Epoch 87/200
1136/1136 [==============================] - 8s - loss: 0.5786 - acc: 0.7306 - val_loss: 0.5571 - val_acc: 0.7193
Epoch 88/200
1136/1136 [==============================] - 8s - loss: 0.5661 - acc: 0.7104 - val_loss: 0.5643 - val_acc: 0.7088
Epoch 89/200
1136/1136 [==============================] - 9s - loss: 0.5675 - acc: 0.7165 - val_loss: 0.5697 - val_acc: 0.7123
Epoch 90/200
1136/1136 [==============================] - 9s - loss: 0.5810 - acc: 0.7130 - val_loss: 0.5639 - val_acc: 0.7228
Epoch 91/200
1136/1136 [==============================] - 9s - loss: 0.5700 - acc: 0.7077 - val_loss: 0.5680 - val_acc: 0.7298
Epoch 92/200
1136/1136 [==============================] - 8s - loss: 0.5555 - acc: 0.6981 - val_loss: 0.5769 - val_acc: 0.7263
Epoch 93/200
1136/1136 [==============================] - 9s - loss: 0.6040 - acc: 0.7069 - val_loss: 0.5528 - val_acc: 0.7193
Epoch 94/200
1136/1136 [==============================] - 9s - loss: 0.5848 - acc: 0.6945 - val_loss: 0.5567 - val_acc: 0.7228
Epoch 95/200
1136/1136 [==============================] - 9s - loss: 0.5631 - acc: 0.7165 - val_loss: 0.5542 - val_acc: 0.7404
Epoch 96/200
1136/1136 [==============================] - 8s - loss: 0.5799 - acc: 0.7236 - val_loss: 0.5480 - val_acc: 0.7298
Epoch 97/200
1136/1136 [==============================] - 8s - loss: 0.5505 - acc: 0.7430 - val_loss: 0.5612 - val_acc: 0.7368
Epoch 98/200
1136/1136 [==============================] - 9s - loss: 0.5678 - acc: 0.7306 - val_loss: 0.5536 - val_acc: 0.7228
Epoch 99/200
1136/1136 [==============================] - 9s - loss: 0.5385 - acc: 0.7447 - val_loss: 0.5620 - val_acc: 0.7158
Epoch 100/200
1136/1136 [==============================] - 8s - loss: 0.5577 - acc: 0.7192 - val_loss: 0.5722 - val_acc: 0.7263
Epoch 101/200
1136/1136 [==============================] - 8s - loss: 0.5554 - acc: 0.7254 - val_loss: 0.5689 - val_acc: 0.7298
Epoch 102/200
1136/1136 [==============================] - 8s - loss: 0.5611 - acc: 0.7227 - val_loss: 0.5612 - val_acc: 0.7228
Epoch 103/200
1136/1136 [==============================] - 8s - loss: 0.5356 - acc: 0.7606 - val_loss: 0.5673 - val_acc: 0.7263
Epoch 104/200
1136/1136 [==============================] - 8s - loss: 0.5736 - acc: 0.7192 - val_loss: 0.5447 - val_acc: 0.7263
Epoch 105/200
1136/1136 [==============================] - 8s - loss: 0.5507 - acc: 0.7113 - val_loss: 0.5688 - val_acc: 0.7404
Epoch 106/200
1136/1136 [==============================] - 8s - loss: 0.5265 - acc: 0.7544 - val_loss: 0.5899 - val_acc: 0.7368
Epoch 107/200
1136/1136 [==============================] - 8s - loss: 0.5357 - acc: 0.7315 - val_loss: 0.5676 - val_acc: 0.7158
Epoch 108/200
1136/1136 [==============================] - 9s - loss: 0.5380 - acc: 0.7509 - val_loss: 0.5455 - val_acc: 0.7123
Epoch 109/200
1136/1136 [==============================] - 8s - loss: 0.5342 - acc: 0.7342 - val_loss: 0.5429 - val_acc: 0.7123
Epoch 110/200
1136/1136 [==============================] - 9s - loss: 0.5285 - acc: 0.7386 - val_loss: 0.5656 - val_acc: 0.7053
Epoch 111/200
1136/1136 [==============================] - 8s - loss: 0.5370 - acc: 0.7306 - val_loss: 0.5559 - val_acc: 0.7053
Epoch 112/200
1136/1136 [==============================] - 8s - loss: 0.5373 - acc: 0.7394 - val_loss: 0.5566 - val_acc: 0.7018
Epoch 113/200
1136/1136 [==============================] - 8s - loss: 0.5408 - acc: 0.7518 - val_loss: 0.5550 - val_acc: 0.7123
Epoch 114/200
1136/1136 [==============================] - 8s - loss: 0.5163 - acc: 0.7377 - val_loss: 0.5759 - val_acc: 0.7123
Epoch 115/200
1136/1136 [==============================] - 8s - loss: 0.5301 - acc: 0.7482 - val_loss: 0.5731 - val_acc: 0.7088
Epoch 116/200
1136/1136 [==============================] - 8s - loss: 0.5547 - acc: 0.7403 - val_loss: 0.5472 - val_acc: 0.7123
Epoch 117/200
1136/1136 [==============================] - 8s - loss: 0.5237 - acc: 0.7544 - val_loss: 0.5445 - val_acc: 0.7053
Epoch 118/200
1136/1136 [==============================] - 8s - loss: 0.5518 - acc: 0.7412 - val_loss: 0.5579 - val_acc: 0.7123
Epoch 119/200
1136/1136 [==============================] - 8s - loss: 0.5139 - acc: 0.7764 - val_loss: 0.5594 - val_acc: 0.7158
Epoch 120/200
1136/1136 [==============================] - 8s - loss: 0.5193 - acc: 0.7421 - val_loss: 0.5644 - val_acc: 0.7158
Epoch 121/200
1136/1136 [==============================] - 9s - loss: 0.5432 - acc: 0.7570 - val_loss: 0.5515 - val_acc: 0.7053
Epoch 122/200
1136/1136 [==============================] - 8s - loss: 0.5349 - acc: 0.7491 - val_loss: 0.5520 - val_acc: 0.7053
Epoch 123/200
1136/1136 [==============================] - 8s - loss: 0.5378 - acc: 0.7614 - val_loss: 0.5619 - val_acc: 0.7193
Epoch 124/200
1136/1136 [==============================] - 8s - loss: 0.5549 - acc: 0.7218 - val_loss: 0.5537 - val_acc: 0.7123
Epoch 125/200
1136/1136 [==============================] - 8s - loss: 0.5184 - acc: 0.7518 - val_loss: 0.5510 - val_acc: 0.7193
Epoch 126/200
1136/1136 [==============================] - 8s - loss: 0.5217 - acc: 0.7412 - val_loss: 0.5619 - val_acc: 0.7123
Epoch 127/200
1136/1136 [==============================] - 8s - loss: 0.5291 - acc: 0.7518 - val_loss: 0.5481 - val_acc: 0.7088
Epoch 128/200
1136/1136 [==============================] - 8s - loss: 0.5374 - acc: 0.7465 - val_loss: 0.5537 - val_acc: 0.7123
Epoch 129/200
1136/1136 [==============================] - 8s - loss: 0.5329 - acc: 0.7509 - val_loss: 0.5548 - val_acc: 0.7193
Epoch 130/200
1136/1136 [==============================] - 8s - loss: 0.5236 - acc: 0.7465 - val_loss: 0.5543 - val_acc: 0.7193
Epoch 131/200
1136/1136 [==============================] - 8s - loss: 0.5277 - acc: 0.7702 - val_loss: 0.5628 - val_acc: 0.7158
Epoch 132/200
1136/1136 [==============================] - 8s - loss: 0.4765 - acc: 0.7553 - val_loss: 0.5727 - val_acc: 0.7088
Epoch 133/200
1136/1136 [==============================] - 8s - loss: 0.5142 - acc: 0.7359 - val_loss: 0.5615 - val_acc: 0.7053
Epoch 134/200
1136/1136 [==============================] - 8s - loss: 0.5142 - acc: 0.7588 - val_loss: 0.5461 - val_acc: 0.7158
Epoch 135/200
1136/1136 [==============================] - 9s - loss: 0.5412 - acc: 0.7438 - val_loss: 0.5364 - val_acc: 0.7123
Epoch 136/200
1136/1136 [==============================] - 8s - loss: 0.5145 - acc: 0.7421 - val_loss: 0.5428 - val_acc: 0.7088
Epoch 137/200
1136/1136 [==============================] - 8s - loss: 0.4913 - acc: 0.7667 - val_loss: 0.5520 - val_acc: 0.7298
Epoch 138/200
1136/1136 [==============================] - 8s - loss: 0.5395 - acc: 0.7394 - val_loss: 0.5477 - val_acc: 0.7368
Epoch 139/200
1136/1136 [==============================] - 8s - loss: 0.5141 - acc: 0.7509 - val_loss: 0.5469 - val_acc: 0.7404
Epoch 140/200
1136/1136 [==============================] - 8s - loss: 0.5006 - acc: 0.7658 - val_loss: 0.5547 - val_acc: 0.7404
Epoch 141/200
1136/1136 [==============================] - 8s - loss: 0.5115 - acc: 0.7474 - val_loss: 0.5436 - val_acc: 0.7474
Epoch 142/200
1136/1136 [==============================] - 8s - loss: 0.5054 - acc: 0.7623 - val_loss: 0.5472 - val_acc: 0.7439
Epoch 143/200
1136/1136 [==============================] - 8s - loss: 0.5079 - acc: 0.7773 - val_loss: 0.5611 - val_acc: 0.7404
Epoch 144/200
1136/1136 [==============================] - 8s - loss: 0.5208 - acc: 0.7658 - val_loss: 0.5532 - val_acc: 0.7439
Epoch 145/200
1136/1136 [==============================] - 8s - loss: 0.5374 - acc: 0.7650 - val_loss: 0.5377 - val_acc: 0.7298
Epoch 146/200
1136/1136 [==============================] - 8s - loss: 0.5145 - acc: 0.7579 - val_loss: 0.5579 - val_acc: 0.7368
Epoch 147/200
1136/1136 [==============================] - 8s - loss: 0.4562 - acc: 0.7887 - val_loss: 0.5775 - val_acc: 0.7333
Epoch 148/200
1136/1136 [==============================] - 9s - loss: 0.5325 - acc: 0.7755 - val_loss: 0.5677 - val_acc: 0.7228
Epoch 149/200
1136/1136 [==============================] - 8s - loss: 0.4741 - acc: 0.7738 - val_loss: 0.5642 - val_acc: 0.7228
Epoch 150/200
1136/1136 [==============================] - 8s - loss: 0.5046 - acc: 0.7746 - val_loss: 0.5598 - val_acc: 0.7439
Epoch 151/200
1136/1136 [==============================] - 8s - loss: 0.4857 - acc: 0.7852 - val_loss: 0.5731 - val_acc: 0.7368
Epoch 152/200
1136/1136 [==============================] - 8s - loss: 0.4933 - acc: 0.7729 - val_loss: 0.5626 - val_acc: 0.7439
Epoch 153/200
1136/1136 [==============================] - 8s - loss: 0.4944 - acc: 0.7676 - val_loss: 0.5594 - val_acc: 0.7333
Epoch 154/200
1136/1136 [==============================] - 8s - loss: 0.5168 - acc: 0.7597 - val_loss: 0.5575 - val_acc: 0.7439
Epoch 155/200
1136/1136 [==============================] - 8s - loss: 0.5058 - acc: 0.7562 - val_loss: 0.5644 - val_acc: 0.7368
Epoch 156/200
1136/1136 [==============================] - 8s - loss: 0.4880 - acc: 0.7650 - val_loss: 0.5788 - val_acc: 0.7333
Epoch 157/200
1136/1136 [==============================] - 8s - loss: 0.4809 - acc: 0.7852 - val_loss: 0.5775 - val_acc: 0.7368
Epoch 158/200
1136/1136 [==============================] - 8s - loss: 0.5249 - acc: 0.7456 - val_loss: 0.5619 - val_acc: 0.7439
Epoch 159/200
1136/1136 [==============================] - 8s - loss: 0.4780 - acc: 0.7667 - val_loss: 0.5725 - val_acc: 0.7404
Epoch 160/200
1136/1136 [==============================] - 8s - loss: 0.4973 - acc: 0.7614 - val_loss: 0.5730 - val_acc: 0.7368
Epoch 161/200
1136/1136 [==============================] - 8s - loss: 0.4692 - acc: 0.7746 - val_loss: 0.5756 - val_acc: 0.7368
Epoch 162/200
1136/1136 [==============================] - 8s - loss: 0.4828 - acc: 0.7782 - val_loss: 0.5838 - val_acc: 0.7368
Epoch 163/200
1136/1136 [==============================] - 8s - loss: 0.4819 - acc: 0.7914 - val_loss: 0.6020 - val_acc: 0.7404
Epoch 164/200
1136/1136 [==============================] - 8s - loss: 0.4829 - acc: 0.7764 - val_loss: 0.5975 - val_acc: 0.7439
Epoch 165/200
1136/1136 [==============================] - 9s - loss: 0.5060 - acc: 0.7746 - val_loss: 0.5773 - val_acc: 0.7439
Epoch 166/200
1136/1136 [==============================] - 8s - loss: 0.4904 - acc: 0.7817 - val_loss: 0.5724 - val_acc: 0.7298
Epoch 167/200
1136/1136 [==============================] - 8s - loss: 0.4901 - acc: 0.7702 - val_loss: 0.5726 - val_acc: 0.7193
Epoch 168/200
1136/1136 [==============================] - 9s - loss: 0.4952 - acc: 0.7641 - val_loss: 0.5742 - val_acc: 0.7298
Epoch 169/200
1136/1136 [==============================] - 8s - loss: 0.4792 - acc: 0.7658 - val_loss: 0.5948 - val_acc: 0.7368
Epoch 170/200
1136/1136 [==============================] - 8s - loss: 0.4963 - acc: 0.7667 - val_loss: 0.5942 - val_acc: 0.7368
Epoch 171/200
1136/1136 [==============================] - 8s - loss: 0.4930 - acc: 0.7658 - val_loss: 0.5912 - val_acc: 0.7439
Epoch 172/200
1136/1136 [==============================] - 8s - loss: 0.4595 - acc: 0.7799 - val_loss: 0.6098 - val_acc: 0.7439
Epoch 173/200
1136/1136 [==============================] - 8s - loss: 0.4624 - acc: 0.7914 - val_loss: 0.6077 - val_acc: 0.7474
Epoch 174/200
1136/1136 [==============================] - 8s - loss: 0.4384 - acc: 0.8160 - val_loss: 0.6035 - val_acc: 0.7474
Epoch 175/200
1136/1136 [==============================] - 8s - loss: 0.4980 - acc: 0.7650 - val_loss: 0.5968 - val_acc: 0.7333
Epoch 176/200
1136/1136 [==============================] - 8s - loss: 0.4719 - acc: 0.7826 - val_loss: 0.6061 - val_acc: 0.7228
Epoch 177/200
1136/1136 [==============================] - 8s - loss: 0.4834 - acc: 0.7861 - val_loss: 0.5947 - val_acc: 0.7298
Epoch 178/200
1136/1136 [==============================] - 9s - loss: 0.4840 - acc: 0.7879 - val_loss: 0.5893 - val_acc: 0.7193
Epoch 179/200
1136/1136 [==============================] - 8s - loss: 0.4581 - acc: 0.7746 - val_loss: 0.5955 - val_acc: 0.7193
Epoch 180/200
1136/1136 [==============================] - 8s - loss: 0.4454 - acc: 0.7905 - val_loss: 0.6171 - val_acc: 0.7158
Epoch 181/200
1136/1136 [==============================] - 8s - loss: 0.4831 - acc: 0.7755 - val_loss: 0.5998 - val_acc: 0.7053
Epoch 182/200
1136/1136 [==============================] - 8s - loss: 0.4528 - acc: 0.7905 - val_loss: 0.6016 - val_acc: 0.7053
Epoch 183/200
1136/1136 [==============================] - 8s - loss: 0.5039 - acc: 0.7764 - val_loss: 0.5833 - val_acc: 0.7053
Epoch 184/200
1136/1136 [==============================] - 8s - loss: 0.4621 - acc: 0.7817 - val_loss: 0.5889 - val_acc: 0.7053
Epoch 185/200
1136/1136 [==============================] - 9s - loss: 0.4604 - acc: 0.7975 - val_loss: 0.6056 - val_acc: 0.7018
Epoch 186/200
1136/1136 [==============================] - 8s - loss: 0.4758 - acc: 0.7764 - val_loss: 0.5772 - val_acc: 0.7088
Epoch 187/200
1136/1136 [==============================] - 8s - loss: 0.4715 - acc: 0.7905 - val_loss: 0.5852 - val_acc: 0.7158
Epoch 188/200
1136/1136 [==============================] - 8s - loss: 0.4476 - acc: 0.7958 - val_loss: 0.6157 - val_acc: 0.7123
Epoch 189/200
1136/1136 [==============================] - 9s - loss: 0.4981 - acc: 0.7720 - val_loss: 0.5960 - val_acc: 0.7088
Epoch 190/200
1136/1136 [==============================] - 8s - loss: 0.4642 - acc: 0.7931 - val_loss: 0.5790 - val_acc: 0.7228
Epoch 191/200
1136/1136 [==============================] - 8s - loss: 0.4739 - acc: 0.7702 - val_loss: 0.5869 - val_acc: 0.7263
Epoch 192/200
1136/1136 [==============================] - 8s - loss: 0.4756 - acc: 0.7861 - val_loss: 0.5949 - val_acc: 0.7193
Epoch 193/200
1136/1136 [==============================] - 8s - loss: 0.4778 - acc: 0.7755 - val_loss: 0.5952 - val_acc: 0.7333
Epoch 194/200
1136/1136 [==============================] - 8s - loss: 0.4551 - acc: 0.8046 - val_loss: 0.6052 - val_acc: 0.7088
Epoch 195/200
1136/1136 [==============================] - 8s - loss: 0.4729 - acc: 0.7799 - val_loss: 0.5947 - val_acc: 0.7053
Epoch 196/200
1136/1136 [==============================] - 8s - loss: 0.4696 - acc: 0.7861 - val_loss: 0.6010 - val_acc: 0.7158
Epoch 197/200
1136/1136 [==============================] - 8s - loss: 0.4494 - acc: 0.7914 - val_loss: 0.6092 - val_acc: 0.7158
Epoch 198/200
1136/1136 [==============================] - 8s - loss: 0.4453 - acc: 0.7826 - val_loss: 0.5993 - val_acc: 0.7193
Epoch 199/200
1136/1136 [==============================] - 8s - loss: 0.4348 - acc: 0.8134 - val_loss: 0.6140 - val_acc: 0.7298
Epoch 200/200
1136/1136 [==============================] - 8s - loss: 0.4714 - acc: 0.7817 - val_loss: 0.6132 - val_acc: 0.7368
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[274]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x2b78748d0&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [207]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># works</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="c1">#embedded_sequences = Dropout(0.5)(embedded_sequences)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normed_twts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-207-7266712ac3e2&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span> lstm_text_model.fit(normed_twts, labels,
<span class="ansi-green-fg">---&gt; 23</span><span class="ansi-red-fg">           epochs=200, batch_size=128,validation_split=0.2)
</span>
<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1356</span>             class_weight<span class="ansi-blue-fg">=</span>class_weight<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1357</span>             check_batch_axis<span class="ansi-blue-fg">=</span>False<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1358</span><span class="ansi-red-fg">             batch_size=batch_size)
</span><span class="ansi-green-intense-fg ansi-bold">   1359</span>         <span class="ansi-red-fg"># Prepare validation data.</span>
<span class="ansi-green-intense-fg ansi-bold">   1360</span>         <span class="ansi-green-fg">if</span> validation_data<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_standardize_user_data</span><span class="ansi-blue-fg">(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)</span>
<span class="ansi-green-intense-fg ansi-bold">   1244</span>                           <span class="ansi-green-fg">for</span> <span class="ansi-blue-fg">(</span>ref<span class="ansi-blue-fg">,</span> sw<span class="ansi-blue-fg">,</span> cw<span class="ansi-blue-fg">,</span> mode<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1245</span>                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]
<span class="ansi-green-fg">-&gt; 1246</span><span class="ansi-red-fg">         </span>_check_array_lengths<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> sample_weights<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1247</span>         _check_loss_and_target_compatibility(y,
<span class="ansi-green-intense-fg ansi-bold">   1248</span>                                              self<span class="ansi-blue-fg">.</span>_feed_loss_fns<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_check_array_lengths</span><span class="ansi-blue-fg">(inputs, targets, weights)</span>
<span class="ansi-green-intense-fg ansi-bold">    235</span>                          <span class="ansi-blue-fg">'the same number of samples as target arrays. '</span>
<span class="ansi-green-intense-fg ansi-bold">    236</span>                          <span class="ansi-blue-fg">'Found '</span> <span class="ansi-blue-fg">+</span> str<span class="ansi-blue-fg">(</span>list<span class="ansi-blue-fg">(</span>set_x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> <span class="ansi-blue-fg">' input samples '</span>
<span class="ansi-green-fg">--&gt; 237</span><span class="ansi-red-fg">                          'and ' + str(list(set_y)[0]) + ' target samples.')
</span><span class="ansi-green-intense-fg ansi-bold">    238</span>     <span class="ansi-green-fg">if</span> len<span class="ansi-blue-fg">(</span>set_w<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">&gt;</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    239</span>         raise ValueError('All sample_weight arrays should have '

<span class="ansi-red-fg">ValueError</span>: Input arrays should have the same number of samples as target arrays. Found 158 input samples and 1421 target samples.</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># For saving info after each epoch, from https://machinelearningmastery.com/check-point-deep-learning-models-keras/</span>

<span class="c1">#filepath = "weights.best.hdf5"</span>
<span class="c1">#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')</span>
<span class="c1">#callbacks_list = [checkpoint]</span>
<span class="c1"># Fit the model</span>
<span class="c1">#model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [146]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="err">†</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#filepath="weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"</span>
<span class="c1">#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')</span>
<span class="c1">#callbacks_list = [checkpoint]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [155]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">CSVLogger</span>
<span class="c1">#CSVLogger(filename, separator=',', append=False)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Try-saving">Try saving<a class="anchor-link" href="#Try-saving">¶</a></h1><ol>
<li>loss/accuracy to file</li>
<li>best model to file</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># works</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="c1">#embedded_sequences = Dropout(0.5)(embedded_sequences)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="c1">#class LossHistory(Callback):</span>
<span class="c1">#    def on_train_begin(self, logs={}):</span>
<span class="c1">#        self.losses = []</span>
<span class="c1">#</span>
<span class="c1">#    def on_batch_end(self, batch, logs={}):</span>
<span class="c1">#        self.losses.append(logs.get('loss'))</span>
<span class="c1">#        </span>
<span class="n">history</span> <span class="o">=</span> <span class="n">LossHistory</span><span class="p">()</span>
<span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">'weights.hdf5'</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">csv_logger</span> <span class="o">=</span> <span class="n">CSVLogger</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">'test_log'</span><span class="p">)</span>
<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normed_twts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">,</span><span class="n">csv_logger</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-6-11fcca373e8d&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-red-fg"># works</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span>sequence_input <span class="ansi-blue-fg">=</span> Input<span class="ansi-blue-fg">(</span>shape<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">(</span>MAX_SEQUENCE_LENGTH<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> dtype<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'int32'</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> embedded_sequences <span class="ansi-blue-fg">=</span> embedding_layer<span class="ansi-blue-fg">(</span>sequence_input<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-red-fg">#embedded_sequences = Dropout(0.5)(embedded_sequences)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> 

<span class="ansi-red-fg">NameError</span>: name 'Input' is not defined</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#history</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Try-loading-model-and-then-predicting.-Get-confusion-matrix-etc.">Try loading model and then predicting. Get confusion matrix etc.<a class="anchor-link" href="#Try-loading-model-and-then-predicting.-Get-confusion-matrix-etc.">¶</a></h3><ol>
<li>tokenize text for the test set</li>
<li>is the embedding layer the same?</li>
<li>convert the test labels to categorical</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [164]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [166]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model_loaded</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">'weights.hdf5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [218]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">test_truth_values</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'Sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">test_truth_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [175]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">raw_test_twts</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">'SentimentText'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [177]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#raw_test_twts</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [178]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">MAX_NB_WORDS</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">raw_test_twts</span><span class="p">)</span>
<span class="n">test_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">raw_test_twts</span><span class="p">)</span>

<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> unique tokens.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>

<span class="n">normed_twts</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">test_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span> <span class="c1"># append zeros to make tweet length consistent</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'Shape of data tensor:'</span><span class="p">,</span> <span class="n">normed_twts</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Shape of label tensor:'</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 1000 unique tokens.
('Shape of data tensor:', (158, 50))
('Shape of label tensor:', (1421, 2))
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [212]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">pred_probs</span> <span class="o">=</span> <span class="n">model_loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">normed_twts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [213]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">get_preds</span><span class="p">(</span><span class="n">pred_probs</span><span class="p">):</span>
    <span class="sd">"""Average first and last element of a 1-D array"""</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">pred_probs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="n">pred_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [214]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">func1d</span><span class="o">=</span><span class="n">get_preds</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">arr</span><span class="o">=</span><span class="n">pred_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Get-confusion-matrix-here!">Get confusion matrix here!<a class="anchor-link" href="#Get-confusion-matrix-here!">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [224]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">roc_auc_score</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [223]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">test_truth_values</span><span class="p">,</span><span class="n">y_pred</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[223]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array([[41, 36],
       [37, 44]])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [230]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#pred_probs[:,1]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [231]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">test_truth_values</span><span class="p">,</span><span class="n">y_score</span><span class="o">=</span><span class="n">pred_probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[231]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>0.55683822350489021</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [215]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">preds</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[215]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [193]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="p">((</span><span class="n">preds</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[193]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array([[False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False],
       [ True, False],
       [False,  True],
       [ True, False],
       [ True, False],
       [ True, False],
       [False,  True],
       [False,  True],
       [False,  True],
       [ True, False]], dtype=bool)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [232]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># works</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>

<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate)</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>


<span class="n">lstm_text_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>

<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="k">class</span> <span class="nc">LossHistory</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="p">{}):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="p">{}):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'loss'</span><span class="p">))</span>
        
<span class="n">history</span> <span class="o">=</span> <span class="n">LossHistory</span><span class="p">()</span>
<span class="c1">#checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)</span>
<span class="c1">#csv_logger = CSVLogger(filename='test_log')</span>
<span class="n">lstm_text_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normed_twts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">history</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-232-c4c5e95e0ba1&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span> <span class="ansi-red-fg">#csv_logger = CSVLogger(filename='test_log')</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span> lstm_text_model.fit(normed_twts, labels,
<span class="ansi-green-fg">---&gt; 32</span><span class="ansi-red-fg">           epochs=5, batch_size=128,validation_split=0.2,callbacks=[history])
</span>
<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1356</span>             class_weight<span class="ansi-blue-fg">=</span>class_weight<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1357</span>             check_batch_axis<span class="ansi-blue-fg">=</span>False<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1358</span><span class="ansi-red-fg">             batch_size=batch_size)
</span><span class="ansi-green-intense-fg ansi-bold">   1359</span>         <span class="ansi-red-fg"># Prepare validation data.</span>
<span class="ansi-green-intense-fg ansi-bold">   1360</span>         <span class="ansi-green-fg">if</span> validation_data<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_standardize_user_data</span><span class="ansi-blue-fg">(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)</span>
<span class="ansi-green-intense-fg ansi-bold">   1244</span>                           <span class="ansi-green-fg">for</span> <span class="ansi-blue-fg">(</span>ref<span class="ansi-blue-fg">,</span> sw<span class="ansi-blue-fg">,</span> cw<span class="ansi-blue-fg">,</span> mode<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1245</span>                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]
<span class="ansi-green-fg">-&gt; 1246</span><span class="ansi-red-fg">         </span>_check_array_lengths<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> sample_weights<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1247</span>         _check_loss_and_target_compatibility(y,
<span class="ansi-green-intense-fg ansi-bold">   1248</span>                                              self<span class="ansi-blue-fg">.</span>_feed_loss_fns<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_check_array_lengths</span><span class="ansi-blue-fg">(inputs, targets, weights)</span>
<span class="ansi-green-intense-fg ansi-bold">    235</span>                          <span class="ansi-blue-fg">'the same number of samples as target arrays. '</span>
<span class="ansi-green-intense-fg ansi-bold">    236</span>                          <span class="ansi-blue-fg">'Found '</span> <span class="ansi-blue-fg">+</span> str<span class="ansi-blue-fg">(</span>list<span class="ansi-blue-fg">(</span>set_x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> <span class="ansi-blue-fg">' input samples '</span>
<span class="ansi-green-fg">--&gt; 237</span><span class="ansi-red-fg">                          'and ' + str(list(set_y)[0]) + ' target samples.')
</span><span class="ansi-green-intense-fg ansi-bold">    238</span>     <span class="ansi-green-fg">if</span> len<span class="ansi-blue-fg">(</span>set_w<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">&gt;</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    239</span>         raise ValueError('All sample_weight arrays should have '

<span class="ansi-red-fg">ValueError</span>: Input arrays should have the same number of samples as target arrays. Found 158 input samples and 1421 target samples.</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [152]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.72025269, 0.69783241, 0.7075066, 0.69239682, 0.7294426, 0.73108113, 0.70959055, 0.71615851, 0.74441814, 0.70909685, 0.67699343, 0.67883885, 0.6813643, 0.74068528, 0.75949514, 0.70605087, 0.6850968, 0.70608282, 0.72900188, 0.72371936, 0.68646336, 0.6814993, 0.66767734, 0.7154178, 0.75811887, 0.70580626, 0.7465536, 0.70256901, 0.69963771, 0.72974288, 0.68019539, 0.74535739, 0.68574411, 0.71051663, 0.69415092, 0.69757336, 0.6692493, 0.67864335, 0.68992305, 0.68607128, 0.7280122, 0.74837089, 0.69359052, 0.7264241, 0.69326633]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [153]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">history</span><span class="o">.</span><span class="n">losses</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[153]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>[0.72025269,
 0.69783241,
 0.7075066,
 0.69239682,
 0.7294426,
 0.73108113,
 0.70959055,
 0.71615851,
 0.74441814,
 0.70909685,
 0.67699343,
 0.67883885,
 0.6813643,
 0.74068528,
 0.75949514,
 0.70605087,
 0.6850968,
 0.70608282,
 0.72900188,
 0.72371936,
 0.68646336,
 0.6814993,
 0.66767734,
 0.7154178,
 0.75811887,
 0.70580626,
 0.7465536,
 0.70256901,
 0.69963771,
 0.72974288,
 0.68019539,
 0.74535739,
 0.68574411,
 0.71051663,
 0.69415092,
 0.69757336,
 0.6692493,
 0.67864335,
 0.68992305,
 0.68607128,
 0.7280122,
 0.74837089,
 0.69359052,
 0.7264241,
 0.69326633]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [145]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'Sentiment'</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[145]:</div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>ItemID</th>
<th>SentimentSource</th>
<th>SentimentText</th>
</tr>
<tr>
<th>Sentiment</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>788435</td>
<td>788435</td>
<td>788435</td>
</tr>
<tr>
<th>1</th>
<td>790177</td>
<td>790177</td>
<td>790177</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [32]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">df_train</span><span class="p">[</span><span class="s1">'Sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[32]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>142074</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Text-features-and-manual-features">Text features and manual features<a class="anchor-link" href="#Text-features-and-manual-features">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [23]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="c1">#model.add(Dense(64, input_dim=64,</span>
<span class="c1">#                kernel_regularizer=regularizers.l2(0.01),</span>
<span class="c1">#                activity_regularizer=regularizers.l1(0.01)))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># First, load and define architecture for inception</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1">#LSTM_layer = Bidirectional(LSTM(units=128))(embedded_sequences)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="n">derived_features</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">34</span><span class="p">,))</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">derived_features</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">LSTM_layer</span><span class="p">,</span><span class="n">hidden_layer</span><span class="p">])</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>
<span class="c1">#hidden_layer_3 = Dropout(0.5)(hidden_layer_3)</span>



<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional additional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate) # optional additional hidden layer</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate) # optional additional hidden layer</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels_mw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span> <span class="c1"># comment out if penultimate layer is used</span>

<span class="n">lstm_text_and_manual_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">sequence_input</span><span class="p">,</span><span class="n">derived_features</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">normed_twts</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span><span class="n">manual_features_scaled</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">]],</span> <span class="n">labels_mw</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1079 samples, validate on 270 samples
Epoch 1/300
1079/1079 [==============================] - 6s - loss: 1.6457 - acc: 0.3281 - val_loss: 1.2292 - val_acc: 0.5370
Epoch 2/300
1079/1079 [==============================] - 3s - loss: 1.4830 - acc: 0.4180 - val_loss: 1.2057 - val_acc: 0.5519
Epoch 3/300
1079/1079 [==============================] - 4s - loss: 1.4444 - acc: 0.4032 - val_loss: 1.2229 - val_acc: 0.5519
Epoch 4/300
1079/1079 [==============================] - 4s - loss: 1.3499 - acc: 0.4337 - val_loss: 1.2147 - val_acc: 0.5407
Epoch 5/300
1079/1079 [==============================] - 4s - loss: 1.3373 - acc: 0.4374 - val_loss: 1.2084 - val_acc: 0.5630
Epoch 6/300
1079/1079 [==============================] - 4s - loss: 1.2906 - acc: 0.4634 - val_loss: 1.2027 - val_acc: 0.5556
Epoch 7/300
1079/1079 [==============================] - 4s - loss: 1.2718 - acc: 0.4643 - val_loss: 1.1872 - val_acc: 0.5593
Epoch 8/300
1079/1079 [==============================] - 4s - loss: 1.2566 - acc: 0.4838 - val_loss: 1.1839 - val_acc: 0.5519
Epoch 9/300
1079/1079 [==============================] - 4s - loss: 1.2287 - acc: 0.4866 - val_loss: 1.1792 - val_acc: 0.5593
Epoch 10/300
1079/1079 [==============================] - 4s - loss: 1.2222 - acc: 0.4986 - val_loss: 1.1775 - val_acc: 0.5519
Epoch 11/300
1079/1079 [==============================] - 4s - loss: 1.2101 - acc: 0.5014 - val_loss: 1.1677 - val_acc: 0.5519
Epoch 12/300
1079/1079 [==============================] - 4s - loss: 1.2437 - acc: 0.5116 - val_loss: 1.1592 - val_acc: 0.5519
Epoch 13/300
1079/1079 [==============================] - 4s - loss: 1.2264 - acc: 0.4940 - val_loss: 1.1609 - val_acc: 0.5519
Epoch 14/300
1079/1079 [==============================] - 4s - loss: 1.2064 - acc: 0.4856 - val_loss: 1.1596 - val_acc: 0.5556
Epoch 15/300
1079/1079 [==============================] - 4s - loss: 1.1808 - acc: 0.5125 - val_loss: 1.1484 - val_acc: 0.5556
Epoch 16/300
1079/1079 [==============================] - 5s - loss: 1.1690 - acc: 0.5181 - val_loss: 1.1412 - val_acc: 0.5593
Epoch 17/300
1079/1079 [==============================] - 5s - loss: 1.1892 - acc: 0.5134 - val_loss: 1.1364 - val_acc: 0.5593
Epoch 18/300
1079/1079 [==============================] - 5s - loss: 1.1566 - acc: 0.5264 - val_loss: 1.1329 - val_acc: 0.5556
Epoch 19/300
1079/1079 [==============================] - 5s - loss: 1.1803 - acc: 0.5273 - val_loss: 1.1349 - val_acc: 0.5481
Epoch 20/300
1079/1079 [==============================] - 5s - loss: 1.1427 - acc: 0.5477 - val_loss: 1.1294 - val_acc: 0.5481
Epoch 21/300
1079/1079 [==============================] - 4s - loss: 1.1633 - acc: 0.5153 - val_loss: 1.1212 - val_acc: 0.5519
Epoch 22/300
1079/1079 [==============================] - 5s - loss: 1.1503 - acc: 0.5385 - val_loss: 1.1113 - val_acc: 0.5556
Epoch 23/300
1079/1079 [==============================] - 4s - loss: 1.1380 - acc: 0.5375 - val_loss: 1.1004 - val_acc: 0.5556
Epoch 24/300
1079/1079 [==============================] - 5s - loss: 1.1384 - acc: 0.5246 - val_loss: 1.0982 - val_acc: 0.5593
Epoch 25/300
1079/1079 [==============================] - 5s - loss: 1.1269 - acc: 0.5348 - val_loss: 1.0967 - val_acc: 0.5667
Epoch 26/300
1079/1079 [==============================] - 5s - loss: 1.1524 - acc: 0.5181 - val_loss: 1.0931 - val_acc: 0.5630
Epoch 27/300
1079/1079 [==============================] - 5s - loss: 1.1447 - acc: 0.5162 - val_loss: 1.0883 - val_acc: 0.5667
Epoch 28/300
1079/1079 [==============================] - 5s - loss: 1.1592 - acc: 0.5273 - val_loss: 1.0837 - val_acc: 0.5704
Epoch 29/300
1079/1079 [==============================] - 6s - loss: 1.1432 - acc: 0.5394 - val_loss: 1.0825 - val_acc: 0.5667
Epoch 30/300
1079/1079 [==============================] - 5s - loss: 1.1260 - acc: 0.5449 - val_loss: 1.0849 - val_acc: 0.5667
Epoch 31/300
1079/1079 [==============================] - 5s - loss: 1.1288 - acc: 0.5412 - val_loss: 1.0845 - val_acc: 0.5667
Epoch 32/300
1079/1079 [==============================] - 5s - loss: 1.1257 - acc: 0.5357 - val_loss: 1.0788 - val_acc: 0.5630
Epoch 33/300
1079/1079 [==============================] - 5s - loss: 1.1209 - acc: 0.5246 - val_loss: 1.0741 - val_acc: 0.5667
Epoch 34/300
1079/1079 [==============================] - 4s - loss: 1.1161 - acc: 0.5320 - val_loss: 1.0687 - val_acc: 0.5704
Epoch 35/300
1079/1079 [==============================] - 4s - loss: 1.0922 - acc: 0.5533 - val_loss: 1.0673 - val_acc: 0.5630
Epoch 36/300
1079/1079 [==============================] - 4s - loss: 1.1094 - acc: 0.5570 - val_loss: 1.0660 - val_acc: 0.5630
Epoch 37/300
1079/1079 [==============================] - 5s - loss: 1.1183 - acc: 0.5329 - val_loss: 1.0643 - val_acc: 0.5704
Epoch 38/300
1079/1079 [==============================] - 5s - loss: 1.0942 - acc: 0.5403 - val_loss: 1.0624 - val_acc: 0.5778
Epoch 39/300
1079/1079 [==============================] - 4s - loss: 1.0876 - acc: 0.5440 - val_loss: 1.0598 - val_acc: 0.5741
Epoch 40/300
1079/1079 [==============================] - 4s - loss: 1.1134 - acc: 0.5412 - val_loss: 1.0565 - val_acc: 0.5704
Epoch 41/300
1079/1079 [==============================] - 4s - loss: 1.1090 - acc: 0.5496 - val_loss: 1.0588 - val_acc: 0.5630
Epoch 42/300
1079/1079 [==============================] - 4s - loss: 1.0825 - acc: 0.5449 - val_loss: 1.0589 - val_acc: 0.5630
Epoch 43/300
1079/1079 [==============================] - 4s - loss: 1.0883 - acc: 0.5570 - val_loss: 1.0545 - val_acc: 0.5593
Epoch 44/300
1079/1079 [==============================] - 4s - loss: 1.1015 - acc: 0.5626 - val_loss: 1.0524 - val_acc: 0.5630
Epoch 45/300
1079/1079 [==============================] - 4s - loss: 1.0979 - acc: 0.5459 - val_loss: 1.0507 - val_acc: 0.5667
Epoch 46/300
1079/1079 [==============================] - 4s - loss: 1.1206 - acc: 0.5329 - val_loss: 1.0529 - val_acc: 0.5667
Epoch 47/300
1079/1079 [==============================] - 4s - loss: 1.1004 - acc: 0.5589 - val_loss: 1.0508 - val_acc: 0.5667
Epoch 48/300
1079/1079 [==============================] - 4s - loss: 1.1047 - acc: 0.5366 - val_loss: 1.0505 - val_acc: 0.5667
Epoch 49/300
 640/1079 [================&gt;.............] - ETA: 1s - loss: 1.0953 - acc: 0.5547</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [30]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># First, load and define architecture for inception</span>
<span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1">#LSTM_layer = Bidirectional(LSTM(units=128))(embedded_sequences)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">activity_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.15</span><span class="p">),</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.15</span><span class="p">)))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="c1">#LSTM_layer = Dropout(0.5)(LSTM_layer)</span>

<span class="n">derived_features</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">34</span><span class="p">,))</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">derived_features</span><span class="p">)</span>
<span class="c1">#hidden_layer = Dropout(0.5)(hidden_layer)</span>

<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">LSTM_layer</span><span class="p">,</span><span class="n">hidden_layer</span><span class="p">])</span>
<span class="c1">#merged_flat = Dropout(0.5)(merged_flat)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="c1">#hidden_layer_3 = Dropout(0.5)(hidden_layer_3)</span>
<span class="c1">#hidden_layer_3 = Dropout(0.5)(hidden_layer_3)</span>



<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional additional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate) # optional additional hidden layer</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate) # optional additional hidden layer</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels_mw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span> <span class="c1"># comment out if penultimate layer is used</span>

<span class="n">lstm_text_and_manual_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">sequence_input</span><span class="p">,</span><span class="n">derived_features</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'Adagrad'</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">normed_twts</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span><span class="n">manual_features_scaled</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">]],</span> <span class="n">labels_mw</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1079 samples, validate on 270 samples
Epoch 1/300
1079/1079 [==============================] - 8s - loss: 58.7533 - acc: 0.5227 - val_loss: 23.9846 - val_acc: 0.5815
Epoch 2/300
1079/1079 [==============================] - 3s - loss: 24.6012 - acc: 0.6024 - val_loss: 13.7000 - val_acc: 0.5926
Epoch 3/300
1079/1079 [==============================] - 4s - loss: 14.7170 - acc: 0.6293 - val_loss: 8.6459 - val_acc: 0.6148
Epoch 4/300
1079/1079 [==============================] - 4s - loss: 9.6332 - acc: 0.6469 - val_loss: 5.8885 - val_acc: 0.5889
Epoch 5/300
1079/1079 [==============================] - 4s - loss: 6.5781 - acc: 0.6525 - val_loss: 4.1720 - val_acc: 0.6111
Epoch 6/300
1079/1079 [==============================] - 4s - loss: 4.6232 - acc: 0.6747 - val_loss: 3.1159 - val_acc: 0.6185
Epoch 7/300
1079/1079 [==============================] - 4s - loss: 3.3722 - acc: 0.6701 - val_loss: 2.4696 - val_acc: 0.6148
Epoch 8/300
1079/1079 [==============================] - 4s - loss: 2.5382 - acc: 0.6812 - val_loss: 2.0490 - val_acc: 0.6037
Epoch 9/300
1079/1079 [==============================] - 4s - loss: 1.9852 - acc: 0.7016 - val_loss: 1.7645 - val_acc: 0.6000
Epoch 10/300
1079/1079 [==============================] - 4s - loss: 1.6205 - acc: 0.7016 - val_loss: 1.6059 - val_acc: 0.5852
Epoch 11/300
1079/1079 [==============================] - 4s - loss: 1.3735 - acc: 0.7173 - val_loss: 1.4946 - val_acc: 0.5741
Epoch 12/300
1079/1079 [==============================] - 4s - loss: 1.2001 - acc: 0.7220 - val_loss: 1.3861 - val_acc: 0.5926
Epoch 13/300
1079/1079 [==============================] - 4s - loss: 1.0717 - acc: 0.7405 - val_loss: 1.3724 - val_acc: 0.5667
Epoch 14/300
1079/1079 [==============================] - 4s - loss: 0.9840 - acc: 0.7433 - val_loss: 1.2889 - val_acc: 0.6037
Epoch 15/300
1079/1079 [==============================] - 4s - loss: 0.9402 - acc: 0.7683 - val_loss: 1.3086 - val_acc: 0.5630
Epoch 16/300
1079/1079 [==============================] - 4s - loss: 0.8626 - acc: 0.7646 - val_loss: 1.2537 - val_acc: 0.5963
Epoch 17/300
1079/1079 [==============================] - 4s - loss: 0.8108 - acc: 0.7766 - val_loss: 1.2628 - val_acc: 0.5852
Epoch 18/300
1079/1079 [==============================] - 4s - loss: 0.7802 - acc: 0.7794 - val_loss: 1.2556 - val_acc: 0.5667
Epoch 19/300
1079/1079 [==============================] - 5s - loss: 0.7462 - acc: 0.7915 - val_loss: 1.2386 - val_acc: 0.5667
Epoch 20/300
1079/1079 [==============================] - 5s - loss: 0.7284 - acc: 0.7989 - val_loss: 1.2294 - val_acc: 0.5926
Epoch 21/300
1079/1079 [==============================] - 5s - loss: 0.6991 - acc: 0.8026 - val_loss: 1.2351 - val_acc: 0.5741
Epoch 22/300
1079/1079 [==============================] - 4s - loss: 0.6920 - acc: 0.8165 - val_loss: 1.2322 - val_acc: 0.5630
Epoch 23/300
1079/1079 [==============================] - 4s - loss: 0.6759 - acc: 0.8202 - val_loss: 1.2449 - val_acc: 0.5852
Epoch 24/300
1079/1079 [==============================] - 4s - loss: 0.6412 - acc: 0.8285 - val_loss: 1.2605 - val_acc: 0.5519
Epoch 25/300
1079/1079 [==============================] - 4s - loss: 0.6172 - acc: 0.8369 - val_loss: 1.2518 - val_acc: 0.5778
Epoch 26/300
1079/1079 [==============================] - 4s - loss: 0.6169 - acc: 0.8378 - val_loss: 1.2909 - val_acc: 0.5407
Epoch 27/300
1079/1079 [==============================] - 4s - loss: 0.6000 - acc: 0.8480 - val_loss: 1.2630 - val_acc: 0.5593
Epoch 28/300
1079/1079 [==============================] - 4s - loss: 0.5738 - acc: 0.8489 - val_loss: 1.2293 - val_acc: 0.6000
Epoch 29/300
1079/1079 [==============================] - 4s - loss: 0.5471 - acc: 0.8675 - val_loss: 1.2512 - val_acc: 0.5704
Epoch 30/300
1079/1079 [==============================] - 4s - loss: 0.5359 - acc: 0.8712 - val_loss: 1.2470 - val_acc: 0.5741
Epoch 31/300
1079/1079 [==============================] - 4s - loss: 0.5488 - acc: 0.8786 - val_loss: 1.2681 - val_acc: 0.5667
Epoch 32/300
1079/1079 [==============================] - 4s - loss: 0.5159 - acc: 0.8684 - val_loss: 1.2632 - val_acc: 0.5778
Epoch 33/300
1079/1079 [==============================] - 4s - loss: 0.5678 - acc: 0.8591 - val_loss: 1.3458 - val_acc: 0.5296
Epoch 34/300
1079/1079 [==============================] - 4s - loss: 0.4922 - acc: 0.8962 - val_loss: 1.2805 - val_acc: 0.5741
Epoch 35/300
1079/1079 [==============================] - 4s - loss: 0.4744 - acc: 0.8971 - val_loss: 1.3453 - val_acc: 0.5222
Epoch 36/300
1079/1079 [==============================] - 4s - loss: 0.4698 - acc: 0.9036 - val_loss: 1.3052 - val_acc: 0.5593
Epoch 37/300
1079/1079 [==============================] - 4s - loss: 0.4714 - acc: 0.8971 - val_loss: 1.2950 - val_acc: 0.5704
Epoch 38/300
1079/1079 [==============================] - 4s - loss: 0.4435 - acc: 0.9129 - val_loss: 1.4394 - val_acc: 0.5074
Epoch 39/300
1079/1079 [==============================] - 4s - loss: 0.5624 - acc: 0.8656 - val_loss: 1.3378 - val_acc: 0.6074
Epoch 40/300
1079/1079 [==============================] - 4s - loss: 0.4651 - acc: 0.9036 - val_loss: 1.3054 - val_acc: 0.5741
Epoch 41/300
1079/1079 [==============================] - 4s - loss: 0.4263 - acc: 0.9129 - val_loss: 1.3009 - val_acc: 0.5741
Epoch 42/300
1079/1079 [==============================] - 4s - loss: 0.4098 - acc: 0.9147 - val_loss: 1.3121 - val_acc: 0.5852
Epoch 43/300
1079/1079 [==============================] - 4s - loss: 0.4190 - acc: 0.9138 - val_loss: 1.2968 - val_acc: 0.5926
Epoch 44/300
1079/1079 [==============================] - 4s - loss: 0.4873 - acc: 0.8869 - val_loss: 1.3969 - val_acc: 0.5407
Epoch 45/300
1079/1079 [==============================] - 4s - loss: 0.3969 - acc: 0.9203 - val_loss: 1.3180 - val_acc: 0.5741
Epoch 46/300
1079/1079 [==============================] - 4s - loss: 0.3855 - acc: 0.9259 - val_loss: 1.3281 - val_acc: 0.5778
Epoch 47/300
1079/1079 [==============================] - 4s - loss: 0.3704 - acc: 0.9314 - val_loss: 1.3559 - val_acc: 0.5519
Epoch 48/300
1079/1079 [==============================] - 4s - loss: 0.3850 - acc: 0.9296 - val_loss: 1.3754 - val_acc: 0.6037
Epoch 49/300
1079/1079 [==============================] - 4s - loss: 0.4068 - acc: 0.9212 - val_loss: 1.3358 - val_acc: 0.5667
Epoch 50/300
1079/1079 [==============================] - 4s - loss: 0.3702 - acc: 0.9333 - val_loss: 1.3527 - val_acc: 0.5630
Epoch 51/300
1079/1079 [==============================] - 4s - loss: 0.3463 - acc: 0.9416 - val_loss: 1.3415 - val_acc: 0.5815
Epoch 52/300
1079/1079 [==============================] - 4s - loss: 0.3426 - acc: 0.9379 - val_loss: 1.3671 - val_acc: 0.5556
Epoch 53/300
1079/1079 [==============================] - 4s - loss: 0.3457 - acc: 0.9425 - val_loss: 1.3505 - val_acc: 0.5852
Epoch 54/300
1079/1079 [==============================] - 4s - loss: 0.3285 - acc: 0.9388 - val_loss: 1.3638 - val_acc: 0.5704
Epoch 55/300
1079/1079 [==============================] - 4s - loss: 0.3360 - acc: 0.9444 - val_loss: 1.4187 - val_acc: 0.5556
Epoch 56/300
1079/1079 [==============================] - 4s - loss: 0.3283 - acc: 0.9500 - val_loss: 1.3894 - val_acc: 0.5667
Epoch 57/300
1079/1079 [==============================] - 4s - loss: 0.3495 - acc: 0.9425 - val_loss: 1.4444 - val_acc: 0.5444
Epoch 58/300
1079/1079 [==============================] - 5s - loss: 0.3380 - acc: 0.9370 - val_loss: 1.3933 - val_acc: 0.5667
Epoch 59/300
1079/1079 [==============================] - 4s - loss: 0.3220 - acc: 0.9527 - val_loss: 1.4673 - val_acc: 0.5704
Epoch 60/300
 256/1079 [======&gt;.......................] - ETA: 3s - loss: 0.3083 - acc: 0.9609</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-30-282ae84def94&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     33</span> 
<span class="ansi-green-intense-fg ansi-bold">     34</span> lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],
<span class="ansi-green-fg">---&gt; 35</span><span class="ansi-red-fg">           epochs=300, batch_size=128,validation_split=0.2)
</span>
<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1428</span>                               val_f<span class="ansi-blue-fg">=</span>val_f<span class="ansi-blue-fg">,</span> val_ins<span class="ansi-blue-fg">=</span>val_ins<span class="ansi-blue-fg">,</span> shuffle<span class="ansi-blue-fg">=</span>shuffle<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1429</span>                               callback_metrics<span class="ansi-blue-fg">=</span>callback_metrics<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1430</span><span class="ansi-red-fg">                               initial_epoch=initial_epoch)
</span><span class="ansi-green-intense-fg ansi-bold">   1431</span> 
<span class="ansi-green-intense-fg ansi-bold">   1432</span>     <span class="ansi-green-fg">def</span> evaluate<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">32</span><span class="ansi-blue-fg">,</span> verbose<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> sample_weight<span class="ansi-blue-fg">=</span>None<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_fit_loop</span><span class="ansi-blue-fg">(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)</span>
<span class="ansi-green-intense-fg ansi-bold">   1077</span>                 batch_logs<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">'size'</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> len<span class="ansi-blue-fg">(</span>batch_ids<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1078</span>                 callbacks<span class="ansi-blue-fg">.</span>on_batch_begin<span class="ansi-blue-fg">(</span>batch_index<span class="ansi-blue-fg">,</span> batch_logs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">-&gt; 1079</span><span class="ansi-red-fg">                 </span>outs <span class="ansi-blue-fg">=</span> f<span class="ansi-blue-fg">(</span>ins_batch<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1080</span>                 <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> isinstance<span class="ansi-blue-fg">(</span>outs<span class="ansi-blue-fg">,</span> list<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1081</span>                     outs <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>outs<span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, inputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2266</span>         updated = session.run(self.outputs + [self.updates_op],
<span class="ansi-green-intense-fg ansi-bold">   2267</span>                               feed_dict<span class="ansi-blue-fg">=</span>feed_dict<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 2268</span><span class="ansi-red-fg">                               **self.session_kwargs)
</span><span class="ansi-green-intense-fg ansi-bold">   2269</span>         <span class="ansi-green-fg">return</span> updated<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span>len<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>outputs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">   2270</span> 

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-fg">(self, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">    787</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    788</span>       result = self._run(None, fetches, feed_dict, options_ptr,
<span class="ansi-green-fg">--&gt; 789</span><span class="ansi-red-fg">                          run_metadata_ptr)
</span><span class="ansi-green-intense-fg ansi-bold">    790</span>       <span class="ansi-green-fg">if</span> run_metadata<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    791</span>         proto_data <span class="ansi-blue-fg">=</span> tf_session<span class="ansi-blue-fg">.</span>TF_GetBuffer<span class="ansi-blue-fg">(</span>run_metadata_ptr<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_run</span><span class="ansi-blue-fg">(self, handle, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">    995</span>     <span class="ansi-green-fg">if</span> final_fetches <span class="ansi-green-fg">or</span> final_targets<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    996</span>       results = self._do_run(handle, final_targets, final_fetches,
<span class="ansi-green-fg">--&gt; 997</span><span class="ansi-red-fg">                              feed_dict_string, options, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">    998</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    999</span>       results <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_do_run</span><span class="ansi-blue-fg">(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">   1130</span>     <span class="ansi-green-fg">if</span> handle <span class="ansi-green-fg">is</span> None<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1131</span>       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
<span class="ansi-green-fg">-&gt; 1132</span><span class="ansi-red-fg">                            target_list, options, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">   1133</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1134</span>       return self._do_call(_prun_fn, self._session, handle, feed_dict,

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_do_call</span><span class="ansi-blue-fg">(self, fn, *args)</span>
<span class="ansi-green-intense-fg ansi-bold">   1137</span>   <span class="ansi-green-fg">def</span> _do_call<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> fn<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1138</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">-&gt; 1139</span><span class="ansi-red-fg">       </span><span class="ansi-green-fg">return</span> fn<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1140</span>     <span class="ansi-green-fg">except</span> errors<span class="ansi-blue-fg">.</span>OpError <span class="ansi-green-fg">as</span> e<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1141</span>       message <span class="ansi-blue-fg">=</span> compat<span class="ansi-blue-fg">.</span>as_text<span class="ansi-blue-fg">(</span>e<span class="ansi-blue-fg">.</span>message<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_run_fn</span><span class="ansi-blue-fg">(session, feed_dict, fetch_list, target_list, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">   1119</span>         return tf_session.TF_Run(session, options,
<span class="ansi-green-intense-fg ansi-bold">   1120</span>                                  feed_dict<span class="ansi-blue-fg">,</span> fetch_list<span class="ansi-blue-fg">,</span> target_list<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1121</span><span class="ansi-red-fg">                                  status, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">   1122</span> 
<span class="ansi-green-intense-fg ansi-bold">   1123</span>     <span class="ansi-green-fg">def</span> _prun_fn<span class="ansi-blue-fg">(</span>session<span class="ansi-blue-fg">,</span> handle<span class="ansi-blue-fg">,</span> feed_dict<span class="ansi-blue-fg">,</span> fetch_list<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [18]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sgd</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[18]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>{'decay': 0.0010000000474974513,
 'lr': 0.009999999776482582,
 'momentum': 0.8999999761581421,
 'nesterov': True}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [29]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [30]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1">#LSTM_layer = Bidirectional(LSTM(units=128))(embedded_sequences)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="n">derived_features</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">34</span><span class="p">,))</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">derived_features</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">LSTM_layer</span><span class="p">,</span><span class="n">hidden_layer</span><span class="p">])</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>

<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional additional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate) # optional additional hidden layer</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate) # optional additional hidden layer</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels_mw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span> <span class="c1"># comment out if penultimate layer is used</span>

<span class="n">lstm_text_and_manual_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">sequence_input</span><span class="p">,</span><span class="n">derived_features</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">normed_twts</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span><span class="n">manual_features_scaled</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">]],</span> <span class="n">labels_mw</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 959 samples, validate on 240 samples
Epoch 1/300
959/959 [==============================] - 5s - loss: 1.4412 - acc: 0.3983 - val_loss: 1.1824 - val_acc: 0.5292
Epoch 2/300
959/959 [==============================] - 3s - loss: 1.3077 - acc: 0.4588 - val_loss: 1.1508 - val_acc: 0.5250
Epoch 3/300
959/959 [==============================] - 3s - loss: 1.2229 - acc: 0.4984 - val_loss: 1.1416 - val_acc: 0.5375
Epoch 4/300
959/959 [==============================] - 3s - loss: 1.2000 - acc: 0.5005 - val_loss: 1.1219 - val_acc: 0.5583
Epoch 5/300
959/959 [==============================] - 3s - loss: 1.1569 - acc: 0.5297 - val_loss: 1.1036 - val_acc: 0.5667
Epoch 6/300
959/959 [==============================] - 3s - loss: 1.1326 - acc: 0.5266 - val_loss: 1.0848 - val_acc: 0.5542
Epoch 7/300
959/959 [==============================] - 3s - loss: 1.1415 - acc: 0.5255 - val_loss: 1.0802 - val_acc: 0.5875
Epoch 8/300
959/959 [==============================] - 3s - loss: 1.1354 - acc: 0.5172 - val_loss: 1.0763 - val_acc: 0.5667
Epoch 9/300
959/959 [==============================] - 3s - loss: 1.1145 - acc: 0.5235 - val_loss: 1.0758 - val_acc: 0.5542
Epoch 10/300
959/959 [==============================] - 3s - loss: 1.1026 - acc: 0.5506 - val_loss: 1.0729 - val_acc: 0.5750
Epoch 11/300
959/959 [==============================] - 3s - loss: 1.0969 - acc: 0.5516 - val_loss: 1.0582 - val_acc: 0.6000
Epoch 12/300
959/959 [==============================] - 3s - loss: 1.0821 - acc: 0.5547 - val_loss: 1.0468 - val_acc: 0.6208
Epoch 13/300
959/959 [==============================] - 3s - loss: 1.0746 - acc: 0.5746 - val_loss: 1.0422 - val_acc: 0.6125
Epoch 14/300
959/959 [==============================] - 3s - loss: 1.0660 - acc: 0.5714 - val_loss: 1.0335 - val_acc: 0.6083
Epoch 15/300
959/959 [==============================] - 3s - loss: 1.0909 - acc: 0.5610 - val_loss: 1.0265 - val_acc: 0.6125
Epoch 16/300
959/959 [==============================] - 3s - loss: 1.0314 - acc: 0.5704 - val_loss: 1.0337 - val_acc: 0.6250
Epoch 17/300
959/959 [==============================] - 3s - loss: 1.0415 - acc: 0.5735 - val_loss: 1.0309 - val_acc: 0.6125
Epoch 18/300
959/959 [==============================] - 3s - loss: 1.0418 - acc: 0.5829 - val_loss: 1.0221 - val_acc: 0.6083
Epoch 19/300
959/959 [==============================] - 3s - loss: 1.0524 - acc: 0.5798 - val_loss: 1.0187 - val_acc: 0.6042
Epoch 20/300
959/959 [==============================] - 3s - loss: 1.0532 - acc: 0.5704 - val_loss: 1.0216 - val_acc: 0.6042
Epoch 21/300
959/959 [==============================] - 3s - loss: 1.0262 - acc: 0.5902 - val_loss: 1.0206 - val_acc: 0.5958
Epoch 22/300
959/959 [==============================] - 3s - loss: 1.0030 - acc: 0.6038 - val_loss: 1.0127 - val_acc: 0.5958
Epoch 23/300
959/959 [==============================] - 3s - loss: 1.0378 - acc: 0.5766 - val_loss: 1.0070 - val_acc: 0.5833
Epoch 24/300
959/959 [==============================] - 3s - loss: 1.0006 - acc: 0.5766 - val_loss: 1.0033 - val_acc: 0.6000
Epoch 25/300
959/959 [==============================] - 3s - loss: 1.0295 - acc: 0.5714 - val_loss: 1.0005 - val_acc: 0.6167
Epoch 26/300
959/959 [==============================] - 3s - loss: 1.0332 - acc: 0.5735 - val_loss: 1.0036 - val_acc: 0.6208
Epoch 27/300
959/959 [==============================] - 3s - loss: 1.0068 - acc: 0.5798 - val_loss: 1.0010 - val_acc: 0.6250
Epoch 28/300
959/959 [==============================] - 3s - loss: 1.0278 - acc: 0.5881 - val_loss: 0.9986 - val_acc: 0.6292
Epoch 29/300
959/959 [==============================] - 3s - loss: 1.0188 - acc: 0.6006 - val_loss: 1.0010 - val_acc: 0.6167
Epoch 30/300
959/959 [==============================] - 3s - loss: 1.0045 - acc: 0.5892 - val_loss: 0.9937 - val_acc: 0.6167
Epoch 31/300
959/959 [==============================] - 3s - loss: 1.0006 - acc: 0.5923 - val_loss: 0.9859 - val_acc: 0.6167
Epoch 32/300
959/959 [==============================] - 3s - loss: 1.0194 - acc: 0.5839 - val_loss: 0.9823 - val_acc: 0.6167
Epoch 33/300
959/959 [==============================] - 3s - loss: 0.9844 - acc: 0.6090 - val_loss: 0.9836 - val_acc: 0.6125
Epoch 34/300
959/959 [==============================] - 3s - loss: 1.0104 - acc: 0.5860 - val_loss: 0.9803 - val_acc: 0.6083
Epoch 35/300
959/959 [==============================] - 3s - loss: 1.0061 - acc: 0.5923 - val_loss: 0.9770 - val_acc: 0.6125
Epoch 36/300
959/959 [==============================] - 3s - loss: 0.9829 - acc: 0.5996 - val_loss: 0.9740 - val_acc: 0.6125
Epoch 37/300
959/959 [==============================] - 3s - loss: 0.9731 - acc: 0.6069 - val_loss: 0.9758 - val_acc: 0.6125
Epoch 38/300
959/959 [==============================] - 3s - loss: 0.9658 - acc: 0.6142 - val_loss: 0.9786 - val_acc: 0.6292
Epoch 39/300
959/959 [==============================] - 3s - loss: 0.9796 - acc: 0.5881 - val_loss: 0.9747 - val_acc: 0.6333
Epoch 40/300
959/959 [==============================] - 3s - loss: 0.9633 - acc: 0.6048 - val_loss: 0.9686 - val_acc: 0.6167
Epoch 41/300
959/959 [==============================] - 3s - loss: 0.9692 - acc: 0.6225 - val_loss: 0.9637 - val_acc: 0.6250
Epoch 42/300
959/959 [==============================] - 3s - loss: 0.9822 - acc: 0.5985 - val_loss: 0.9608 - val_acc: 0.6208
Epoch 43/300
959/959 [==============================] - 3s - loss: 0.9699 - acc: 0.6038 - val_loss: 0.9585 - val_acc: 0.6250
Epoch 44/300
959/959 [==============================] - 3s - loss: 0.9700 - acc: 0.6163 - val_loss: 0.9558 - val_acc: 0.6333
Epoch 45/300
959/959 [==============================] - 3s - loss: 0.9626 - acc: 0.6100 - val_loss: 0.9544 - val_acc: 0.6458
Epoch 46/300
959/959 [==============================] - 3s - loss: 0.9580 - acc: 0.5996 - val_loss: 0.9484 - val_acc: 0.6458
Epoch 47/300
959/959 [==============================] - 4s - loss: 0.9731 - acc: 0.6017 - val_loss: 0.9480 - val_acc: 0.6458
Epoch 48/300
959/959 [==============================] - 3s - loss: 0.9518 - acc: 0.6215 - val_loss: 0.9432 - val_acc: 0.6458
Epoch 49/300
959/959 [==============================] - 3s - loss: 0.9415 - acc: 0.6184 - val_loss: 0.9412 - val_acc: 0.6458
Epoch 50/300
959/959 [==============================] - 3s - loss: 0.9406 - acc: 0.6111 - val_loss: 0.9383 - val_acc: 0.6500
Epoch 51/300
959/959 [==============================] - 3s - loss: 0.9674 - acc: 0.6215 - val_loss: 0.9396 - val_acc: 0.6417
Epoch 52/300
959/959 [==============================] - 3s - loss: 0.9563 - acc: 0.6267 - val_loss: 0.9380 - val_acc: 0.6500
Epoch 53/300
959/959 [==============================] - 3s - loss: 0.9357 - acc: 0.6371 - val_loss: 0.9315 - val_acc: 0.6583
Epoch 54/300
959/959 [==============================] - 3s - loss: 0.9414 - acc: 0.6194 - val_loss: 0.9281 - val_acc: 0.6542
Epoch 55/300
959/959 [==============================] - 4s - loss: 0.9326 - acc: 0.6277 - val_loss: 0.9263 - val_acc: 0.6542
Epoch 56/300
959/959 [==============================] - 3s - loss: 0.9462 - acc: 0.6204 - val_loss: 0.9272 - val_acc: 0.6625
Epoch 57/300
959/959 [==============================] - 4s - loss: 0.9254 - acc: 0.6267 - val_loss: 0.9243 - val_acc: 0.6625
Epoch 58/300
959/959 [==============================] - 4s - loss: 0.9252 - acc: 0.6152 - val_loss: 0.9237 - val_acc: 0.6667
Epoch 59/300
959/959 [==============================] - 3s - loss: 0.9470 - acc: 0.6246 - val_loss: 0.9223 - val_acc: 0.6625
Epoch 60/300
959/959 [==============================] - 3s - loss: 0.9082 - acc: 0.6382 - val_loss: 0.9202 - val_acc: 0.6708
Epoch 61/300
959/959 [==============================] - 3s - loss: 0.9184 - acc: 0.6184 - val_loss: 0.9167 - val_acc: 0.6583
Epoch 62/300
959/959 [==============================] - 3s - loss: 0.9425 - acc: 0.6100 - val_loss: 0.9139 - val_acc: 0.6667
Epoch 63/300
959/959 [==============================] - 3s - loss: 0.8864 - acc: 0.6413 - val_loss: 0.9126 - val_acc: 0.6625
Epoch 64/300
959/959 [==============================] - 3s - loss: 0.9253 - acc: 0.6361 - val_loss: 0.9117 - val_acc: 0.6583
Epoch 65/300
959/959 [==============================] - 3s - loss: 0.9245 - acc: 0.6319 - val_loss: 0.9103 - val_acc: 0.6500
Epoch 66/300
959/959 [==============================] - 3s - loss: 0.9334 - acc: 0.6225 - val_loss: 0.9105 - val_acc: 0.6708
Epoch 67/300
959/959 [==============================] - 3s - loss: 0.9208 - acc: 0.6361 - val_loss: 0.9149 - val_acc: 0.6542
Epoch 68/300
959/959 [==============================] - 4s - loss: 0.9140 - acc: 0.6330 - val_loss: 0.9137 - val_acc: 0.6583
Epoch 69/300
959/959 [==============================] - 3s - loss: 0.8947 - acc: 0.6455 - val_loss: 0.9268 - val_acc: 0.6292
Epoch 70/300
959/959 [==============================] - 3s - loss: 0.9100 - acc: 0.6382 - val_loss: 0.9095 - val_acc: 0.6583
Epoch 71/300
959/959 [==============================] - 3s - loss: 0.9019 - acc: 0.6309 - val_loss: 0.9083 - val_acc: 0.6625
Epoch 72/300
959/959 [==============================] - 3s - loss: 0.9031 - acc: 0.6392 - val_loss: 0.9014 - val_acc: 0.6625
Epoch 73/300
959/959 [==============================] - 3s - loss: 0.9021 - acc: 0.6434 - val_loss: 0.9096 - val_acc: 0.6500
Epoch 74/300
959/959 [==============================] - 3s - loss: 0.9054 - acc: 0.6496 - val_loss: 0.9047 - val_acc: 0.6500
Epoch 75/300
959/959 [==============================] - 3s - loss: 0.8978 - acc: 0.6392 - val_loss: 0.9097 - val_acc: 0.6375
Epoch 76/300
959/959 [==============================] - 3s - loss: 0.9088 - acc: 0.6142 - val_loss: 0.9058 - val_acc: 0.6542
Epoch 77/300
959/959 [==============================] - 4s - loss: 0.9123 - acc: 0.6330 - val_loss: 0.9040 - val_acc: 0.6542
Epoch 78/300
959/959 [==============================] - 4s - loss: 0.9045 - acc: 0.6434 - val_loss: 0.9022 - val_acc: 0.6500
Epoch 79/300
959/959 [==============================] - 3s - loss: 0.8795 - acc: 0.6475 - val_loss: 0.9034 - val_acc: 0.6542
Epoch 80/300
959/959 [==============================] - 3s - loss: 0.9158 - acc: 0.6173 - val_loss: 0.9089 - val_acc: 0.6458
Epoch 81/300
959/959 [==============================] - 4s - loss: 0.8807 - acc: 0.6569 - val_loss: 0.9074 - val_acc: 0.6417
Epoch 82/300
959/959 [==============================] - 3s - loss: 0.8882 - acc: 0.6257 - val_loss: 0.9049 - val_acc: 0.6542
Epoch 83/300
959/959 [==============================] - 3s - loss: 0.8630 - acc: 0.6465 - val_loss: 0.8999 - val_acc: 0.6458
Epoch 84/300
959/959 [==============================] - 3s - loss: 0.8774 - acc: 0.6413 - val_loss: 0.8995 - val_acc: 0.6458
Epoch 85/300
959/959 [==============================] - 4s - loss: 0.8876 - acc: 0.6465 - val_loss: 0.8986 - val_acc: 0.6458
Epoch 86/300
959/959 [==============================] - 4s - loss: 0.8797 - acc: 0.6621 - val_loss: 0.8969 - val_acc: 0.6500
Epoch 87/300
959/959 [==============================] - 3s - loss: 0.8941 - acc: 0.6548 - val_loss: 0.8951 - val_acc: 0.6542
Epoch 88/300
959/959 [==============================] - 3s - loss: 0.8449 - acc: 0.6642 - val_loss: 0.9001 - val_acc: 0.6458
Epoch 89/300
959/959 [==============================] - 3s - loss: 0.8525 - acc: 0.6611 - val_loss: 0.8968 - val_acc: 0.6458
Epoch 90/300
959/959 [==============================] - 3s - loss: 0.8711 - acc: 0.6590 - val_loss: 0.8976 - val_acc: 0.6417
Epoch 91/300
959/959 [==============================] - 3s - loss: 0.8703 - acc: 0.6611 - val_loss: 0.8961 - val_acc: 0.6458
Epoch 92/300
959/959 [==============================] - 3s - loss: 0.8630 - acc: 0.6590 - val_loss: 0.8958 - val_acc: 0.6417
Epoch 93/300
959/959 [==============================] - 3s - loss: 0.8965 - acc: 0.6350 - val_loss: 0.8933 - val_acc: 0.6417
Epoch 94/300
959/959 [==============================] - 3s - loss: 0.8448 - acc: 0.6726 - val_loss: 0.9040 - val_acc: 0.6458
Epoch 95/300
959/959 [==============================] - 3s - loss: 0.8740 - acc: 0.6559 - val_loss: 0.8963 - val_acc: 0.6583
Epoch 96/300
959/959 [==============================] - 3s - loss: 0.8489 - acc: 0.6569 - val_loss: 0.8944 - val_acc: 0.6417
Epoch 97/300
959/959 [==============================] - 3s - loss: 0.8642 - acc: 0.6528 - val_loss: 0.8935 - val_acc: 0.6458
Epoch 98/300
959/959 [==============================] - 3s - loss: 0.8612 - acc: 0.6580 - val_loss: 0.8952 - val_acc: 0.6500
Epoch 99/300
959/959 [==============================] - 3s - loss: 0.8233 - acc: 0.6715 - val_loss: 0.9033 - val_acc: 0.6292
Epoch 100/300
959/959 [==============================] - 4s - loss: 0.8569 - acc: 0.6621 - val_loss: 0.8910 - val_acc: 0.6583
Epoch 101/300
959/959 [==============================] - 4s - loss: 0.8398 - acc: 0.6642 - val_loss: 0.9033 - val_acc: 0.6375
Epoch 102/300
959/959 [==============================] - 3s - loss: 0.8454 - acc: 0.6340 - val_loss: 0.8910 - val_acc: 0.6417
Epoch 103/300
959/959 [==============================] - 3s - loss: 0.8309 - acc: 0.6694 - val_loss: 0.9039 - val_acc: 0.6375
Epoch 104/300
959/959 [==============================] - 3s - loss: 0.8704 - acc: 0.6569 - val_loss: 0.8921 - val_acc: 0.6417
Epoch 105/300
959/959 [==============================] - 3s - loss: 0.8242 - acc: 0.6507 - val_loss: 0.8896 - val_acc: 0.6417
Epoch 106/300
959/959 [==============================] - 3s - loss: 0.8413 - acc: 0.6590 - val_loss: 0.8909 - val_acc: 0.6500
Epoch 107/300
959/959 [==============================] - 3s - loss: 0.8713 - acc: 0.6455 - val_loss: 0.9010 - val_acc: 0.6625
Epoch 108/300
959/959 [==============================] - 3s - loss: 0.8475 - acc: 0.6694 - val_loss: 0.8858 - val_acc: 0.6458
Epoch 109/300
959/959 [==============================] - 3s - loss: 0.8484 - acc: 0.6663 - val_loss: 0.8957 - val_acc: 0.6417
Epoch 110/300
959/959 [==============================] - 3s - loss: 0.8367 - acc: 0.6809 - val_loss: 0.8858 - val_acc: 0.6500
Epoch 111/300
959/959 [==============================] - 4s - loss: 0.8393 - acc: 0.6486 - val_loss: 0.8845 - val_acc: 0.6458
Epoch 112/300
959/959 [==============================] - 4s - loss: 0.8469 - acc: 0.6580 - val_loss: 0.8807 - val_acc: 0.6500
Epoch 113/300
959/959 [==============================] - 4s - loss: 0.8401 - acc: 0.6653 - val_loss: 0.8809 - val_acc: 0.6458
Epoch 114/300
959/959 [==============================] - 4s - loss: 0.8222 - acc: 0.6736 - val_loss: 0.8892 - val_acc: 0.6500
Epoch 115/300
959/959 [==============================] - 4s - loss: 0.8492 - acc: 0.6632 - val_loss: 0.8832 - val_acc: 0.6542
Epoch 116/300
959/959 [==============================] - 3s - loss: 0.8511 - acc: 0.6653 - val_loss: 0.9042 - val_acc: 0.6417
Epoch 117/300
959/959 [==============================] - 3s - loss: 0.8551 - acc: 0.6465 - val_loss: 0.8878 - val_acc: 0.6500
Epoch 118/300
959/959 [==============================] - 4s - loss: 0.8397 - acc: 0.6444 - val_loss: 0.8907 - val_acc: 0.6542
Epoch 119/300
959/959 [==============================] - 4s - loss: 0.8306 - acc: 0.6569 - val_loss: 0.8829 - val_acc: 0.6500
Epoch 120/300
959/959 [==============================] - 3s - loss: 0.8557 - acc: 0.6392 - val_loss: 0.8912 - val_acc: 0.6458
Epoch 121/300
959/959 [==============================] - 3s - loss: 0.8227 - acc: 0.6538 - val_loss: 0.8823 - val_acc: 0.6417
Epoch 122/300
959/959 [==============================] - 3s - loss: 0.8092 - acc: 0.6778 - val_loss: 0.8859 - val_acc: 0.6500
Epoch 123/300
959/959 [==============================] - 3s - loss: 0.8153 - acc: 0.6788 - val_loss: 0.8860 - val_acc: 0.6583
Epoch 124/300
959/959 [==============================] - 3s - loss: 0.8101 - acc: 0.6799 - val_loss: 0.8882 - val_acc: 0.6542
Epoch 125/300
959/959 [==============================] - 3s - loss: 0.8000 - acc: 0.6830 - val_loss: 0.8849 - val_acc: 0.6625
Epoch 126/300
959/959 [==============================] - 3s - loss: 0.7980 - acc: 0.6799 - val_loss: 0.8835 - val_acc: 0.6500
Epoch 127/300
959/959 [==============================] - 3s - loss: 0.8027 - acc: 0.6799 - val_loss: 0.8821 - val_acc: 0.6458
Epoch 128/300
959/959 [==============================] - 3s - loss: 0.8162 - acc: 0.6767 - val_loss: 0.8923 - val_acc: 0.6458
Epoch 129/300
959/959 [==============================] - 3s - loss: 0.8184 - acc: 0.6830 - val_loss: 0.8811 - val_acc: 0.6542
Epoch 130/300
959/959 [==============================] - 3s - loss: 0.8308 - acc: 0.6663 - val_loss: 0.8810 - val_acc: 0.6500
Epoch 131/300
959/959 [==============================] - 3s - loss: 0.8234 - acc: 0.6642 - val_loss: 0.8846 - val_acc: 0.6500
Epoch 132/300
959/959 [==============================] - 3s - loss: 0.8282 - acc: 0.6830 - val_loss: 0.8949 - val_acc: 0.6250
Epoch 133/300
959/959 [==============================] - 3s - loss: 0.8093 - acc: 0.6726 - val_loss: 0.8868 - val_acc: 0.6542
Epoch 134/300
959/959 [==============================] - 3s - loss: 0.8172 - acc: 0.6590 - val_loss: 0.8846 - val_acc: 0.6583
Epoch 135/300
959/959 [==============================] - 3s - loss: 0.7683 - acc: 0.6851 - val_loss: 0.8960 - val_acc: 0.6375
Epoch 136/300
959/959 [==============================] - 3s - loss: 0.7911 - acc: 0.6913 - val_loss: 0.8797 - val_acc: 0.6542
Epoch 137/300
959/959 [==============================] - 3s - loss: 0.8138 - acc: 0.6632 - val_loss: 0.8830 - val_acc: 0.6542
Epoch 138/300
959/959 [==============================] - 4s - loss: 0.8336 - acc: 0.6757 - val_loss: 0.8816 - val_acc: 0.6667
Epoch 139/300
959/959 [==============================] - 3s - loss: 0.8028 - acc: 0.6632 - val_loss: 0.8801 - val_acc: 0.6667
Epoch 140/300
959/959 [==============================] - 3s - loss: 0.8071 - acc: 0.6736 - val_loss: 0.8812 - val_acc: 0.6708
Epoch 141/300
959/959 [==============================] - 3s - loss: 0.7977 - acc: 0.6872 - val_loss: 0.8808 - val_acc: 0.6583
Epoch 142/300
959/959 [==============================] - 3s - loss: 0.7761 - acc: 0.6945 - val_loss: 0.8817 - val_acc: 0.6542
Epoch 143/300
959/959 [==============================] - 3s - loss: 0.8074 - acc: 0.6861 - val_loss: 0.8790 - val_acc: 0.6667
Epoch 144/300
959/959 [==============================] - 3s - loss: 0.8000 - acc: 0.6684 - val_loss: 0.8776 - val_acc: 0.6542
Epoch 145/300
959/959 [==============================] - 3s - loss: 0.7966 - acc: 0.6840 - val_loss: 0.8840 - val_acc: 0.6542
Epoch 146/300
959/959 [==============================] - 3s - loss: 0.8154 - acc: 0.6830 - val_loss: 0.8791 - val_acc: 0.6542
Epoch 147/300
959/959 [==============================] - 4s - loss: 0.8246 - acc: 0.7007 - val_loss: 0.8858 - val_acc: 0.6417
Epoch 148/300
959/959 [==============================] - 4s - loss: 0.7854 - acc: 0.6674 - val_loss: 0.8821 - val_acc: 0.6708
Epoch 149/300
959/959 [==============================] - 3s - loss: 0.7807 - acc: 0.6966 - val_loss: 0.8781 - val_acc: 0.6583
Epoch 150/300
959/959 [==============================] - 3s - loss: 0.7915 - acc: 0.6715 - val_loss: 0.8799 - val_acc: 0.6583
Epoch 151/300
959/959 [==============================] - 3s - loss: 0.8144 - acc: 0.6861 - val_loss: 0.8789 - val_acc: 0.6625
Epoch 152/300
959/959 [==============================] - 3s - loss: 0.7893 - acc: 0.6986 - val_loss: 0.8886 - val_acc: 0.6542
Epoch 153/300
959/959 [==============================] - 3s - loss: 0.7786 - acc: 0.6653 - val_loss: 0.8852 - val_acc: 0.6500
Epoch 154/300
959/959 [==============================] - 3s - loss: 0.7965 - acc: 0.6872 - val_loss: 0.8810 - val_acc: 0.6667
Epoch 155/300
959/959 [==============================] - 3s - loss: 0.7970 - acc: 0.6851 - val_loss: 0.8787 - val_acc: 0.6625
Epoch 156/300
959/959 [==============================] - 3s - loss: 0.7740 - acc: 0.7049 - val_loss: 0.8911 - val_acc: 0.6542
Epoch 157/300
959/959 [==============================] - 3s - loss: 0.7994 - acc: 0.6861 - val_loss: 0.8829 - val_acc: 0.6500
Epoch 158/300
959/959 [==============================] - 3s - loss: 0.7628 - acc: 0.7049 - val_loss: 0.8868 - val_acc: 0.6542
Epoch 159/300
959/959 [==============================] - 3s - loss: 0.7836 - acc: 0.6903 - val_loss: 0.8846 - val_acc: 0.6667
Epoch 160/300
959/959 [==============================] - 4s - loss: 0.8002 - acc: 0.6903 - val_loss: 0.8857 - val_acc: 0.6583
Epoch 161/300
959/959 [==============================] - 4s - loss: 0.8029 - acc: 0.6924 - val_loss: 0.8843 - val_acc: 0.6500
Epoch 162/300
959/959 [==============================] - 4s - loss: 0.7858 - acc: 0.6924 - val_loss: 0.8787 - val_acc: 0.6500
Epoch 163/300
959/959 [==============================] - 4s - loss: 0.7840 - acc: 0.6840 - val_loss: 0.8821 - val_acc: 0.6667
Epoch 164/300
959/959 [==============================] - 4s - loss: 0.7892 - acc: 0.6851 - val_loss: 0.8845 - val_acc: 0.6542
Epoch 165/300
959/959 [==============================] - 4s - loss: 0.7926 - acc: 0.6736 - val_loss: 0.8874 - val_acc: 0.6500
Epoch 166/300
959/959 [==============================] - 4s - loss: 0.7602 - acc: 0.7091 - val_loss: 0.8812 - val_acc: 0.6458
Epoch 167/300
959/959 [==============================] - 4s - loss: 0.7980 - acc: 0.6893 - val_loss: 0.8771 - val_acc: 0.6667
Epoch 168/300
959/959 [==============================] - 4s - loss: 0.7988 - acc: 0.6820 - val_loss: 0.8780 - val_acc: 0.6625
Epoch 169/300
959/959 [==============================] - 4s - loss: 0.7926 - acc: 0.6882 - val_loss: 0.8746 - val_acc: 0.6458
Epoch 170/300
959/959 [==============================] - 4s - loss: 0.7823 - acc: 0.6684 - val_loss: 0.8764 - val_acc: 0.6583
Epoch 171/300
959/959 [==============================] - 4s - loss: 0.8044 - acc: 0.6861 - val_loss: 0.8794 - val_acc: 0.6542
Epoch 172/300
959/959 [==============================] - 4s - loss: 0.8350 - acc: 0.6694 - val_loss: 0.8813 - val_acc: 0.6417
Epoch 173/300
959/959 [==============================] - 4s - loss: 0.7925 - acc: 0.6924 - val_loss: 0.8731 - val_acc: 0.6542
Epoch 174/300
959/959 [==============================] - 3s - loss: 0.7658 - acc: 0.6934 - val_loss: 0.8724 - val_acc: 0.6625
Epoch 175/300
959/959 [==============================] - 3s - loss: 0.8008 - acc: 0.6976 - val_loss: 0.8752 - val_acc: 0.6625
Epoch 176/300
959/959 [==============================] - 3s - loss: 0.7826 - acc: 0.6861 - val_loss: 0.8814 - val_acc: 0.6500
Epoch 177/300
959/959 [==============================] - 3s - loss: 0.7749 - acc: 0.6924 - val_loss: 0.8755 - val_acc: 0.6583
Epoch 178/300
959/959 [==============================] - 3s - loss: 0.7737 - acc: 0.6966 - val_loss: 0.8750 - val_acc: 0.6542
Epoch 179/300
959/959 [==============================] - 3s - loss: 0.7543 - acc: 0.7059 - val_loss: 0.8756 - val_acc: 0.6500
Epoch 180/300
959/959 [==============================] - 4s - loss: 0.7534 - acc: 0.6976 - val_loss: 0.8805 - val_acc: 0.6542
Epoch 181/300
959/959 [==============================] - 3s - loss: 0.7322 - acc: 0.7185 - val_loss: 0.8756 - val_acc: 0.6625
Epoch 182/300
959/959 [==============================] - 3s - loss: 0.7753 - acc: 0.6934 - val_loss: 0.8760 - val_acc: 0.6542
Epoch 183/300
959/959 [==============================] - 3s - loss: 0.7490 - acc: 0.7039 - val_loss: 0.8807 - val_acc: 0.6708
Epoch 184/300
959/959 [==============================] - 3s - loss: 0.7426 - acc: 0.6986 - val_loss: 0.8807 - val_acc: 0.6583
Epoch 185/300
959/959 [==============================] - 3s - loss: 0.7729 - acc: 0.6976 - val_loss: 0.8824 - val_acc: 0.6583
Epoch 186/300
959/959 [==============================] - 4s - loss: 0.7704 - acc: 0.7028 - val_loss: 0.8849 - val_acc: 0.6708
Epoch 187/300
959/959 [==============================] - 4s - loss: 0.7981 - acc: 0.6778 - val_loss: 0.8795 - val_acc: 0.6542
Epoch 188/300
959/959 [==============================] - 3s - loss: 0.7625 - acc: 0.6945 - val_loss: 0.8910 - val_acc: 0.6500
Epoch 189/300
959/959 [==============================] - 4s - loss: 0.7587 - acc: 0.6976 - val_loss: 0.8822 - val_acc: 0.6583
Epoch 190/300
959/959 [==============================] - 3s - loss: 0.7571 - acc: 0.6820 - val_loss: 0.8763 - val_acc: 0.6542
Epoch 191/300
959/959 [==============================] - 3s - loss: 0.7907 - acc: 0.6851 - val_loss: 0.8788 - val_acc: 0.6458
Epoch 192/300
959/959 [==============================] - 3s - loss: 0.7332 - acc: 0.7101 - val_loss: 0.8942 - val_acc: 0.6333
Epoch 193/300
959/959 [==============================] - 3s - loss: 0.7482 - acc: 0.7007 - val_loss: 0.8792 - val_acc: 0.6667
Epoch 194/300
959/959 [==============================] - 3s - loss: 0.7632 - acc: 0.6903 - val_loss: 0.8748 - val_acc: 0.6458
Epoch 195/300
959/959 [==============================] - 3s - loss: 0.7563 - acc: 0.6799 - val_loss: 0.8853 - val_acc: 0.6625
Epoch 196/300
959/959 [==============================] - 3s - loss: 0.7702 - acc: 0.6945 - val_loss: 0.8743 - val_acc: 0.6583
Epoch 197/300
959/959 [==============================] - 3s - loss: 0.7409 - acc: 0.7059 - val_loss: 0.8774 - val_acc: 0.6583
Epoch 198/300
959/959 [==============================] - 3s - loss: 0.7370 - acc: 0.7153 - val_loss: 0.8759 - val_acc: 0.6750
Epoch 199/300
959/959 [==============================] - 3s - loss: 0.7530 - acc: 0.6986 - val_loss: 0.9019 - val_acc: 0.6333
Epoch 200/300
959/959 [==============================] - 3s - loss: 0.7682 - acc: 0.7018 - val_loss: 0.8835 - val_acc: 0.6417
Epoch 201/300
959/959 [==============================] - 3s - loss: 0.7536 - acc: 0.7185 - val_loss: 0.8753 - val_acc: 0.6625
Epoch 202/300
959/959 [==============================] - 3s - loss: 0.7965 - acc: 0.6486 - val_loss: 0.8790 - val_acc: 0.6458
Epoch 203/300
959/959 [==============================] - 3s - loss: 0.7327 - acc: 0.7258 - val_loss: 0.8748 - val_acc: 0.6542
Epoch 204/300
959/959 [==============================] - 3s - loss: 0.7288 - acc: 0.6986 - val_loss: 0.8717 - val_acc: 0.6583
Epoch 205/300
959/959 [==============================] - 3s - loss: 0.7601 - acc: 0.7143 - val_loss: 0.9068 - val_acc: 0.6167
Epoch 206/300
959/959 [==============================] - 3s - loss: 0.7624 - acc: 0.7007 - val_loss: 0.8733 - val_acc: 0.6458
Epoch 207/300
959/959 [==============================] - 3s - loss: 0.7595 - acc: 0.7039 - val_loss: 0.8744 - val_acc: 0.6417
Epoch 208/300
959/959 [==============================] - 3s - loss: 0.7450 - acc: 0.6997 - val_loss: 0.8786 - val_acc: 0.6417
Epoch 209/300
959/959 [==============================] - 3s - loss: 0.7819 - acc: 0.6945 - val_loss: 0.8865 - val_acc: 0.6375
Epoch 210/300
959/959 [==============================] - 3s - loss: 0.7381 - acc: 0.7039 - val_loss: 0.8776 - val_acc: 0.6417
Epoch 211/300
959/959 [==============================] - 3s - loss: 0.7608 - acc: 0.7122 - val_loss: 0.8771 - val_acc: 0.6333
Epoch 212/300
959/959 [==============================] - 3s - loss: 0.7450 - acc: 0.7091 - val_loss: 0.8827 - val_acc: 0.6458
Epoch 213/300
959/959 [==============================] - 3s - loss: 0.7447 - acc: 0.6997 - val_loss: 0.8797 - val_acc: 0.6458
Epoch 214/300
959/959 [==============================] - 3s - loss: 0.7448 - acc: 0.7007 - val_loss: 0.8844 - val_acc: 0.6667
Epoch 215/300
959/959 [==============================] - 3s - loss: 0.7685 - acc: 0.6924 - val_loss: 0.8828 - val_acc: 0.6292
Epoch 216/300
959/959 [==============================] - 3s - loss: 0.7192 - acc: 0.7143 - val_loss: 0.9027 - val_acc: 0.6333
Epoch 217/300
959/959 [==============================] - 3s - loss: 0.7499 - acc: 0.7049 - val_loss: 0.8855 - val_acc: 0.6458
Epoch 218/300
959/959 [==============================] - 3s - loss: 0.7462 - acc: 0.6986 - val_loss: 0.8801 - val_acc: 0.6542
Epoch 219/300
959/959 [==============================] - 3s - loss: 0.7425 - acc: 0.7059 - val_loss: 0.8776 - val_acc: 0.6458
Epoch 220/300
959/959 [==============================] - 3s - loss: 0.7264 - acc: 0.7101 - val_loss: 0.8783 - val_acc: 0.6667
Epoch 221/300
959/959 [==============================] - 3s - loss: 0.7614 - acc: 0.6934 - val_loss: 0.8760 - val_acc: 0.6542
Epoch 222/300
959/959 [==============================] - 3s - loss: 0.7362 - acc: 0.6966 - val_loss: 0.8864 - val_acc: 0.6333
Epoch 223/300
959/959 [==============================] - 3s - loss: 0.7191 - acc: 0.7018 - val_loss: 0.8744 - val_acc: 0.6625
Epoch 224/300
959/959 [==============================] - 3s - loss: 0.7149 - acc: 0.7226 - val_loss: 0.8761 - val_acc: 0.6458
Epoch 225/300
959/959 [==============================] - 3s - loss: 0.7342 - acc: 0.7185 - val_loss: 0.8873 - val_acc: 0.6417
Epoch 226/300
959/959 [==============================] - 3s - loss: 0.7292 - acc: 0.6913 - val_loss: 0.9002 - val_acc: 0.6208
Epoch 227/300
959/959 [==============================] - 3s - loss: 0.7117 - acc: 0.7185 - val_loss: 0.8773 - val_acc: 0.6500
Epoch 228/300
959/959 [==============================] - 4s - loss: 0.7421 - acc: 0.7028 - val_loss: 0.8772 - val_acc: 0.6500
Epoch 229/300
959/959 [==============================] - 4s - loss: 0.7243 - acc: 0.7080 - val_loss: 0.8808 - val_acc: 0.6500
Epoch 230/300
959/959 [==============================] - 4s - loss: 0.7373 - acc: 0.7226 - val_loss: 0.8784 - val_acc: 0.6458
Epoch 231/300
959/959 [==============================] - 4s - loss: 0.7403 - acc: 0.7091 - val_loss: 0.8807 - val_acc: 0.6500
Epoch 232/300
959/959 [==============================] - 4s - loss: 0.7610 - acc: 0.6861 - val_loss: 0.8765 - val_acc: 0.6625
Epoch 233/300
959/959 [==============================] - 4s - loss: 0.7222 - acc: 0.7362 - val_loss: 0.8793 - val_acc: 0.6583
Epoch 234/300
959/959 [==============================] - 3s - loss: 0.6846 - acc: 0.7268 - val_loss: 0.8732 - val_acc: 0.6542
Epoch 235/300
959/959 [==============================] - 3s - loss: 0.7194 - acc: 0.7164 - val_loss: 0.9051 - val_acc: 0.6167
Epoch 236/300
959/959 [==============================] - 4s - loss: 0.7498 - acc: 0.7049 - val_loss: 0.8779 - val_acc: 0.6375
Epoch 237/300
959/959 [==============================] - 4s - loss: 0.7129 - acc: 0.7299 - val_loss: 0.8945 - val_acc: 0.6292
Epoch 238/300
959/959 [==============================] - 3s - loss: 0.7561 - acc: 0.7205 - val_loss: 0.8984 - val_acc: 0.6417
Epoch 239/300
959/959 [==============================] - 3s - loss: 0.7388 - acc: 0.7132 - val_loss: 0.8785 - val_acc: 0.6417
Epoch 240/300
959/959 [==============================] - 3s - loss: 0.7130 - acc: 0.7153 - val_loss: 0.8776 - val_acc: 0.6542
Epoch 241/300
959/959 [==============================] - 3s - loss: 0.7110 - acc: 0.7299 - val_loss: 0.8912 - val_acc: 0.6500
Epoch 242/300
959/959 [==============================] - 4s - loss: 0.7179 - acc: 0.7174 - val_loss: 0.8780 - val_acc: 0.6500
Epoch 243/300
959/959 [==============================] - 4s - loss: 0.7231 - acc: 0.7143 - val_loss: 0.8775 - val_acc: 0.6542
Epoch 244/300
959/959 [==============================] - 4s - loss: 0.7311 - acc: 0.7112 - val_loss: 0.8790 - val_acc: 0.6542
Epoch 245/300
959/959 [==============================] - 4s - loss: 0.7323 - acc: 0.7059 - val_loss: 0.8863 - val_acc: 0.6542
Epoch 246/300
959/959 [==============================] - 4s - loss: 0.7164 - acc: 0.7216 - val_loss: 0.8790 - val_acc: 0.6583
Epoch 247/300
959/959 [==============================] - 4s - loss: 0.7221 - acc: 0.7237 - val_loss: 0.8773 - val_acc: 0.6500
Epoch 248/300
959/959 [==============================] - 4s - loss: 0.7126 - acc: 0.7237 - val_loss: 0.8770 - val_acc: 0.6417
Epoch 249/300
959/959 [==============================] - 4s - loss: 0.7090 - acc: 0.7247 - val_loss: 0.8737 - val_acc: 0.6500
Epoch 250/300
959/959 [==============================] - 3s - loss: 0.7200 - acc: 0.7080 - val_loss: 0.8785 - val_acc: 0.6458
Epoch 251/300
959/959 [==============================] - 3s - loss: 0.7175 - acc: 0.7018 - val_loss: 0.8860 - val_acc: 0.6375
Epoch 252/300
959/959 [==============================] - 3s - loss: 0.7284 - acc: 0.7101 - val_loss: 0.8854 - val_acc: 0.6375
Epoch 253/300
959/959 [==============================] - 3s - loss: 0.7479 - acc: 0.7164 - val_loss: 0.9198 - val_acc: 0.6167
Epoch 254/300
959/959 [==============================] - 3s - loss: 0.7078 - acc: 0.7143 - val_loss: 0.8785 - val_acc: 0.6542
Epoch 255/300
959/959 [==============================] - 3s - loss: 0.7161 - acc: 0.7164 - val_loss: 0.8781 - val_acc: 0.6500
Epoch 256/300
959/959 [==============================] - 3s - loss: 0.7226 - acc: 0.7091 - val_loss: 0.8771 - val_acc: 0.6292
Epoch 257/300
959/959 [==============================] - 3s - loss: 0.6977 - acc: 0.7362 - val_loss: 0.8800 - val_acc: 0.6417
Epoch 258/300
959/959 [==============================] - 3s - loss: 0.7022 - acc: 0.7247 - val_loss: 0.8823 - val_acc: 0.6333
Epoch 259/300
959/959 [==============================] - 3s - loss: 0.7024 - acc: 0.7331 - val_loss: 0.8894 - val_acc: 0.6458
Epoch 260/300
959/959 [==============================] - 3s - loss: 0.7507 - acc: 0.7028 - val_loss: 0.8848 - val_acc: 0.6333
Epoch 261/300
959/959 [==============================] - 4s - loss: 0.6823 - acc: 0.7393 - val_loss: 0.8797 - val_acc: 0.6292
Epoch 262/300
959/959 [==============================] - 4s - loss: 0.7166 - acc: 0.7205 - val_loss: 0.8807 - val_acc: 0.6417
Epoch 263/300
959/959 [==============================] - 4s - loss: 0.7101 - acc: 0.7091 - val_loss: 0.8820 - val_acc: 0.6375
Epoch 264/300
959/959 [==============================] - 4s - loss: 0.6733 - acc: 0.7268 - val_loss: 0.8889 - val_acc: 0.6417
Epoch 265/300
959/959 [==============================] - 4s - loss: 0.7314 - acc: 0.7185 - val_loss: 0.8890 - val_acc: 0.6375
Epoch 266/300
959/959 [==============================] - 4s - loss: 0.7019 - acc: 0.7205 - val_loss: 0.8786 - val_acc: 0.6375
Epoch 267/300
959/959 [==============================] - 4s - loss: 0.7204 - acc: 0.7101 - val_loss: 0.8974 - val_acc: 0.6417
Epoch 268/300
959/959 [==============================] - 3s - loss: 0.7293 - acc: 0.7007 - val_loss: 0.8775 - val_acc: 0.6333
Epoch 269/300
959/959 [==============================] - 3s - loss: 0.6652 - acc: 0.7383 - val_loss: 0.8784 - val_acc: 0.6583
Epoch 270/300
959/959 [==============================] - 3s - loss: 0.7322 - acc: 0.7091 - val_loss: 0.8783 - val_acc: 0.6333
Epoch 271/300
959/959 [==============================] - 3s - loss: 0.7315 - acc: 0.7237 - val_loss: 0.8732 - val_acc: 0.6375
Epoch 272/300
959/959 [==============================] - 3s - loss: 0.7456 - acc: 0.6986 - val_loss: 0.8739 - val_acc: 0.6375
Epoch 273/300
959/959 [==============================] - 3s - loss: 0.6927 - acc: 0.7205 - val_loss: 0.8743 - val_acc: 0.6542
Epoch 274/300
959/959 [==============================] - 3s - loss: 0.6941 - acc: 0.7247 - val_loss: 0.8751 - val_acc: 0.6542
Epoch 275/300
959/959 [==============================] - 3s - loss: 0.6952 - acc: 0.7174 - val_loss: 0.8771 - val_acc: 0.6417
Epoch 276/300
959/959 [==============================] - 3s - loss: 0.6700 - acc: 0.7310 - val_loss: 0.8756 - val_acc: 0.6333
Epoch 277/300
959/959 [==============================] - 3s - loss: 0.7261 - acc: 0.7070 - val_loss: 0.8830 - val_acc: 0.6250
Epoch 278/300
959/959 [==============================] - 3s - loss: 0.6803 - acc: 0.7216 - val_loss: 0.8783 - val_acc: 0.6417
Epoch 279/300
959/959 [==============================] - 3s - loss: 0.6945 - acc: 0.7341 - val_loss: 0.8922 - val_acc: 0.6375
Epoch 280/300
959/959 [==============================] - 3s - loss: 0.7015 - acc: 0.7237 - val_loss: 0.8812 - val_acc: 0.6333
Epoch 281/300
959/959 [==============================] - 3s - loss: 0.7204 - acc: 0.7195 - val_loss: 0.8881 - val_acc: 0.6417
Epoch 282/300
959/959 [==============================] - 3s - loss: 0.7001 - acc: 0.7247 - val_loss: 0.8796 - val_acc: 0.6292
Epoch 283/300
959/959 [==============================] - 3s - loss: 0.7356 - acc: 0.7091 - val_loss: 0.8790 - val_acc: 0.6292
Epoch 284/300
959/959 [==============================] - 3s - loss: 0.6848 - acc: 0.7237 - val_loss: 0.8807 - val_acc: 0.6292
Epoch 285/300
959/959 [==============================] - 3s - loss: 0.7040 - acc: 0.7174 - val_loss: 0.8837 - val_acc: 0.6292
Epoch 286/300
959/959 [==============================] - 3s - loss: 0.7133 - acc: 0.7404 - val_loss: 0.8992 - val_acc: 0.6167
Epoch 287/300
959/959 [==============================] - 3s - loss: 0.7023 - acc: 0.7164 - val_loss: 0.8831 - val_acc: 0.6417
Epoch 288/300
959/959 [==============================] - 3s - loss: 0.7037 - acc: 0.7164 - val_loss: 0.8838 - val_acc: 0.6458
Epoch 289/300
959/959 [==============================] - 3s - loss: 0.6739 - acc: 0.7445 - val_loss: 0.8819 - val_acc: 0.6375
Epoch 290/300
959/959 [==============================] - 3s - loss: 0.7188 - acc: 0.7143 - val_loss: 0.8857 - val_acc: 0.6333
Epoch 291/300
959/959 [==============================] - 3s - loss: 0.7057 - acc: 0.7070 - val_loss: 0.8875 - val_acc: 0.6292
Epoch 292/300
959/959 [==============================] - 3s - loss: 0.7170 - acc: 0.7310 - val_loss: 0.8829 - val_acc: 0.6458
Epoch 293/300
959/959 [==============================] - 4s - loss: 0.6774 - acc: 0.7331 - val_loss: 0.8904 - val_acc: 0.6417
Epoch 294/300
959/959 [==============================] - 4s - loss: 0.6678 - acc: 0.7477 - val_loss: 0.8851 - val_acc: 0.6333
Epoch 295/300
959/959 [==============================] - 4s - loss: 0.6853 - acc: 0.7404 - val_loss: 0.8882 - val_acc: 0.6375
Epoch 296/300
959/959 [==============================] - 3s - loss: 0.7191 - acc: 0.7185 - val_loss: 0.8951 - val_acc: 0.6375
Epoch 297/300
959/959 [==============================] - 4s - loss: 0.6779 - acc: 0.7153 - val_loss: 0.8880 - val_acc: 0.6167
Epoch 298/300
959/959 [==============================] - 3s - loss: 0.7099 - acc: 0.7101 - val_loss: 0.8835 - val_acc: 0.6417
Epoch 299/300
959/959 [==============================] - 3s - loss: 0.7106 - acc: 0.7320 - val_loss: 0.8929 - val_acc: 0.6417
Epoch 300/300
959/959 [==============================] - 3s - loss: 0.6991 - acc: 0.7237 - val_loss: 0.8824 - val_acc: 0.6417
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[30]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x151f71290&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [17]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># </span>
<span class="k">class</span> <span class="nc">LossHistory</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="p">{}):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="p">{}):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'loss'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [49]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#sgd = SGD(lr=0.05, decay=0.02, momentum=0.7, nesterov=True)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [51]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1">#LSTM_layer = Bidirectional(LSTM(units=128))(embedded_sequences)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = LSTM(units=128,dropout=0.5,recurrent_dropout=0.5)(embedded_sequences)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>


<span class="n">derived_features</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">34</span><span class="p">,))</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">derived_features</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">LSTM_layer</span><span class="p">,</span><span class="n">hidden_layer</span><span class="p">])</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>
<span class="c1">#hidden_layer_3 = Dropout(0.5)(hidden_layer_3) # can I add this extra layer for more regularization?</span>
<span class="c1">#hidden_layer_3 = Dropout(0.5)(hidden_layer_3) # can I add this extra layer for more regularization?</span>

<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional additional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate) # optional additional hidden layer</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate) # optional additional hidden layer</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels_mw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span> <span class="c1"># comment out if penultimate layer is used</span>

<span class="n">lstm_text_and_manual_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">sequence_input</span><span class="p">,</span><span class="n">derived_features</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">LossHistory</span><span class="p">()</span>

<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">normed_twts</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span><span class="n">manual_features_scaled</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">]],</span> <span class="n">labels_mw</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">history</span><span class="p">])</span>

<span class="c1">#lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],</span>
<span class="c1">#          epochs=10, batch_size=128,validation_split=0.2)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1079 samples, validate on 270 samples
Epoch 1/200
1079/1079 [==============================] - 9s - loss: 1.3756 - acc: 0.4171 - val_loss: 1.1098 - val_acc: 0.5444
Epoch 2/200
1079/1079 [==============================] - 4s - loss: 1.1286 - acc: 0.5635 - val_loss: 1.0100 - val_acc: 0.6111
Epoch 3/200
1079/1079 [==============================] - 4s - loss: 0.7424 - acc: 0.7220 - val_loss: 1.2281 - val_acc: 0.5889
Epoch 4/200
1079/1079 [==============================] - 4s - loss: 0.4954 - acc: 0.8044 - val_loss: 1.3052 - val_acc: 0.5444
Epoch 5/200
1079/1079 [==============================] - 4s - loss: 0.3399 - acc: 0.8869 - val_loss: 1.6340 - val_acc: 0.5556
Epoch 6/200
1079/1079 [==============================] - 4s - loss: 0.1930 - acc: 0.9351 - val_loss: 2.0177 - val_acc: 0.5630
Epoch 7/200
1079/1079 [==============================] - 4s - loss: 0.1356 - acc: 0.9564 - val_loss: 2.3551 - val_acc: 0.5519
Epoch 8/200
1079/1079 [==============================] - 4s - loss: 0.1375 - acc: 0.9546 - val_loss: 2.2133 - val_acc: 0.5630
Epoch 9/200
1079/1079 [==============================] - 4s - loss: 0.1054 - acc: 0.9676 - val_loss: 2.1801 - val_acc: 0.5481
Epoch 10/200
1079/1079 [==============================] - 4s - loss: 0.0802 - acc: 0.9815 - val_loss: 2.4414 - val_acc: 0.5815
Epoch 11/200
1079/1079 [==============================] - 4s - loss: 0.0805 - acc: 0.9741 - val_loss: 2.6426 - val_acc: 0.5444
Epoch 12/200
1079/1079 [==============================] - 4s - loss: 0.0901 - acc: 0.9666 - val_loss: 2.4701 - val_acc: 0.5926
Epoch 13/200
1079/1079 [==============================] - 4s - loss: 0.0622 - acc: 0.9805 - val_loss: 2.7212 - val_acc: 0.5444
Epoch 14/200
1079/1079 [==============================] - 4s - loss: 0.0676 - acc: 0.9796 - val_loss: 2.9006 - val_acc: 0.5667
Epoch 15/200
1079/1079 [==============================] - 4s - loss: 0.0435 - acc: 0.9870 - val_loss: 2.8592 - val_acc: 0.5407
Epoch 16/200
1079/1079 [==============================] - 4s - loss: 0.0512 - acc: 0.9824 - val_loss: 2.7580 - val_acc: 0.5333
Epoch 17/200
1079/1079 [==============================] - 4s - loss: 0.0477 - acc: 0.9861 - val_loss: 2.7399 - val_acc: 0.5815
Epoch 18/200
1079/1079 [==============================] - 4s - loss: 0.0388 - acc: 0.9917 - val_loss: 2.7953 - val_acc: 0.5630
Epoch 19/200
1079/1079 [==============================] - 4s - loss: 0.0236 - acc: 0.9944 - val_loss: 2.9412 - val_acc: 0.5630
Epoch 20/200
1079/1079 [==============================] - 4s - loss: 0.0256 - acc: 0.9907 - val_loss: 3.2573 - val_acc: 0.5593
Epoch 21/200
1079/1079 [==============================] - 4s - loss: 0.0224 - acc: 0.9935 - val_loss: 3.3772 - val_acc: 0.5778
Epoch 22/200
1079/1079 [==============================] - 4s - loss: 0.0460 - acc: 0.9870 - val_loss: 3.4836 - val_acc: 0.5370
Epoch 23/200
1079/1079 [==============================] - 4s - loss: 0.0379 - acc: 0.9917 - val_loss: 3.3820 - val_acc: 0.5741
Epoch 24/200
1079/1079 [==============================] - 4s - loss: 0.0399 - acc: 0.9926 - val_loss: 3.1787 - val_acc: 0.5852
Epoch 25/200
1079/1079 [==============================] - 4s - loss: 0.0335 - acc: 0.9907 - val_loss: 2.9349 - val_acc: 0.5519
Epoch 26/200
1079/1079 [==============================] - 4s - loss: 0.0225 - acc: 0.9917 - val_loss: 2.9972 - val_acc: 0.5963
Epoch 27/200
1079/1079 [==============================] - 4s - loss: 0.0162 - acc: 0.9954 - val_loss: 3.2776 - val_acc: 0.5556
Epoch 28/200
1079/1079 [==============================] - 4s - loss: 0.0245 - acc: 0.9944 - val_loss: 3.3964 - val_acc: 0.5519
Epoch 29/200
1079/1079 [==============================] - 4s - loss: 0.0348 - acc: 0.9917 - val_loss: 3.5733 - val_acc: 0.5593
Epoch 30/200
1079/1079 [==============================] - 4s - loss: 0.0246 - acc: 0.9954 - val_loss: 3.1442 - val_acc: 0.5556
Epoch 31/200
1079/1079 [==============================] - 4s - loss: 0.0200 - acc: 0.9944 - val_loss: 3.1411 - val_acc: 0.5593
Epoch 32/200
1079/1079 [==============================] - 4s - loss: 0.0201 - acc: 0.9935 - val_loss: 3.4480 - val_acc: 0.5333
Epoch 33/200
1079/1079 [==============================] - 4s - loss: 0.0160 - acc: 0.9954 - val_loss: 3.4728 - val_acc: 0.5630
Epoch 34/200
1079/1079 [==============================] - 4s - loss: 0.0111 - acc: 0.9972 - val_loss: 3.5989 - val_acc: 0.5704
Epoch 35/200
1079/1079 [==============================] - 4s - loss: 0.0144 - acc: 0.9963 - val_loss: 3.7108 - val_acc: 0.5519
Epoch 36/200
1079/1079 [==============================] - 4s - loss: 0.0089 - acc: 0.9981 - val_loss: 3.5757 - val_acc: 0.5630
Epoch 37/200
1079/1079 [==============================] - 4s - loss: 0.0185 - acc: 0.9944 - val_loss: 3.5384 - val_acc: 0.5741
Epoch 38/200
1079/1079 [==============================] - 4s - loss: 0.0120 - acc: 0.9954 - val_loss: 3.6495 - val_acc: 0.5556
Epoch 39/200
1079/1079 [==============================] - 4s - loss: 0.0126 - acc: 0.9954 - val_loss: 3.4503 - val_acc: 0.5667
Epoch 40/200
1079/1079 [==============================] - 4s - loss: 0.0055 - acc: 0.9981 - val_loss: 3.4777 - val_acc: 0.5593
Epoch 41/200
1079/1079 [==============================] - 4s - loss: 0.0090 - acc: 0.9972 - val_loss: 3.6082 - val_acc: 0.5481
Epoch 42/200
1079/1079 [==============================] - 4s - loss: 0.0175 - acc: 0.9944 - val_loss: 3.5905 - val_acc: 0.5481
Epoch 43/200
1079/1079 [==============================] - 4s - loss: 0.0108 - acc: 0.9972 - val_loss: 3.5794 - val_acc: 0.5704
Epoch 44/200
1079/1079 [==============================] - 4s - loss: 0.0051 - acc: 0.9981 - val_loss: 3.6670 - val_acc: 0.5667
Epoch 45/200
1079/1079 [==============================] - 4s - loss: 0.0068 - acc: 0.9981 - val_loss: 3.7103 - val_acc: 0.5704
Epoch 46/200
1079/1079 [==============================] - 4s - loss: 0.0134 - acc: 0.9944 - val_loss: 3.7082 - val_acc: 0.5556
Epoch 47/200
1079/1079 [==============================] - 4s - loss: 0.0074 - acc: 0.9972 - val_loss: 3.7907 - val_acc: 0.5593
Epoch 48/200
1079/1079 [==============================] - 4s - loss: 0.0045 - acc: 1.0000 - val_loss: 3.8599 - val_acc: 0.5593
Epoch 49/200
1079/1079 [==============================] - 4s - loss: 0.0209 - acc: 0.9935 - val_loss: 3.6201 - val_acc: 0.5667
Epoch 50/200
1079/1079 [==============================] - 4s - loss: 0.0142 - acc: 0.9926 - val_loss: 3.7747 - val_acc: 0.5407
Epoch 51/200
1079/1079 [==============================] - 4s - loss: 0.0155 - acc: 0.9954 - val_loss: 3.3621 - val_acc: 0.5889
Epoch 52/200
1079/1079 [==============================] - 4s - loss: 0.0121 - acc: 0.9972 - val_loss: 3.4060 - val_acc: 0.5963
Epoch 53/200
1079/1079 [==============================] - 4s - loss: 0.0111 - acc: 0.9954 - val_loss: 3.5351 - val_acc: 0.5704
Epoch 54/200
1079/1079 [==============================] - 4s - loss: 0.0061 - acc: 0.9981 - val_loss: 3.7040 - val_acc: 0.5481
Epoch 55/200
1079/1079 [==============================] - 4s - loss: 0.0132 - acc: 0.9963 - val_loss: 3.5031 - val_acc: 0.5741
Epoch 56/200
1079/1079 [==============================] - 4s - loss: 0.0048 - acc: 1.0000 - val_loss: 3.4532 - val_acc: 0.5704
Epoch 57/200
1079/1079 [==============================] - 4s - loss: 0.0072 - acc: 0.9963 - val_loss: 3.5290 - val_acc: 0.5741
Epoch 58/200
1079/1079 [==============================] - 4s - loss: 0.0142 - acc: 0.9944 - val_loss: 3.8142 - val_acc: 0.5259
Epoch 59/200
1079/1079 [==============================] - 4s - loss: 0.0105 - acc: 0.9963 - val_loss: 3.5710 - val_acc: 0.5926
Epoch 60/200
1079/1079 [==============================] - 4s - loss: 0.0076 - acc: 0.9972 - val_loss: 3.7536 - val_acc: 0.5630
Epoch 61/200
1079/1079 [==============================] - 4s - loss: 0.0043 - acc: 0.9991 - val_loss: 3.9775 - val_acc: 0.5444
Epoch 62/200
1079/1079 [==============================] - 4s - loss: 0.0036 - acc: 0.9991 - val_loss: 3.9938 - val_acc: 0.5704
Epoch 63/200
1079/1079 [==============================] - 4s - loss: 0.0026 - acc: 1.0000 - val_loss: 4.0050 - val_acc: 0.5852
Epoch 64/200
1079/1079 [==============================] - 4s - loss: 0.0092 - acc: 0.9972 - val_loss: 4.0801 - val_acc: 0.5704
Epoch 65/200
1079/1079 [==============================] - 4s - loss: 0.0051 - acc: 0.9972 - val_loss: 4.0687 - val_acc: 0.5556
Epoch 66/200
1079/1079 [==============================] - 4s - loss: 0.0070 - acc: 0.9963 - val_loss: 3.9492 - val_acc: 0.5630
Epoch 67/200
1079/1079 [==============================] - 4s - loss: 0.0303 - acc: 0.9917 - val_loss: 3.8158 - val_acc: 0.5593
Epoch 68/200
1079/1079 [==============================] - 4s - loss: 0.0094 - acc: 0.9963 - val_loss: 3.6810 - val_acc: 0.5815
Epoch 69/200
1079/1079 [==============================] - 4s - loss: 0.0092 - acc: 0.9963 - val_loss: 3.5165 - val_acc: 0.5630
Epoch 70/200
1079/1079 [==============================] - 4s - loss: 0.0080 - acc: 0.9963 - val_loss: 3.5533 - val_acc: 0.5444
Epoch 71/200
1079/1079 [==============================] - 4s - loss: 0.0109 - acc: 0.9963 - val_loss: 3.8377 - val_acc: 0.5481
Epoch 72/200
1079/1079 [==============================] - 4s - loss: 0.0087 - acc: 0.9972 - val_loss: 3.9565 - val_acc: 0.5444
Epoch 73/200
1079/1079 [==============================] - 4s - loss: 0.0034 - acc: 1.0000 - val_loss: 4.0644 - val_acc: 0.5481
Epoch 74/200
1079/1079 [==============================] - 4s - loss: 0.0025 - acc: 1.0000 - val_loss: 4.0853 - val_acc: 0.5519
Epoch 75/200
1079/1079 [==============================] - 4s - loss: 0.0095 - acc: 0.9972 - val_loss: 3.7461 - val_acc: 0.5667
Epoch 76/200
1079/1079 [==============================] - 4s - loss: 0.0037 - acc: 0.9991 - val_loss: 3.6588 - val_acc: 0.5667
Epoch 77/200
1079/1079 [==============================] - 4s - loss: 0.0051 - acc: 0.9981 - val_loss: 3.7486 - val_acc: 0.5667
Epoch 78/200
1079/1079 [==============================] - 4s - loss: 0.0030 - acc: 0.9991 - val_loss: 4.0240 - val_acc: 0.5630
Epoch 79/200
1079/1079 [==============================] - 4s - loss: 0.0013 - acc: 1.0000 - val_loss: 4.0851 - val_acc: 0.5593
Epoch 80/200
1079/1079 [==============================] - 4s - loss: 0.0049 - acc: 0.9981 - val_loss: 3.9619 - val_acc: 0.5593
Epoch 81/200
1079/1079 [==============================] - 4s - loss: 0.0131 - acc: 0.9972 - val_loss: 3.8608 - val_acc: 0.5593
Epoch 82/200
1079/1079 [==============================] - 4s - loss: 0.0097 - acc: 0.9972 - val_loss: 3.7717 - val_acc: 0.5556
Epoch 83/200
1079/1079 [==============================] - 4s - loss: 0.0033 - acc: 0.9981 - val_loss: 3.7843 - val_acc: 0.5778
Epoch 84/200
1079/1079 [==============================] - 4s - loss: 0.0104 - acc: 0.9963 - val_loss: 3.8639 - val_acc: 0.5778
Epoch 85/200
1079/1079 [==============================] - 4s - loss: 0.0041 - acc: 0.9991 - val_loss: 4.0992 - val_acc: 0.5778
Epoch 86/200
1079/1079 [==============================] - 4s - loss: 0.0040 - acc: 0.9991 - val_loss: 4.1036 - val_acc: 0.5630
Epoch 87/200
1079/1079 [==============================] - 4s - loss: 0.0081 - acc: 0.9972 - val_loss: 4.0621 - val_acc: 0.5778
Epoch 88/200
1079/1079 [==============================] - 4s - loss: 0.0161 - acc: 0.9963 - val_loss: 4.2910 - val_acc: 0.5481
Epoch 89/200
1079/1079 [==============================] - 4s - loss: 0.0173 - acc: 0.9935 - val_loss: 3.4214 - val_acc: 0.5926
Epoch 90/200
1079/1079 [==============================] - 4s - loss: 0.0262 - acc: 0.9917 - val_loss: 3.7214 - val_acc: 0.5556
Epoch 91/200
1079/1079 [==============================] - 4s - loss: 0.0106 - acc: 0.9954 - val_loss: 3.9681 - val_acc: 0.5407
Epoch 92/200
1079/1079 [==============================] - 4s - loss: 0.0081 - acc: 0.9981 - val_loss: 3.7957 - val_acc: 0.5556
Epoch 93/200
1079/1079 [==============================] - 4s - loss: 0.0061 - acc: 0.9963 - val_loss: 4.0212 - val_acc: 0.5667
Epoch 94/200
1079/1079 [==============================] - 4s - loss: 0.0168 - acc: 0.9963 - val_loss: 3.8465 - val_acc: 0.5630
Epoch 95/200
1079/1079 [==============================] - 4s - loss: 0.0050 - acc: 0.9972 - val_loss: 3.5614 - val_acc: 0.5926
Epoch 96/200
1079/1079 [==============================] - 4s - loss: 0.0061 - acc: 0.9972 - val_loss: 3.6303 - val_acc: 0.5741
Epoch 97/200
1079/1079 [==============================] - 4s - loss: 0.0059 - acc: 0.9981 - val_loss: 3.7978 - val_acc: 0.5630
Epoch 98/200
1079/1079 [==============================] - 4s - loss: 0.0056 - acc: 0.9991 - val_loss: 3.8533 - val_acc: 0.5593
Epoch 99/200
1079/1079 [==============================] - 4s - loss: 0.0064 - acc: 0.9981 - val_loss: 3.8111 - val_acc: 0.5778
Epoch 100/200
1079/1079 [==============================] - 4s - loss: 0.0021 - acc: 1.0000 - val_loss: 3.8455 - val_acc: 0.5778
Epoch 101/200
1079/1079 [==============================] - 4s - loss: 0.0057 - acc: 0.9991 - val_loss: 3.9193 - val_acc: 0.5630
Epoch 102/200
1079/1079 [==============================] - 4s - loss: 0.0091 - acc: 0.9981 - val_loss: 3.9580 - val_acc: 0.5667
Epoch 103/200
1079/1079 [==============================] - 4s - loss: 0.0024 - acc: 0.9991 - val_loss: 4.0448 - val_acc: 0.5630
Epoch 104/200
1079/1079 [==============================] - 4s - loss: 0.0015 - acc: 1.0000 - val_loss: 4.0977 - val_acc: 0.5556
Epoch 105/200
1079/1079 [==============================] - 4s - loss: 0.0029 - acc: 0.9991 - val_loss: 4.1803 - val_acc: 0.5556
Epoch 106/200
1079/1079 [==============================] - 4s - loss: 0.0028 - acc: 0.9981 - val_loss: 4.0790 - val_acc: 0.5667
Epoch 107/200
1079/1079 [==============================] - 4s - loss: 0.0098 - acc: 0.9991 - val_loss: 4.0526 - val_acc: 0.5704
Epoch 108/200
1079/1079 [==============================] - 4s - loss: 0.0045 - acc: 0.9991 - val_loss: 4.2599 - val_acc: 0.5593
Epoch 109/200
1079/1079 [==============================] - 4s - loss: 0.0035 - acc: 0.9981 - val_loss: 4.4004 - val_acc: 0.5296
Epoch 110/200
1079/1079 [==============================] - 4s - loss: 0.0034 - acc: 0.9981 - val_loss: 4.4069 - val_acc: 0.5333
Epoch 111/200
1079/1079 [==============================] - 4s - loss: 0.0023 - acc: 1.0000 - val_loss: 4.2943 - val_acc: 0.5444
Epoch 112/200
1079/1079 [==============================] - 4s - loss: 0.0040 - acc: 0.9981 - val_loss: 4.3057 - val_acc: 0.5556
Epoch 113/200
1079/1079 [==============================] - 4s - loss: 0.0018 - acc: 0.9991 - val_loss: 4.3200 - val_acc: 0.5667
Epoch 114/200
1079/1079 [==============================] - 4s - loss: 0.0086 - acc: 0.9972 - val_loss: 3.9717 - val_acc: 0.5852
Epoch 115/200
1079/1079 [==============================] - 4s - loss: 0.0199 - acc: 0.9944 - val_loss: 3.7965 - val_acc: 0.5519
Epoch 116/200
1079/1079 [==============================] - 4s - loss: 0.0121 - acc: 0.9981 - val_loss: 3.6670 - val_acc: 0.5667
Epoch 117/200
1079/1079 [==============================] - 4s - loss: 0.0098 - acc: 0.9972 - val_loss: 3.6229 - val_acc: 0.5741
Epoch 118/200
1079/1079 [==============================] - 4s - loss: 0.0035 - acc: 0.9991 - val_loss: 3.8224 - val_acc: 0.5556
Epoch 119/200
1079/1079 [==============================] - 4s - loss: 0.0015 - acc: 1.0000 - val_loss: 3.9056 - val_acc: 0.5556
Epoch 120/200
1079/1079 [==============================] - 4s - loss: 0.0040 - acc: 0.9981 - val_loss: 3.9838 - val_acc: 0.5481
Epoch 121/200
1079/1079 [==============================] - 4s - loss: 0.0046 - acc: 0.9972 - val_loss: 3.9091 - val_acc: 0.5741
Epoch 122/200
1079/1079 [==============================] - 4s - loss: 0.0035 - acc: 0.9991 - val_loss: 3.8520 - val_acc: 0.5963
Epoch 123/200
1024/1079 [===========================&gt;..] - ETA: 0s - loss: 0.0039 - acc: 0.9990    </pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-51-1ed20b8ab71a&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     34</span> 
<span class="ansi-green-intense-fg ansi-bold">     35</span> lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],
<span class="ansi-green-fg">---&gt; 36</span><span class="ansi-red-fg">           epochs=200, batch_size=128,validation_split=0.2,callbacks=[history])
</span><span class="ansi-green-intense-fg ansi-bold">     37</span> 
<span class="ansi-green-intense-fg ansi-bold">     38</span> <span class="ansi-red-fg">#lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1428</span>                               val_f<span class="ansi-blue-fg">=</span>val_f<span class="ansi-blue-fg">,</span> val_ins<span class="ansi-blue-fg">=</span>val_ins<span class="ansi-blue-fg">,</span> shuffle<span class="ansi-blue-fg">=</span>shuffle<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1429</span>                               callback_metrics<span class="ansi-blue-fg">=</span>callback_metrics<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1430</span><span class="ansi-red-fg">                               initial_epoch=initial_epoch)
</span><span class="ansi-green-intense-fg ansi-bold">   1431</span> 
<span class="ansi-green-intense-fg ansi-bold">   1432</span>     <span class="ansi-green-fg">def</span> evaluate<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">32</span><span class="ansi-blue-fg">,</span> verbose<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> sample_weight<span class="ansi-blue-fg">=</span>None<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_fit_loop</span><span class="ansi-blue-fg">(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)</span>
<span class="ansi-green-intense-fg ansi-bold">   1077</span>                 batch_logs<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">'size'</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> len<span class="ansi-blue-fg">(</span>batch_ids<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1078</span>                 callbacks<span class="ansi-blue-fg">.</span>on_batch_begin<span class="ansi-blue-fg">(</span>batch_index<span class="ansi-blue-fg">,</span> batch_logs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">-&gt; 1079</span><span class="ansi-red-fg">                 </span>outs <span class="ansi-blue-fg">=</span> f<span class="ansi-blue-fg">(</span>ins_batch<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1080</span>                 <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> isinstance<span class="ansi-blue-fg">(</span>outs<span class="ansi-blue-fg">,</span> list<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1081</span>                     outs <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>outs<span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, inputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2266</span>         updated = session.run(self.outputs + [self.updates_op],
<span class="ansi-green-intense-fg ansi-bold">   2267</span>                               feed_dict<span class="ansi-blue-fg">=</span>feed_dict<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 2268</span><span class="ansi-red-fg">                               **self.session_kwargs)
</span><span class="ansi-green-intense-fg ansi-bold">   2269</span>         <span class="ansi-green-fg">return</span> updated<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span>len<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>outputs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">   2270</span> 

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-fg">(self, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">    787</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    788</span>       result = self._run(None, fetches, feed_dict, options_ptr,
<span class="ansi-green-fg">--&gt; 789</span><span class="ansi-red-fg">                          run_metadata_ptr)
</span><span class="ansi-green-intense-fg ansi-bold">    790</span>       <span class="ansi-green-fg">if</span> run_metadata<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    791</span>         proto_data <span class="ansi-blue-fg">=</span> tf_session<span class="ansi-blue-fg">.</span>TF_GetBuffer<span class="ansi-blue-fg">(</span>run_metadata_ptr<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_run</span><span class="ansi-blue-fg">(self, handle, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">    995</span>     <span class="ansi-green-fg">if</span> final_fetches <span class="ansi-green-fg">or</span> final_targets<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    996</span>       results = self._do_run(handle, final_targets, final_fetches,
<span class="ansi-green-fg">--&gt; 997</span><span class="ansi-red-fg">                              feed_dict_string, options, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">    998</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    999</span>       results <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_do_run</span><span class="ansi-blue-fg">(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">   1130</span>     <span class="ansi-green-fg">if</span> handle <span class="ansi-green-fg">is</span> None<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1131</span>       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
<span class="ansi-green-fg">-&gt; 1132</span><span class="ansi-red-fg">                            target_list, options, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">   1133</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1134</span>       return self._do_call(_prun_fn, self._session, handle, feed_dict,

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_do_call</span><span class="ansi-blue-fg">(self, fn, *args)</span>
<span class="ansi-green-intense-fg ansi-bold">   1137</span>   <span class="ansi-green-fg">def</span> _do_call<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> fn<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1138</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">-&gt; 1139</span><span class="ansi-red-fg">       </span><span class="ansi-green-fg">return</span> fn<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1140</span>     <span class="ansi-green-fg">except</span> errors<span class="ansi-blue-fg">.</span>OpError <span class="ansi-green-fg">as</span> e<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1141</span>       message <span class="ansi-blue-fg">=</span> compat<span class="ansi-blue-fg">.</span>as_text<span class="ansi-blue-fg">(</span>e<span class="ansi-blue-fg">.</span>message<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_run_fn</span><span class="ansi-blue-fg">(session, feed_dict, fetch_list, target_list, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">   1119</span>         return tf_session.TF_Run(session, options,
<span class="ansi-green-intense-fg ansi-bold">   1120</span>                                  feed_dict<span class="ansi-blue-fg">,</span> fetch_list<span class="ansi-blue-fg">,</span> target_list<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1121</span><span class="ansi-red-fg">                                  status, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">   1122</span> 
<span class="ansi-green-intense-fg ansi-bold">   1123</span>     <span class="ansi-green-fg">def</span> _prun_fn<span class="ansi-blue-fg">(</span>session<span class="ansi-blue-fg">,</span> handle<span class="ansi-blue-fg">,</span> feed_dict<span class="ansi-blue-fg">,</span> fetch_list<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [45]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
<span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>

<span class="c1">#LSTM_layer = Bidirectional(LSTM(units=128))(embedded_sequences)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">))(</span><span class="n">embedded_sequences</span><span class="p">)</span>
<span class="c1">#LSTM_layer = Bidirectional(LSTM_layer)</span>
<span class="n">LSTM_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">LSTM_layer</span><span class="p">)</span>

<span class="n">derived_features</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">34</span><span class="p">,))</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">derived_features</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">LSTM_layer</span><span class="p">,</span><span class="n">hidden_layer</span><span class="p">])</span>
<span class="n">merged_flat</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>

<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">merged_flat</span><span class="p">)</span>
<span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span>

<span class="c1">#penultimate = Dense(units=128)(LSTM_layer) # optional additional hidden layer</span>
<span class="c1">#penultimate = Dropout(0.5)(penultimate) # optional additional hidden layer</span>
<span class="c1">#preds = Dense(labels_mw.shape[1], activation='softmax')(penultimate) # optional additional hidden layer</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">labels_mw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">hidden_layer_3</span><span class="p">)</span> <span class="c1"># comment out if penultimate layer is used</span>

<span class="n">lstm_text_and_manual_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">sequence_input</span><span class="p">,</span><span class="n">derived_features</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'Adam'</span><span class="p">,</span>              
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">],)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">LossHistory</span><span class="p">()</span>

<span class="n">lstm_text_and_manual_model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">normed_twts</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span><span class="n">manual_features_scaled</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">]],</span> <span class="n">labels_mw</span><span class="p">[</span><span class="n">rand_ix</span><span class="p">],</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">history</span><span class="p">])</span>

<span class="c1">#lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],</span>
<span class="c1">#          epochs=10, batch_size=128,validation_split=0.2)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 1079 samples, validate on 270 samples
Epoch 1/200
1079/1079 [==============================] - 8s - loss: 1.4782 - acc: 0.3568 - val_loss: 1.1399 - val_acc: 0.5407
Epoch 2/200
1079/1079 [==============================] - 4s - loss: 1.2409 - acc: 0.4930 - val_loss: 1.0713 - val_acc: 0.5778
Epoch 3/200
 512/1079 [=============&gt;................] - ETA: 2s - loss: 1.1761 - acc: 0.5332</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-45-401af5b432e9&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span> 
<span class="ansi-green-intense-fg ansi-bold">     32</span> lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],
<span class="ansi-green-fg">---&gt; 33</span><span class="ansi-red-fg">           epochs=200, batch_size=128,validation_split=0.2,callbacks=[history])
</span><span class="ansi-green-intense-fg ansi-bold">     34</span> 
<span class="ansi-green-intense-fg ansi-bold">     35</span> <span class="ansi-red-fg">#lstm_text_and_manual_model.fit([normed_twts[rand_ix],manual_features_scaled[rand_ix]], labels_mw[rand_ix],</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1428</span>                               val_f<span class="ansi-blue-fg">=</span>val_f<span class="ansi-blue-fg">,</span> val_ins<span class="ansi-blue-fg">=</span>val_ins<span class="ansi-blue-fg">,</span> shuffle<span class="ansi-blue-fg">=</span>shuffle<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1429</span>                               callback_metrics<span class="ansi-blue-fg">=</span>callback_metrics<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1430</span><span class="ansi-red-fg">                               initial_epoch=initial_epoch)
</span><span class="ansi-green-intense-fg ansi-bold">   1431</span> 
<span class="ansi-green-intense-fg ansi-bold">   1432</span>     <span class="ansi-green-fg">def</span> evaluate<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">32</span><span class="ansi-blue-fg">,</span> verbose<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> sample_weight<span class="ansi-blue-fg">=</span>None<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc</span> in <span class="ansi-cyan-fg">_fit_loop</span><span class="ansi-blue-fg">(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)</span>
<span class="ansi-green-intense-fg ansi-bold">   1077</span>                 batch_logs<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">'size'</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> len<span class="ansi-blue-fg">(</span>batch_ids<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1078</span>                 callbacks<span class="ansi-blue-fg">.</span>on_batch_begin<span class="ansi-blue-fg">(</span>batch_index<span class="ansi-blue-fg">,</span> batch_logs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">-&gt; 1079</span><span class="ansi-red-fg">                 </span>outs <span class="ansi-blue-fg">=</span> f<span class="ansi-blue-fg">(</span>ins_batch<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1080</span>                 <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> isinstance<span class="ansi-blue-fg">(</span>outs<span class="ansi-blue-fg">,</span> list<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1081</span>                     outs <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>outs<span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, inputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2266</span>         updated = session.run(self.outputs + [self.updates_op],
<span class="ansi-green-intense-fg ansi-bold">   2267</span>                               feed_dict<span class="ansi-blue-fg">=</span>feed_dict<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 2268</span><span class="ansi-red-fg">                               **self.session_kwargs)
</span><span class="ansi-green-intense-fg ansi-bold">   2269</span>         <span class="ansi-green-fg">return</span> updated<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span>len<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>outputs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">   2270</span> 

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-fg">(self, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">    787</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    788</span>       result = self._run(None, fetches, feed_dict, options_ptr,
<span class="ansi-green-fg">--&gt; 789</span><span class="ansi-red-fg">                          run_metadata_ptr)
</span><span class="ansi-green-intense-fg ansi-bold">    790</span>       <span class="ansi-green-fg">if</span> run_metadata<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    791</span>         proto_data <span class="ansi-blue-fg">=</span> tf_session<span class="ansi-blue-fg">.</span>TF_GetBuffer<span class="ansi-blue-fg">(</span>run_metadata_ptr<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_run</span><span class="ansi-blue-fg">(self, handle, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">    995</span>     <span class="ansi-green-fg">if</span> final_fetches <span class="ansi-green-fg">or</span> final_targets<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    996</span>       results = self._do_run(handle, final_targets, final_fetches,
<span class="ansi-green-fg">--&gt; 997</span><span class="ansi-red-fg">                              feed_dict_string, options, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">    998</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    999</span>       results <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_do_run</span><span class="ansi-blue-fg">(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">   1130</span>     <span class="ansi-green-fg">if</span> handle <span class="ansi-green-fg">is</span> None<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1131</span>       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
<span class="ansi-green-fg">-&gt; 1132</span><span class="ansi-red-fg">                            target_list, options, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">   1133</span>     <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1134</span>       return self._do_call(_prun_fn, self._session, handle, feed_dict,

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_do_call</span><span class="ansi-blue-fg">(self, fn, *args)</span>
<span class="ansi-green-intense-fg ansi-bold">   1137</span>   <span class="ansi-green-fg">def</span> _do_call<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> fn<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1138</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">-&gt; 1139</span><span class="ansi-red-fg">       </span><span class="ansi-green-fg">return</span> fn<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">   1140</span>     <span class="ansi-green-fg">except</span> errors<span class="ansi-blue-fg">.</span>OpError <span class="ansi-green-fg">as</span> e<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   1141</span>       message <span class="ansi-blue-fg">=</span> compat<span class="ansi-blue-fg">.</span>as_text<span class="ansi-blue-fg">(</span>e<span class="ansi-blue-fg">.</span>message<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/Users/pf494t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc</span> in <span class="ansi-cyan-fg">_run_fn</span><span class="ansi-blue-fg">(session, feed_dict, fetch_list, target_list, options, run_metadata)</span>
<span class="ansi-green-intense-fg ansi-bold">   1119</span>         return tf_session.TF_Run(session, options,
<span class="ansi-green-intense-fg ansi-bold">   1120</span>                                  feed_dict<span class="ansi-blue-fg">,</span> fetch_list<span class="ansi-blue-fg">,</span> target_list<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1121</span><span class="ansi-red-fg">                                  status, run_metadata)
</span><span class="ansi-green-intense-fg ansi-bold">   1122</span> 
<span class="ansi-green-intense-fg ansi-bold">   1123</span>     <span class="ansi-green-fg">def</span> _prun_fn<span class="ansi-blue-fg">(</span>session<span class="ansi-blue-fg">,</span> handle<span class="ansi-blue-fg">,</span> feed_dict<span class="ansi-blue-fg">,</span> fetch_list<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [128]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="mi">959</span><span class="o">/</span><span class="mi">128</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[128]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>7</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [129]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[129]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>16</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [130]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">history</span><span class="o">.</span><span class="n">losses</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[130]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>[1.57277,
 1.5974461,
 1.585048,
 1.7846346,
 1.7608865,
 1.5420271,
 1.5961046,
 1.5438222,
 1.4333953,
 1.5578783,
 1.6414911,
 1.5692115,
 1.6736684,
 1.4775219,
 1.4439013,
 1.6056043]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Benchmark">Benchmark<a class="anchor-link" href="#Benchmark">¶</a></h1><p>Compare deep learning results with bag-of-words classifier using random forests. These scripts adapted from mw167's sentiment classifier used for the spotlight on Silicon Valley</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [21]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Binarizer</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span>  <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span><span class="c1"># , cohen_kappa_score</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Imputer</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">regexp_tokenize</span><span class="p">,</span> <span class="n">wordpunct_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">itemfreq</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/pf494t/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
/Users/pf494t/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="get-BoW-features-for-top-500">get BoW features for top 500<a class="anchor-link" href="#get-BoW-features-for-top-500">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [22]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">stem_it</span><span class="p">(</span><span class="n">lw</span><span class="p">):</span>
    <span class="n">rl</span> <span class="o">=</span> <span class="s2">""</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">lw</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">j</span><span class="p">:</span>
        <span class="n">rl</span> <span class="o">+=</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">rl</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>

<span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [55]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">df_train</span><span class="p">[</span><span class="s1">'body'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">'body'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">stem_it</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">num_feat</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">countv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'\w+|\S+'</span><span class="p">),</span> 
            <span class="n">ngram_range</span><span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> 
            <span class="n">stop_words</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">'english'</span><span class="p">),</span>
            <span class="n">analyzer</span><span class="o">=</span><span class="s1">'word'</span><span class="p">,</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="n">num_feat</span><span class="p">,</span> 
            <span class="n">min_df</span><span class="o">=</span> <span class="mi">5</span><span class="p">)</span> <span class="c1">#Switch with TfidfVectorizer, CountVectorizer</span>

<span class="n">countv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">'body'</span><span class="p">])</span>
<span class="n">bow_features</span> <span class="o">=</span> <span class="n">countv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">'body'</span><span class="p">])</span>
<span class="n">bow_features</span> <span class="o">=</span> <span class="n">bow_features</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span> <span class="c1">#this is sparse matrix representation</span>

<span class="n">bow_features</span> <span class="o">=</span>  <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">bow_features</span><span class="p">)</span> <span class="c1">#this is NOT sparse matrix representation</span>
<span class="n">bow_features</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">countv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="n">bow_features</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">index</span>

<span class="k">print</span> <span class="s2">"Number of features extracted: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">bow_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of features extracted: 474
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/pf494t/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  if __name__ == '__main__':
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [56]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#df_train</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [57]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">data_and_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:],</span><span class="n">bow_features</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [58]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#testing</span>
<span class="k">print</span> <span class="n">data_and_labels</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">df_train</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">bow_features</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>(1199, 509)
(1199, 37)
(1199, 474)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [59]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data_and_labels</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.7</span>

<span class="n">trainFrame</span> <span class="o">=</span> <span class="n">data_and_labels</span><span class="p">[</span><span class="n">msk</span><span class="p">]</span>
<span class="n">testFrame</span> <span class="o">=</span> <span class="n">data_and_labels</span><span class="p">[</span><span class="o">~</span><span class="n">msk</span><span class="p">]</span>

<span class="n">trainFrame</span> <span class="o">=</span> <span class="n">trainFrame</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">testFrame</span> <span class="o">=</span> <span class="n">testFrame</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [160]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">svm_clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span> <span class="c1">#Classification Accuracy</span>

<span class="n">param_dist</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">],</span>
					<span class="n">kernel</span><span class="o">=</span><span class="p">[</span><span class="s1">'rbf'</span><span class="p">,</span><span class="s1">'linear'</span><span class="p">,</span><span class="s1">'sigmoid'</span><span class="p">],</span>
					<span class="n">gamma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1">#gamma is auto</span>

<span class="n">param_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">svm_clf</span><span class="p">,</span> 
                        <span class="n">param_dist</span><span class="p">,</span> 
                        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                        <span class="n">scoring</span><span class="o">=</span><span class="s1">'accuracy'</span><span class="p">,</span> 
                        <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>  <span class="c1">#7*3</span>
                        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                        <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
                        <span class="n">refit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                        
<span class="n">param_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">trainFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">"Best paramenters are: "</span><span class="p">,</span> <span class="n">param_search</span><span class="o">.</span><span class="n">best_params_</span>
 
<span class="n">svm_clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">param_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">'C'</span><span class="p">],</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">param_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">'kernel'</span><span class="p">],</span>
					<span class="n">gamma</span> <span class="o">=</span> <span class="n">param_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">'gamma'</span><span class="p">])</span>


<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">svm_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">trainFrame</span><span class="p">[</span><span class="s1">'sentiment_code'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">svm_predictions</span> <span class="o">=</span> <span class="n">svm_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="c1">#svm_predictions_proba =  svm_clf.predict_proba(testFrame.iloc[:,1:].values)</span>
<span class="c1">#svm_predictions[svm_predictions==1] = 2</span>
<span class="c1">#svm_predictions[np.product(svm_predictions_proba,axis=1)&gt;=0.21] = 1</span>

<span class="c1">#multiclass_classifier_quality(testFrame.sentiment_code.values,sentitest_pred_logit)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Best paramenters are:  {'kernel': 'sigmoid', 'C': 100, 'gamma': 0.001}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [161]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">multiclass_classifier_quality</span><span class="p">(</span><span class="n">svm_predictions</span><span class="p">,</span><span class="n">testFrame</span><span class="p">[</span><span class="s1">'sentiment_code'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>
Confusion Matrix:
[[ 10   2   1   6]
 [ 29 147  40  27]
 [  5   8  23  11]
 [  5  11   6  38]]

Classification Accuracy
59.078591 percent

Precision-Recall
             precision    recall  f1-score   support

          0       0.20      0.53      0.29        19
          1       0.88      0.60      0.72       243
          2       0.33      0.49      0.39        47
          3       0.46      0.63      0.54        60

avg / total       0.70      0.59      0.62       369


Most recurrent class is (ZERO RULE) 65.8536585366

Random guessing accuracy is 31.0287086611
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [156]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">itemfreq</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">sentiment_code</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Mixed'</span><span class="p">,</span><span class="s1">'Negative'</span><span class="p">,</span><span class="s1">'Neutral'</span><span class="p">,</span><span class="s1">'Positive'</span><span class="p">]</span>
<span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">l</span><span class="p">,</span><span class="n">a</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="n">multiclass_classifier_quality</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">svm_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-156-79e19cf5b38f&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>a <span class="ansi-blue-fg">=</span> itemfreq<span class="ansi-blue-fg">(</span>X<span class="ansi-blue-fg">.</span>sentiment_code<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> l <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">'Mixed'</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">'Negative'</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">'Neutral'</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">'Positive'</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> zip<span class="ansi-blue-fg">(</span>a<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span>l<span class="ansi-blue-fg">,</span>a<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">/</span>float<span class="ansi-blue-fg">(</span>sum<span class="ansi-blue-fg">(</span>a<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">*</span><span class="ansi-cyan-fg">100</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> multiclass_classifier_quality<span class="ansi-blue-fg">(</span>testFrame<span class="ansi-blue-fg">.</span>sentiment_code<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">,</span> svm_predictions<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name 'X' is not defined</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [154]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">svm_predictions</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[154]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array([2, 2, 1, 3, 1, 0, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1,
       1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1,
       1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 3, 1, 0, 1, 1,
       2, 1, 1, 1, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 2, 2,
       1, 1, 2, 1, 2, 1, 3, 3, 1, 2, 2, 3, 1, 1, 1, 1, 0, 2, 1, 1, 3, 1, 2,
       1, 3, 1, 3, 1, 1, 3, 2, 1, 0, 1, 3, 1, 1, 3, 1, 3, 2, 1, 2, 1, 1, 1,
       1, 0, 1, 2, 1, 3, 1, 1, 0, 1, 1, 1, 1, 3, 3, 1, 1, 3, 2, 2, 3, 2, 1,
       1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1,
       2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 0, 3, 1, 1, 1, 2, 1, 0, 1, 3,
       2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 3,
       0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 2, 3, 3, 1, 1, 2, 1, 1,
       1, 2, 0, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1,
       1, 3, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 3, 1, 1, 3, 1,
       1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 3, 1, 1,
       3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3,
       1, 2, 1, 1, 3, 3, 0, 2, 3, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2,
       1])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [60]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">numtrees</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1">#(sqrt(p))</span>
<span class="c1">#forest = RandomForestClassifier(n_estimators = numtrees, random_state=0, max_features=0.95, max_depth=9, min_samples_leaf=5) #RANDOM SEED SET</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">numtrees</span><span class="p">,</span> 
							<span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
							<span class="c1">#class_weight = 'subsample')</span>
							<span class="c1">#class_weight={0:0, 1:1, 2:1, 3:1})</span>
	

<span class="n">cv_num</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">param_space</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> 
                <span class="n">max_features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">1.001</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">),</span> 
                <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>

<span class="c1">#http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</span>
<span class="c1">#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score</span>
<span class="n">param_searcher</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span> 
                        <span class="n">param_space</span><span class="p">,</span> 
                        <span class="n">cv</span><span class="o">=</span><span class="n">cv_num</span><span class="p">,</span> 
                        <span class="n">scoring</span><span class="o">=</span><span class="s1">'f1_micro'</span><span class="p">,</span>
                        <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span> 
                        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                        <span class="n">refit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">param_searcher</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">trainFrame</span><span class="p">[</span><span class="s1">'sentiment_code'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">"Best paramenters are: "</span><span class="p">,</span> <span class="n">param_searcher</span><span class="o">.</span><span class="n">best_params_</span>


<span class="n">forest</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">param_searcher</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">'max_depth'</span><span class="p">],</span> <span class="n">max_features</span><span class="o">=</span><span class="n">param_searcher</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">'max_features'</span><span class="p">],</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">param_searcher</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">'min_samples_leaf'</span><span class="p">])</span>           

<span class="n">forest</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">trainFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1">#------------------------------------Printing TOP K features-------------------------------------</span>
<span class="c1">#</span>
<span class="n">fnames</span> <span class="o">=</span> <span class="n">data_and_labels</span><span class="o">.</span><span class="n">columns</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">forest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Best paramenters are:  {'max_features': 0.70000000000000018, 'max_depth': 18, 'min_samples_leaf': 21}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [41]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">data_and_labels</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[41]:</div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>sentiment_code</th>
<th>emoji_sentiment</th>
<th>positive_emoticon</th>
<th>negative_emoticon</th>
<th>capitals</th>
<th>nExclamations</th>
<th>nlen</th>
<th>acronym_value</th>
<th>positive_word_count</th>
<th>negative_word_count</th>
<th>...</th>
<th>worst custom servic</th>
<th>would</th>
<th>wow</th>
<th>wtf</th>
<th>yall</th>
<th>ye</th>
<th>yeah</th>
<th>year</th>
<th>yesterday</th>
<th>yet</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.079710</td>
<td>0.0</td>
<td>0.935714</td>
<td>0.0</td>
<td>3.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>1</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.184000</td>
<td>0.0</td>
<td>0.664286</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>2</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.160000</td>
<td>0.0</td>
<td>0.164286</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>3</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.065421</td>
<td>0.0</td>
<td>0.742857</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>4</th>
<td>3.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.121212</td>
<td>0.0</td>
<td>0.228571</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>5</th>
<td>3.0</td>
<td>0.417804</td>
<td>0.0</td>
<td>0.0</td>
<td>0.061538</td>
<td>0.0</td>
<td>0.435714</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>6</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.063830</td>
<td>0.0</td>
<td>0.328571</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>7</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.066667</td>
<td>0.0</td>
<td>0.821429</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>8</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.137931</td>
<td>0.0</td>
<td>0.192857</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>9</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.296296</td>
<td>1.0</td>
<td>0.907143</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>10</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.068182</td>
<td>0.0</td>
<td>0.628571</td>
<td>0.0</td>
<td>2.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>11</th>
<td>3.0</td>
<td>0.563761</td>
<td>0.0</td>
<td>0.0</td>
<td>0.063291</td>
<td>2.0</td>
<td>0.478571</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>12</th>
<td>0.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.071429</td>
<td>1.0</td>
<td>0.828571</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>13</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.148148</td>
<td>0.0</td>
<td>0.378571</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>14</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.285714</td>
<td>0.0</td>
<td>0.100000</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>15</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>16</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.181818</td>
<td>0.0</td>
<td>0.157143</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>17</th>
<td>0.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.063492</td>
<td>0.0</td>
<td>0.421429</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>18</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.214286</td>
<td>0.0</td>
<td>0.200000</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>19</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.135135</td>
<td>0.0</td>
<td>0.257143</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>20</th>
<td>1.0</td>
<td>0.563761</td>
<td>0.0</td>
<td>0.0</td>
<td>0.013605</td>
<td>0.0</td>
<td>0.835714</td>
<td>0.0</td>
<td>2.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>21</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.090909</td>
<td>0.0</td>
<td>0.071429</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>22</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.000000</td>
<td>0.0</td>
<td>0.221429</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>23</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>24</th>
<td>3.0</td>
<td>0.522114</td>
<td>0.0</td>
<td>0.0</td>
<td>0.261905</td>
<td>0.0</td>
<td>0.121429</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>25</th>
<td>0.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.181818</td>
<td>1.0</td>
<td>0.514286</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>26</th>
<td>3.0</td>
<td>0.746087</td>
<td>0.0</td>
<td>0.0</td>
<td>0.044444</td>
<td>1.0</td>
<td>0.914286</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>27</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.188679</td>
<td>0.0</td>
<td>0.371429</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>28</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.056818</td>
<td>0.0</td>
<td>0.621429</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>29</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>...</th>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<th>1466</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.193548</td>
<td>0.0</td>
<td>0.214286</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1467</th>
<td>3.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.168224</td>
<td>0.0</td>
<td>0.607143</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1468</th>
<td>3.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.142857</td>
<td>0.0</td>
<td>0.228571</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1470</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.051471</td>
<td>0.0</td>
<td>0.950000</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1471</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.128571</td>
<td>0.0</td>
<td>0.357143</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1472</th>
<td>0.0</td>
<td>0.147972</td>
<td>0.0</td>
<td>0.0</td>
<td>0.206897</td>
<td>1.0</td>
<td>0.721429</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1473</th>
<td>0.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.057471</td>
<td>0.0</td>
<td>0.592857</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1474</th>
<td>1.0</td>
<td>-0.374729</td>
<td>0.0</td>
<td>0.0</td>
<td>0.050000</td>
<td>0.0</td>
<td>0.921429</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1475</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.160920</td>
<td>0.0</td>
<td>0.464286</td>
<td>0.0</td>
<td>1.0</td>
<td>3.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1476</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.151515</td>
<td>0.0</td>
<td>0.228571</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1477</th>
<td>3.0</td>
<td>0.220968</td>
<td>0.0</td>
<td>0.0</td>
<td>0.222222</td>
<td>0.0</td>
<td>0.207143</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1478</th>
<td>0.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.029851</td>
<td>0.0</td>
<td>0.464286</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1479</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.104167</td>
<td>4.0</td>
<td>0.314286</td>
<td>0.0</td>
<td>0.0</td>
<td>2.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1480</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.051471</td>
<td>0.0</td>
<td>0.942857</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1481</th>
<td>2.0</td>
<td>0.412435</td>
<td>0.0</td>
<td>0.0</td>
<td>0.120370</td>
<td>0.0</td>
<td>0.635714</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1482</th>
<td>3.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.144144</td>
<td>0.0</td>
<td>0.785714</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1483</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.085714</td>
<td>0.0</td>
<td>0.235714</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1484</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.095238</td>
<td>2.0</td>
<td>0.735714</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1485</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.059406</td>
<td>0.0</td>
<td>0.714286</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1486</th>
<td>3.0</td>
<td>0.632917</td>
<td>0.0</td>
<td>0.0</td>
<td>0.073171</td>
<td>2.0</td>
<td>0.700000</td>
<td>0.0</td>
<td>3.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1487</th>
<td>3.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.058824</td>
<td>0.0</td>
<td>0.364286</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1489</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.106195</td>
<td>0.0</td>
<td>0.742857</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1491</th>
<td>3.0</td>
<td>0.220968</td>
<td>0.0</td>
<td>0.0</td>
<td>0.089431</td>
<td>0.0</td>
<td>0.685714</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1492</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.018868</td>
<td>0.0</td>
<td>0.357143</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1493</th>
<td>2.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.153846</td>
<td>0.0</td>
<td>0.185714</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1494</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.058394</td>
<td>0.0</td>
<td>0.935714</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1495</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.035971</td>
<td>0.0</td>
<td>0.942857</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1496</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.042857</td>
<td>0.0</td>
<td>0.492857</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1497</th>
<td>3.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.071429</td>
<td>0.0</td>
<td>0.807143</td>
<td>0.0</td>
<td>2.0</td>
<td>0.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr>
<th>1498</th>
<td>1.0</td>
<td>0.212465</td>
<td>0.0</td>
<td>0.0</td>
<td>0.350000</td>
<td>0.0</td>
<td>0.135714</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
</tbody>
</table>
<p>1433 rows × 509 columns</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [61]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#sentitest_pred = forest.predict(testFrame.ix[:,0:-2].values)</span>
<span class="n">sentitest_pred</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">multiclass_classifier_quality</span><span class="p">(</span><span class="n">true</span><span class="p">,</span><span class="n">pred</span><span class="p">):</span>
    <span class="sd">"""'multiclass_classifier_quality' It displays some performance metrics for a multiclass classifier."""</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Confusion Matrix:"</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true</span><span class="p">,</span><span class="n">pred</span><span class="p">))</span>
    <span class="k">print</span> <span class="n">a</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Classification Accuracy"</span>
    <span class="k">print</span> <span class="s2">"</span><span class="si">%f</span><span class="s2"> percent"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Precision-Recall"</span>
    <span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Most recurrent class is (ZERO RULE)"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">itemfreq</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Random guessing accuracy is"</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span><span class="n">r1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    

<span class="n">multiclass_classifier_quality</span><span class="p">(</span><span class="n">testFrame</span><span class="p">[</span><span class="s1">'sentiment_code'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">sentitest_pred</span><span class="p">)</span>
<span class="c1">#three_class_kappa(testFrame.sentiment_code.values, sentitest_pred)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>
Confusion Matrix:
[[  0  32   6  11]
 [  0 145  10  13]
 [  0  44  15  11]
 [  0  27  14  41]]

Classification Accuracy
54.471545 percent

Precision-Recall
             precision    recall  f1-score   support

          0       0.00      0.00      0.00        49
          1       0.58      0.86      0.70       168
          2       0.33      0.21      0.26        70
          3       0.54      0.50      0.52        82

avg / total       0.45      0.54      0.48       369


Most recurrent class is (ZERO RULE) 45.5284552846

Random guessing accuracy is 31.0287086611
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/pf494t/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [62]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">multiclass_classifier_quality</span><span class="p">(</span><span class="n">true</span><span class="p">,</span><span class="n">pred</span><span class="p">):</span>
    <span class="sd">"""'multiclass_classifier_quality' It displays some performance metrics for a multiclass classifier."""</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Confusion Matrix:"</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true</span><span class="p">,</span><span class="n">pred</span><span class="p">))</span>
    <span class="k">print</span> <span class="n">a</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Classification Accuracy"</span>
    <span class="k">print</span> <span class="s2">"</span><span class="si">%f</span><span class="s2"> percent"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Precision-Recall"</span>
    <span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Most recurrent class is (ZERO RULE)"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">itemfreq</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
    <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Random guessing accuracy is"</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span><span class="n">r1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [63]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#for f in range(100):</span>
<span class="c1">#	print("%d. feature %d = '%s'  \t (%f)" % (f + 1, indices[f], fnames[indices[f]], importances[indices[f]]))</span>
<span class="c1">#	</span>
<span class="c1">#						</span>
<span class="n">sentitest_pred</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">multiclass_classifier_quality</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">sentitest_pred</span><span class="p">)</span>
<span class="n">three_class_kappa</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">sentitest_pred</span><span class="p">)</span>

<span class="n">analyze</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">analyze</span><span class="p">[</span><span class="s1">'score'</span><span class="p">]</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">analyze</span><span class="p">[</span><span class="s1">'real'</span><span class="p">]</span> <span class="o">=</span> <span class="n">testFrame</span><span class="o">.</span><span class="n">sentiment_code</span><span class="o">.</span><span class="n">values</span>
<span class="n">analyze</span><span class="p">[</span><span class="s1">'predictions'</span><span class="p">]</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">analyze</span><span class="p">[</span><span class="s1">'raw_tweet'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">testFrame</span><span class="o">.</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>
Confusion Matrix:
[[  0  32   6  11]
 [  0 145  10  13]
 [  0  44  15  11]
 [  0  27  14  41]]

Classification Accuracy
54.471545 percent

Precision-Recall
             precision    recall  f1-score   support

          0       0.00      0.00      0.00        49
          1       0.58      0.86      0.70       168
          2       0.33      0.21      0.26        70
          3       0.54      0.50      0.52        82

avg / total       0.45      0.54      0.48       369


Most recurrent class is (ZERO RULE) 45.5284552846

Random guessing accuracy is 31.0287086611
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/pf494t/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: DeprecationWarning: 
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing

See the documentation here:
http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-63-d5b0594cd469&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> 
<span class="ansi-green-intense-fg ansi-bold">      7</span> multiclass_classifier_quality<span class="ansi-blue-fg">(</span>testFrame<span class="ansi-blue-fg">.</span>sentiment_code<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">,</span> sentitest_pred<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">----&gt; 8</span><span class="ansi-red-fg"> </span>three_class_kappa<span class="ansi-blue-fg">(</span>testFrame<span class="ansi-blue-fg">.</span>sentiment_code<span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">,</span> sentitest_pred<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      9</span> 
<span class="ansi-green-intense-fg ansi-bold">     10</span> analyze <span class="ansi-blue-fg">=</span> pd<span class="ansi-blue-fg">.</span>DataFrame<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name 'three_class_kappa' is not defined</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Junk-below">Junk below<a class="anchor-link" href="#Junk-below">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">texts</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'body'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">MAX_NB_WORDS</span><span class="p">)</span>
<span class="c1">#filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n'</span>
<span class="c1">#tokenizer = Tokenizer(num_words=MAX_NB_WORDS,filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n')</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> unique tokens.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>

<span class="c1">#labels = to_categorical(np.asarray(labels))</span>
<span class="c1">#labels = to_categorical(np.asarray(df['sentiment']))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Shape of data tensor:'</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Shape of label tensor:'</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">df2</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">labeled_twts_dir</span> <span class="o">+</span> <span class="s1">'/manual_features_from_3batch_tweets.csv'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">ls</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">pf494t</span><span class="o">/</span><span class="n">local_scripts</span><span class="o">/</span><span class="n">handlabeled_tweets_from_nicholas</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="nb">sum</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'sentiment'</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()[</span><span class="s1">'body'</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'sentiment'</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">'sentiment'</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()[</span><span class="s1">'body'</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">xx</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">x_vals</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I'd expect if you change to</p>
<p>X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows, 1)
X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows, 1)
and</p>
<p>input_shape=(img_cols, img_rows, 1)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">gloveData</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'body'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">aggregateWordVectors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">wordVecs</span><span class="p">,</span><span class="n">vecSize</span><span class="p">))),</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"glove_"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vecSize</span><span class="o">*</span><span class="mi">3</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">gloveData</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">infty</span><span class="p">,</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">gloveData</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">infty</span><span class="p">,</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Go back and change 0 to median value...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">gloveData</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Need to go back and just delete all the mixed clases</span>
<span class="n">sentiment_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Negative'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s1">'Neutral'</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="s1">'Positive'</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="s1">'Mixed'</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.7</span>
<span class="n">trainFrame</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">msk</span><span class="p">]</span>
<span class="n">testFrame</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">msk</span><span class="p">]</span>
<span class="n">trainFrame</span> <span class="o">=</span> <span class="n">trainFrame</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">testFrame</span> <span class="o">=</span> <span class="n">testFrame</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">msk</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">msk</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentiment_dict</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">y_train</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">msk</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">~</span><span class="n">msk</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentiment_dict</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">y_test</span><span class="p">]</span>
<span class="c1">#df['sentiment']</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">X_train_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_train_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_train_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_train_2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X_test_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_test_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_test_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_test_2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">print</span> <span class="n">X_train_2</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">X_test_2</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">y_train_2</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">y_test_2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># mine</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> <span class="c1"># or Graph or whatever</span>
<span class="c1"># Do I need an input layer???</span>
<span class="c1">#model.add(Embedding(output_dim=trainFrame.shape[1], input_dim=len(vocab) + 1, mask_zero=True, weights=[trainFrame.values])) # note you have to put embedding weights in a list by convention</span>
<span class="c1">#model.add(Convolution1D(filters=num_filters,kernel_size=sz,\</span>
<span class="c1">#                        padding="valid",activation="relu",\</span>
<span class="c1">#                        input_shape=(600,1),strides=1))  </span>
<span class="c1">#model.add(Convolution1D(32,3, activation='relu',input_shape=(1,600)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1042</span><span class="p">,</span><span class="mi">600</span><span class="p">,),</span><span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">))</span>
<span class="c1">#model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span> <span class="c1"># for this is the architecture for predicting the next word, but insert your own here</span>
<span class="c1">#model.add(Dense(n_symbols, activation='softmax')) # for this is the architecture for predicting the next word, but insert your own here</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">"sparse_categorical_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">"adam"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">y_train_2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_2</span><span class="p">,</span><span class="n">y_train_2</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="c1">#model.fit(x_train, y_train, batch_size=32, epochs=10)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> <span class="c1"># or Graph or whatever</span>
<span class="c1"># Do I need an input layer???</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1042</span><span class="p">,</span><span class="mi">600</span><span class="p">,),</span><span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span> <span class="c1"># for this is the architecture for predicting the next word, but insert your own here</span>
<span class="c1">#model.add(Dense(n_symbols, activation='softmax')) # for this is the architecture for predicting the next word, but insert your own here</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">"sparse_categorical_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">"adam"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train_2</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y_train_2</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test_2</span><span class="p">,</span> <span class="n">y_test_2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Now-I-need-to-check-on-the-test-set!">Now I need to check on the test set!<a class="anchor-link" href="#Now-I-need-to-check-on-the-test-set!">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Build model</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">"CNN-static"</span> <span class="k">else</span> <span class="p">(</span><span class="n">sequence_length</span><span class="p">,)</span>
<span class="n">model_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>

<span class="c1"># Static model do not have embedding layer</span>
<span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">"CNN-static"</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">[</span><span class="mi">0</span><span class="p">])(</span><span class="n">model_input</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
<span class="c1">#    z = Embedding(output_dim=trainFrame.shape[1], input_dim=len(vocab) + 1, \</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> \
              <span class="n">mask_zero</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">trainFrame</span><span class="o">.</span><span class="n">values</span><span class="p">])(</span><span class="n">model_input</span><span class="p">)</span>
<span class="c1">#    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, \</span>
<span class="c1">#            name="embedding")(model_input)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">[</span><span class="mi">0</span><span class="p">])(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># Convolutional block</span>
<span class="n">conv_blocks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sz</span> <span class="ow">in</span> <span class="n">filter_sizes</span><span class="p">:</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">Convolution1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span>
                         <span class="n">kernel_size</span><span class="o">=</span><span class="n">sz</span><span class="p">,</span>
                         <span class="n">padding</span><span class="o">=</span><span class="s2">"valid"</span><span class="p">,</span>
                         <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">,</span>
                         <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">conv</span><span class="p">)</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">conv</span><span class="p">)</span>
    <span class="n">conv_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">()(</span><span class="n">conv_blocks</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">conv_blocks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">conv_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">[</span><span class="mi">1</span><span class="p">])(</span><span class="n">z</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">model_output</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">"binary_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">"adam"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="To-do">To do<a class="anchor-link" href="#To-do">¶</a></h1><ol>
<li>is the dimension right? Should it be 1042 x 600 x 1??<ul>
<li>Because on the validation split, it shouldn't have to change the input dimension?</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="In-development">In development<a class="anchor-link" href="#In-development">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">X_train_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y_train_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_train_3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X_test_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y_test_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_test_3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">print</span> <span class="n">X_train_3</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">y_train_3</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">X_test_3</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">y_test_3</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> <span class="c1"># or Graph or whatever</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">batch_input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">600</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">))</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span> <span class="c1"># for this is the architecture for predicting the next word, but insert your own here</span>
<span class="c1">#model.add(Dense(n_symbols, activation='softmax')) # for this is the architecture for predicting the next word, but insert your own here</span>
<span class="n">model2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">"sparse_categorical_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">"adam"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train_3</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y_train_3</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model2</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test_3</span><span class="p">,</span><span class="n">y_test_3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/python.html">python</a>
      <a href="/tag/machine-learning.html">machine learning</a>
      <a href="/tag/keras.html">keras</a>
      <a href="/tag/nlp.html">nlp</a>
      <a href="/tag/deep-learning.html">deep learning</a>
      <a href="/tag/classification.html">classification</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; Peter Frick </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Peter Frick ",
  "url" : "",
  "image": "https://raw.githubusercontent.com/frickp/insight/master/FrickHeadshot.jpg",
  "description": ""
}
</script>
  
</body>
</html>